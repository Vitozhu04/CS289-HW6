{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of gradient checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.17848559 -0.08654905 -0.4247721   0.42148769]\n",
      " [-0.07843826  0.34782227  0.02674299  0.1348031 ]\n",
      " [ 0.44697368 -1.07405563 -0.62114353  0.16984502]]\n",
      "[[ 0.17848559 -0.08654905 -0.4247721   0.42148769]\n",
      " [-0.07843826  0.34782227  0.02674299  0.1348031 ]\n",
      " [ 0.44697368 -1.07405563 -0.62114353  0.16984502]]\n"
     ]
    }
   ],
   "source": [
    "# gradient checking: compare the analytical gradient with the numerical gradient\n",
    "# taking the affine layer as an example\n",
    "from gradient_check import eval_numerical_gradient_array\n",
    "import numpy as np\n",
    "from layers import *\n",
    "np.random.seed(55)\n",
    "N = 2\n",
    "D = 3\n",
    "M = 4\n",
    "x = np.random.normal(size=(N, D))\n",
    "w = np.random.normal(size=(D, M))\n",
    "b = np.random.normal(size=(M, ))\n",
    "dout = np.random.normal(size=(N, M))\n",
    "\n",
    "# do a forward pass first\n",
    "out, cache = affine_forward(x, w, b)\n",
    "# check grad f/grad w, the [0] below gets the output out of the (output, cache) original output\n",
    "f=lambda w: affine_forward(x, w, b)[0]\n",
    "# compute the analytical gradient you wrote, [1] get the dw out of the (dx, dw, db) original output\n",
    "grad = affine_backward(dout, cache)[1]\n",
    "# compute the numerical gradient using the provided utility function\n",
    "ngrad = eval_numerical_gradient_array(f, w, dout)\n",
    "print(grad)\n",
    "print(ngrad)\n",
    "# they should be similar enough within some small error tolerance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# example of training a network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: put the path to your 'hw6_mds189', which should contain a 'trainval' and 'test' directory\n",
    "path = '/home/wenbo/hw6/hw6_mds189/trainval'\n",
    "from data_utils import load_mds189\n",
    "# load the dataset\n",
    "debug = False  # OPTIONAL: you can change this to True for debugging *only*. Your reported results must be with debug = False\n",
    "feat_train, label_train, feat_val, label_val = load_mds189(path,debug=False)\n",
    "from solver import Solver\n",
    "from classifiers.fc_net import FullyConnectedNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 360) loss: 291.549635\n",
      "(Epoch 0 / 10) train acc: 0.106000; val_acc: 0.115833\n",
      "(Epoch 1 / 10) train acc: 0.367000; val_acc: 0.324167\n",
      "(Epoch 2 / 10) train acc: 0.333000; val_acc: 0.309167\n",
      "(Iteration 101 / 360) loss: 4.727040\n",
      "(Epoch 3 / 10) train acc: 0.342000; val_acc: 0.307500\n",
      "(Epoch 4 / 10) train acc: 0.358000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.362000; val_acc: 0.305000\n",
      "(Iteration 201 / 360) loss: 5.754842\n",
      "(Epoch 6 / 10) train acc: 0.306000; val_acc: 0.305000\n",
      "(Epoch 7 / 10) train acc: 0.339000; val_acc: 0.305000\n",
      "(Epoch 8 / 10) train acc: 0.367000; val_acc: 0.305000\n",
      "(Iteration 301 / 360) loss: 5.040225\n",
      "(Epoch 9 / 10) train acc: 0.319000; val_acc: 0.305000\n",
      "(Epoch 10 / 10) train acc: 0.349000; val_acc: 0.305000\n"
     ]
    }
   ],
   "source": [
    "# problem 2.2\n",
    "np.random.seed(55)\n",
    "data = {\n",
    "      'X_train': feat_train,\n",
    "      'y_train': label_train,\n",
    "      'X_val': feat_val,\n",
    "      'y_val': label_val}\n",
    "\n",
    "# TODO: fill out the hyperparamets\n",
    "hyperparams = {'lr_decay': 0.1,\n",
    "               'num_epochs': 10,\n",
    "               'batch_size': 100,\n",
    "               'learning_rate': 10**-4\n",
    "              }\n",
    "\n",
    "# TODO: fill out the number of units in your hidden layers\n",
    "hidden_dim = [10] # this should be a list of units for each hiddent layer\n",
    "\n",
    "model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim)\n",
    "solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=100)\n",
    "np.random.seed(55)\n",
    "solver.train()\n",
    "#solver.loss_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 360) loss: 291.549635\n",
      "(Epoch 0 / 10) train acc: 0.123000; val_acc: 0.125000\n",
      "(Epoch 1 / 10) train acc: 0.121000; val_acc: 0.125000\n",
      "(Epoch 2 / 10) train acc: 0.130000; val_acc: 0.125000\n",
      "(Iteration 101 / 360) loss: 2.084348\n",
      "(Epoch 3 / 10) train acc: 0.118000; val_acc: 0.125000\n",
      "(Epoch 4 / 10) train acc: 0.128000; val_acc: 0.125000\n",
      "(Epoch 5 / 10) train acc: 0.139000; val_acc: 0.125000\n",
      "(Iteration 201 / 360) loss: 2.066076\n",
      "(Epoch 6 / 10) train acc: 0.123000; val_acc: 0.125000\n",
      "(Epoch 7 / 10) train acc: 0.130000; val_acc: 0.125000\n",
      "(Epoch 8 / 10) train acc: 0.113000; val_acc: 0.125000\n",
      "(Iteration 301 / 360) loss: 2.077316\n",
      "(Epoch 9 / 10) train acc: 0.134000; val_acc: 0.125000\n",
      "(Epoch 10 / 10) train acc: 0.130000; val_acc: 0.125000\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.123000; val_acc: 0.125000\n",
      "(Epoch 1 / 10) train acc: 0.127000; val_acc: 0.126667\n",
      "(Epoch 2 / 10) train acc: 0.129000; val_acc: 0.125000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo/hw6/resources/problem2/layers.py:147: RuntimeWarning: divide by zero encountered in log\n",
      "  loss = -np.sum(np.log(probs[np.arange(N), y])) / N\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 101 / 360) loss: 2.079984\n",
      "(Epoch 3 / 10) train acc: 0.126000; val_acc: 0.125000\n",
      "(Epoch 4 / 10) train acc: 0.122000; val_acc: 0.125000\n",
      "(Epoch 5 / 10) train acc: 0.130000; val_acc: 0.125000\n",
      "(Iteration 201 / 360) loss: 2.080864\n",
      "(Epoch 6 / 10) train acc: 0.108000; val_acc: 0.125000\n",
      "(Epoch 7 / 10) train acc: 0.130000; val_acc: 0.125000\n",
      "(Epoch 8 / 10) train acc: 0.119000; val_acc: 0.125000\n",
      "(Iteration 301 / 360) loss: 2.079033\n",
      "(Epoch 9 / 10) train acc: 0.134000; val_acc: 0.125000\n",
      "(Epoch 10 / 10) train acc: 0.137000; val_acc: 0.125000\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.123000; val_acc: 0.125000\n",
      "(Epoch 1 / 10) train acc: 0.127000; val_acc: 0.126667\n",
      "(Epoch 2 / 10) train acc: 0.126000; val_acc: 0.128333\n",
      "(Iteration 101 / 360) loss: 1.935354\n",
      "(Epoch 3 / 10) train acc: 0.203000; val_acc: 0.190000\n",
      "(Epoch 4 / 10) train acc: 0.201000; val_acc: 0.201667\n",
      "(Epoch 5 / 10) train acc: 0.219000; val_acc: 0.198333\n",
      "(Iteration 201 / 360) loss: 2.080474\n",
      "(Epoch 6 / 10) train acc: 0.108000; val_acc: 0.130000\n",
      "(Epoch 7 / 10) train acc: 0.126000; val_acc: 0.125000\n",
      "(Epoch 8 / 10) train acc: 0.108000; val_acc: 0.125000\n",
      "(Iteration 301 / 360) loss: 2.076088\n",
      "(Epoch 9 / 10) train acc: 0.119000; val_acc: 0.125000\n",
      "(Epoch 10 / 10) train acc: 0.116000; val_acc: 0.125000\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.127000; val_acc: 0.120833\n",
      "(Epoch 1 / 10) train acc: 0.162000; val_acc: 0.150833\n",
      "(Epoch 2 / 10) train acc: 0.146000; val_acc: 0.149167\n",
      "(Iteration 101 / 360) loss: 2.047942\n",
      "(Epoch 3 / 10) train acc: 0.164000; val_acc: 0.151667\n",
      "(Epoch 4 / 10) train acc: 0.145000; val_acc: 0.150833\n",
      "(Epoch 5 / 10) train acc: 0.166000; val_acc: 0.159167\n",
      "(Iteration 201 / 360) loss: 1.998235\n",
      "(Epoch 6 / 10) train acc: 0.149000; val_acc: 0.161667\n",
      "(Epoch 7 / 10) train acc: 0.169000; val_acc: 0.163333\n",
      "(Epoch 8 / 10) train acc: 0.163000; val_acc: 0.162500\n",
      "(Iteration 301 / 360) loss: 2.172621\n",
      "(Epoch 9 / 10) train acc: 0.182000; val_acc: 0.169167\n",
      "(Epoch 10 / 10) train acc: 0.184000; val_acc: 0.169167\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.177000; val_acc: 0.182500\n",
      "(Iteration 101 / 360) loss: 2.489568\n",
      "(Epoch 3 / 10) train acc: 0.167000; val_acc: 0.179167\n",
      "(Epoch 4 / 10) train acc: 0.154000; val_acc: 0.174167\n",
      "(Epoch 5 / 10) train acc: 0.173000; val_acc: 0.170000\n",
      "(Iteration 201 / 360) loss: 2.038348\n",
      "(Epoch 6 / 10) train acc: 0.150000; val_acc: 0.168333\n",
      "(Epoch 7 / 10) train acc: 0.169000; val_acc: 0.164167\n",
      "(Epoch 8 / 10) train acc: 0.147000; val_acc: 0.164167\n",
      "(Iteration 301 / 360) loss: 2.482166\n",
      "(Epoch 9 / 10) train acc: 0.164000; val_acc: 0.162500\n",
      "(Epoch 10 / 10) train acc: 0.162000; val_acc: 0.158333\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.179000; val_acc: 0.193333\n",
      "(Epoch 1 / 10) train acc: 0.145000; val_acc: 0.159167\n",
      "(Epoch 2 / 10) train acc: 0.143000; val_acc: 0.181667\n",
      "(Iteration 101 / 360) loss: 6.393901\n",
      "(Epoch 3 / 10) train acc: 0.173000; val_acc: 0.186667\n",
      "(Epoch 4 / 10) train acc: 0.147000; val_acc: 0.165000\n",
      "(Epoch 5 / 10) train acc: 0.159000; val_acc: 0.169167\n",
      "(Iteration 201 / 360) loss: 3.100756\n",
      "(Epoch 6 / 10) train acc: 0.162000; val_acc: 0.191667\n",
      "(Epoch 7 / 10) train acc: 0.199000; val_acc: 0.191667\n",
      "(Epoch 8 / 10) train acc: 0.202000; val_acc: 0.185000\n",
      "(Iteration 301 / 360) loss: 3.993107\n",
      "(Epoch 9 / 10) train acc: 0.191000; val_acc: 0.182500\n",
      "(Epoch 10 / 10) train acc: 0.182000; val_acc: 0.180833\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.161000; val_acc: 0.176667\n",
      "(Epoch 1 / 10) train acc: 0.158000; val_acc: 0.205833\n",
      "(Epoch 2 / 10) train acc: 0.160000; val_acc: 0.190833\n",
      "(Iteration 101 / 360) loss: 34.714307\n",
      "(Epoch 3 / 10) train acc: 0.179000; val_acc: 0.185000\n",
      "(Epoch 4 / 10) train acc: 0.154000; val_acc: 0.182500\n",
      "(Epoch 5 / 10) train acc: 0.185000; val_acc: 0.175833\n",
      "(Iteration 201 / 360) loss: 31.322892\n",
      "(Epoch 6 / 10) train acc: 0.149000; val_acc: 0.170833\n",
      "(Epoch 7 / 10) train acc: 0.173000; val_acc: 0.167500\n",
      "(Epoch 8 / 10) train acc: 0.146000; val_acc: 0.165833\n",
      "(Iteration 301 / 360) loss: 23.699631\n",
      "(Epoch 9 / 10) train acc: 0.144000; val_acc: 0.168333\n",
      "(Epoch 10 / 10) train acc: 0.144000; val_acc: 0.167500\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.161000; val_acc: 0.176667\n",
      "(Epoch 1 / 10) train acc: 0.145000; val_acc: 0.182500\n",
      "(Epoch 2 / 10) train acc: 0.175000; val_acc: 0.187500\n",
      "(Iteration 101 / 360) loss: 44.097552\n",
      "(Epoch 3 / 10) train acc: 0.181000; val_acc: 0.195833\n",
      "(Epoch 4 / 10) train acc: 0.170000; val_acc: 0.201667\n",
      "(Epoch 5 / 10) train acc: 0.205000; val_acc: 0.205000\n",
      "(Iteration 201 / 360) loss: 49.956546\n",
      "(Epoch 6 / 10) train acc: 0.193000; val_acc: 0.204167\n",
      "(Epoch 7 / 10) train acc: 0.202000; val_acc: 0.205000\n",
      "(Epoch 8 / 10) train acc: 0.163000; val_acc: 0.205000\n",
      "(Iteration 301 / 360) loss: 43.929484\n",
      "(Epoch 9 / 10) train acc: 0.174000; val_acc: 0.206667\n",
      "(Epoch 10 / 10) train acc: 0.174000; val_acc: 0.205833\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.160000; val_acc: 0.176667\n",
      "(Epoch 1 / 10) train acc: 0.140000; val_acc: 0.176667\n",
      "(Epoch 2 / 10) train acc: 0.165000; val_acc: 0.176667\n",
      "(Iteration 101 / 360) loss: 45.923287\n",
      "(Epoch 3 / 10) train acc: 0.169000; val_acc: 0.175833\n",
      "(Epoch 4 / 10) train acc: 0.153000; val_acc: 0.176667\n",
      "(Epoch 5 / 10) train acc: 0.191000; val_acc: 0.176667\n",
      "(Iteration 201 / 360) loss: 53.670673\n",
      "(Epoch 6 / 10) train acc: 0.177000; val_acc: 0.176667\n",
      "(Epoch 7 / 10) train acc: 0.180000; val_acc: 0.177500\n",
      "(Epoch 8 / 10) train acc: 0.157000; val_acc: 0.180833\n",
      "(Iteration 301 / 360) loss: 48.775283\n",
      "(Epoch 9 / 10) train acc: 0.169000; val_acc: 0.182500\n",
      "(Epoch 10 / 10) train acc: 0.172000; val_acc: 0.182500\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.160000; val_acc: 0.176667\n",
      "(Epoch 1 / 10) train acc: 0.138000; val_acc: 0.176667\n",
      "(Epoch 2 / 10) train acc: 0.166000; val_acc: 0.176667\n",
      "(Iteration 101 / 360) loss: 46.130429\n",
      "(Epoch 3 / 10) train acc: 0.165000; val_acc: 0.176667\n",
      "(Epoch 4 / 10) train acc: 0.150000; val_acc: 0.176667\n",
      "(Epoch 5 / 10) train acc: 0.191000; val_acc: 0.176667\n",
      "(Iteration 201 / 360) loss: 54.127022\n",
      "(Epoch 6 / 10) train acc: 0.179000; val_acc: 0.176667\n",
      "(Epoch 7 / 10) train acc: 0.178000; val_acc: 0.176667\n",
      "(Epoch 8 / 10) train acc: 0.154000; val_acc: 0.176667\n",
      "(Iteration 301 / 360) loss: 49.413144\n",
      "(Epoch 9 / 10) train acc: 0.166000; val_acc: 0.176667\n",
      "(Epoch 10 / 10) train acc: 0.166000; val_acc: 0.176667\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(55)\n",
    "accu = []\n",
    "for i in range(10):\n",
    "    data = {\n",
    "          'X_train': feat_train,\n",
    "          'y_train': label_train,\n",
    "          'X_val': feat_val,\n",
    "          'y_val': label_val}\n",
    "\n",
    "    # TODO: fill out the hyperparamets\n",
    "    hyperparams = {'lr_decay': 1,\n",
    "               'num_epochs': 10,\n",
    "               'batch_size': 100,\n",
    "               'learning_rate': 10**-i\n",
    "              }\n",
    "\n",
    "    # TODO: fill out the number of units in your hidden layers\n",
    "    hidden_dim = [10] # this should be a list of units for each hiddent layer\n",
    "\n",
    "    model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim)\n",
    "    solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=100)\n",
    "    np.random.seed(55)\n",
    "    solver.train()\n",
    "    accu.append(solver.best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 0.22\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLnwzAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89b27a25f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(10), accu)\n",
    "print(range(10)[np.argmax(accu)],np.max(accu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 360) loss: 291.549635\n",
      "(Epoch 0 / 10) train acc: 0.106000; val_acc: 0.115833\n",
      "(Epoch 1 / 10) train acc: 0.367000; val_acc: 0.324167\n",
      "(Epoch 2 / 10) train acc: 0.333000; val_acc: 0.309167\n",
      "(Iteration 101 / 360) loss: 4.727040\n",
      "(Epoch 3 / 10) train acc: 0.342000; val_acc: 0.307500\n",
      "(Epoch 4 / 10) train acc: 0.358000; val_acc: 0.305000\n",
      "(Epoch 5 / 10) train acc: 0.362000; val_acc: 0.305000\n",
      "(Iteration 201 / 360) loss: 5.754842\n",
      "(Epoch 6 / 10) train acc: 0.306000; val_acc: 0.305000\n",
      "(Epoch 7 / 10) train acc: 0.339000; val_acc: 0.305000\n",
      "(Epoch 8 / 10) train acc: 0.367000; val_acc: 0.305000\n",
      "(Iteration 301 / 360) loss: 5.040225\n",
      "(Epoch 9 / 10) train acc: 0.319000; val_acc: 0.305000\n",
      "(Epoch 10 / 10) train acc: 0.349000; val_acc: 0.305000\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.183000; val_acc: 0.197500\n",
      "(Iteration 101 / 360) loss: 2.950822\n",
      "(Epoch 3 / 10) train acc: 0.187000; val_acc: 0.197500\n",
      "(Epoch 4 / 10) train acc: 0.172000; val_acc: 0.197500\n",
      "(Epoch 5 / 10) train acc: 0.194000; val_acc: 0.197500\n",
      "(Iteration 201 / 360) loss: 2.378493\n",
      "(Epoch 6 / 10) train acc: 0.166000; val_acc: 0.197500\n",
      "(Epoch 7 / 10) train acc: 0.197000; val_acc: 0.197500\n",
      "(Epoch 8 / 10) train acc: 0.178000; val_acc: 0.197500\n",
      "(Iteration 301 / 360) loss: 3.456010\n",
      "(Epoch 9 / 10) train acc: 0.180000; val_acc: 0.197500\n",
      "(Epoch 10 / 10) train acc: 0.177000; val_acc: 0.197500\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.181000; val_acc: 0.197500\n",
      "(Iteration 101 / 360) loss: 2.870806\n",
      "(Epoch 3 / 10) train acc: 0.185000; val_acc: 0.192500\n",
      "(Epoch 4 / 10) train acc: 0.172000; val_acc: 0.191667\n",
      "(Epoch 5 / 10) train acc: 0.193000; val_acc: 0.191667\n",
      "(Iteration 201 / 360) loss: 2.314916\n",
      "(Epoch 6 / 10) train acc: 0.165000; val_acc: 0.191667\n",
      "(Epoch 7 / 10) train acc: 0.189000; val_acc: 0.191667\n",
      "(Epoch 8 / 10) train acc: 0.175000; val_acc: 0.191667\n",
      "(Iteration 301 / 360) loss: 3.293249\n",
      "(Epoch 9 / 10) train acc: 0.173000; val_acc: 0.191667\n",
      "(Epoch 10 / 10) train acc: 0.173000; val_acc: 0.191667\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.180000; val_acc: 0.191667\n",
      "(Iteration 101 / 360) loss: 2.784238\n",
      "(Epoch 3 / 10) train acc: 0.180000; val_acc: 0.186667\n",
      "(Epoch 4 / 10) train acc: 0.168000; val_acc: 0.186667\n",
      "(Epoch 5 / 10) train acc: 0.188000; val_acc: 0.187500\n",
      "(Iteration 201 / 360) loss: 2.246125\n",
      "(Epoch 6 / 10) train acc: 0.160000; val_acc: 0.186667\n",
      "(Epoch 7 / 10) train acc: 0.185000; val_acc: 0.186667\n",
      "(Epoch 8 / 10) train acc: 0.172000; val_acc: 0.186667\n",
      "(Iteration 301 / 360) loss: 3.100493\n",
      "(Epoch 9 / 10) train acc: 0.173000; val_acc: 0.186667\n",
      "(Epoch 10 / 10) train acc: 0.167000; val_acc: 0.186667\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.180000; val_acc: 0.188333\n",
      "(Iteration 101 / 360) loss: 2.707635\n",
      "(Epoch 3 / 10) train acc: 0.175000; val_acc: 0.181667\n",
      "(Epoch 4 / 10) train acc: 0.165000; val_acc: 0.181667\n",
      "(Epoch 5 / 10) train acc: 0.183000; val_acc: 0.182500\n",
      "(Iteration 201 / 360) loss: 2.181977\n",
      "(Epoch 6 / 10) train acc: 0.157000; val_acc: 0.182500\n",
      "(Epoch 7 / 10) train acc: 0.185000; val_acc: 0.182500\n",
      "(Epoch 8 / 10) train acc: 0.168000; val_acc: 0.182500\n",
      "(Iteration 301 / 360) loss: 2.939285\n",
      "(Epoch 9 / 10) train acc: 0.172000; val_acc: 0.182500\n",
      "(Epoch 10 / 10) train acc: 0.164000; val_acc: 0.182500\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.176000; val_acc: 0.187500\n",
      "(Iteration 101 / 360) loss: 2.644912\n",
      "(Epoch 3 / 10) train acc: 0.173000; val_acc: 0.181667\n",
      "(Epoch 4 / 10) train acc: 0.165000; val_acc: 0.181667\n",
      "(Epoch 5 / 10) train acc: 0.181000; val_acc: 0.181667\n",
      "(Iteration 201 / 360) loss: 2.137088\n",
      "(Epoch 6 / 10) train acc: 0.155000; val_acc: 0.181667\n",
      "(Epoch 7 / 10) train acc: 0.182000; val_acc: 0.180000\n",
      "(Epoch 8 / 10) train acc: 0.165000; val_acc: 0.180000\n",
      "(Iteration 301 / 360) loss: 2.845634\n",
      "(Epoch 9 / 10) train acc: 0.165000; val_acc: 0.180000\n",
      "(Epoch 10 / 10) train acc: 0.165000; val_acc: 0.178333\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.178000; val_acc: 0.184167\n",
      "(Iteration 101 / 360) loss: 2.599871\n",
      "(Epoch 3 / 10) train acc: 0.171000; val_acc: 0.181667\n",
      "(Epoch 4 / 10) train acc: 0.160000; val_acc: 0.178333\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.180000\n",
      "(Iteration 201 / 360) loss: 2.086307\n",
      "(Epoch 6 / 10) train acc: 0.148000; val_acc: 0.179167\n",
      "(Epoch 7 / 10) train acc: 0.180000; val_acc: 0.177500\n",
      "(Epoch 8 / 10) train acc: 0.156000; val_acc: 0.175833\n",
      "(Iteration 301 / 360) loss: 2.765646\n",
      "(Epoch 9 / 10) train acc: 0.160000; val_acc: 0.175833\n",
      "(Epoch 10 / 10) train acc: 0.166000; val_acc: 0.175000\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.177000; val_acc: 0.181667\n",
      "(Iteration 101 / 360) loss: 2.557347\n",
      "(Epoch 3 / 10) train acc: 0.169000; val_acc: 0.180000\n",
      "(Epoch 4 / 10) train acc: 0.157000; val_acc: 0.178333\n",
      "(Epoch 5 / 10) train acc: 0.181000; val_acc: 0.176667\n",
      "(Iteration 201 / 360) loss: 2.064829\n",
      "(Epoch 6 / 10) train acc: 0.147000; val_acc: 0.175000\n",
      "(Epoch 7 / 10) train acc: 0.178000; val_acc: 0.173333\n",
      "(Epoch 8 / 10) train acc: 0.153000; val_acc: 0.172500\n",
      "(Iteration 301 / 360) loss: 2.678601\n",
      "(Epoch 9 / 10) train acc: 0.159000; val_acc: 0.171667\n",
      "(Epoch 10 / 10) train acc: 0.165000; val_acc: 0.170000\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.175000; val_acc: 0.182500\n",
      "(Iteration 101 / 360) loss: 2.525524\n",
      "(Epoch 3 / 10) train acc: 0.169000; val_acc: 0.180000\n",
      "(Epoch 4 / 10) train acc: 0.155000; val_acc: 0.175833\n",
      "(Epoch 5 / 10) train acc: 0.174000; val_acc: 0.174167\n",
      "(Iteration 201 / 360) loss: 2.053320\n",
      "(Epoch 6 / 10) train acc: 0.149000; val_acc: 0.170833\n",
      "(Epoch 7 / 10) train acc: 0.176000; val_acc: 0.169167\n",
      "(Epoch 8 / 10) train acc: 0.150000; val_acc: 0.169167\n",
      "(Iteration 301 / 360) loss: 2.577062\n",
      "(Epoch 9 / 10) train acc: 0.163000; val_acc: 0.166667\n",
      "(Epoch 10 / 10) train acc: 0.161000; val_acc: 0.165833\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.177000; val_acc: 0.182500\n",
      "(Iteration 101 / 360) loss: 2.489568\n",
      "(Epoch 3 / 10) train acc: 0.167000; val_acc: 0.179167\n",
      "(Epoch 4 / 10) train acc: 0.154000; val_acc: 0.174167\n",
      "(Epoch 5 / 10) train acc: 0.173000; val_acc: 0.170000\n",
      "(Iteration 201 / 360) loss: 2.038348\n",
      "(Epoch 6 / 10) train acc: 0.150000; val_acc: 0.168333\n",
      "(Epoch 7 / 10) train acc: 0.169000; val_acc: 0.164167\n",
      "(Epoch 8 / 10) train acc: 0.147000; val_acc: 0.164167\n",
      "(Iteration 301 / 360) loss: 2.482166\n",
      "(Epoch 9 / 10) train acc: 0.164000; val_acc: 0.162500\n",
      "(Epoch 10 / 10) train acc: 0.162000; val_acc: 0.158333\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(55)\n",
    "accu2 = []\n",
    "for i in range(1,11):\n",
    "    data = {\n",
    "          'X_train': feat_train,\n",
    "          'y_train': label_train,\n",
    "          'X_val': feat_val,\n",
    "          'y_val': label_val}\n",
    "\n",
    "    # TODO: fill out the hyperparamets\n",
    "    hyperparams = {'lr_decay': 0.1*i,\n",
    "               'num_epochs': 10,\n",
    "               'batch_size': 100,\n",
    "               'learning_rate': 10**-4\n",
    "              }\n",
    "\n",
    "    # TODO: fill out the number of units in your hidden layers\n",
    "    hidden_dim = [10] # this should be a list of units for each hiddent layer\n",
    "\n",
    "    model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim)\n",
    "    solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=100)\n",
    "    np.random.seed(55)\n",
    "    solver.train()\n",
    "    accu2.append(solver.best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 0.324166666667\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89b2b79470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(1,11), accu2)\n",
    "print(range(1,11)[np.argmax(accu2)],np.max(accu2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 3600) loss: 312.144191\n",
      "(Epoch 0 / 10) train acc: 0.148000; val_acc: 0.141667\n",
      "(Iteration 101 / 3600) loss: 1.845378\n",
      "(Iteration 201 / 3600) loss: 1.966831\n",
      "(Iteration 301 / 3600) loss: 1.853941\n",
      "(Epoch 1 / 10) train acc: 0.206000; val_acc: 0.187500\n",
      "(Iteration 401 / 3600) loss: 1.930444\n",
      "(Iteration 501 / 3600) loss: 1.999430\n",
      "(Iteration 601 / 3600) loss: 1.923726\n",
      "(Iteration 701 / 3600) loss: 2.021568\n",
      "(Epoch 2 / 10) train acc: 0.234000; val_acc: 0.209167\n",
      "(Iteration 801 / 3600) loss: 2.021518\n",
      "(Iteration 901 / 3600) loss: 1.669965\n",
      "(Iteration 1001 / 3600) loss: 1.856470\n",
      "(Epoch 3 / 10) train acc: 0.224000; val_acc: 0.209167\n",
      "(Iteration 1101 / 3600) loss: 2.078911\n",
      "(Iteration 1201 / 3600) loss: 1.827174\n",
      "(Iteration 1301 / 3600) loss: 1.734806\n",
      "(Iteration 1401 / 3600) loss: 1.988363\n",
      "(Epoch 4 / 10) train acc: 0.227000; val_acc: 0.209167\n",
      "(Iteration 1501 / 3600) loss: 2.017159\n",
      "(Iteration 1601 / 3600) loss: 1.821333\n",
      "(Iteration 1701 / 3600) loss: 1.719704\n",
      "(Epoch 5 / 10) train acc: 0.217000; val_acc: 0.209167\n",
      "(Iteration 1801 / 3600) loss: 1.964470\n",
      "(Iteration 1901 / 3600) loss: 1.926999\n",
      "(Iteration 2001 / 3600) loss: 2.106193\n",
      "(Iteration 2101 / 3600) loss: 2.386109\n",
      "(Epoch 6 / 10) train acc: 0.206000; val_acc: 0.209167\n",
      "(Iteration 2201 / 3600) loss: 1.403264\n",
      "(Iteration 2301 / 3600) loss: 1.798430\n",
      "(Iteration 2401 / 3600) loss: 1.726765\n",
      "(Iteration 2501 / 3600) loss: 1.996063\n",
      "(Epoch 7 / 10) train acc: 0.222000; val_acc: 0.209167\n",
      "(Iteration 2601 / 3600) loss: 2.777272\n",
      "(Iteration 2701 / 3600) loss: 2.396367\n",
      "(Iteration 2801 / 3600) loss: 2.079243\n",
      "(Epoch 8 / 10) train acc: 0.219000; val_acc: 0.209167\n",
      "(Iteration 2901 / 3600) loss: 1.988831\n",
      "(Iteration 3001 / 3600) loss: 1.594281\n",
      "(Iteration 3101 / 3600) loss: 1.874613\n",
      "(Iteration 3201 / 3600) loss: 2.088208\n",
      "(Epoch 9 / 10) train acc: 0.218000; val_acc: 0.209167\n",
      "(Iteration 3301 / 3600) loss: 1.817436\n",
      "(Iteration 3401 / 3600) loss: 1.861584\n",
      "(Iteration 3501 / 3600) loss: 2.079235\n",
      "(Epoch 10 / 10) train acc: 0.222000; val_acc: 0.209167\n",
      "(Iteration 1 / 1800) loss: 42.452937\n",
      "(Epoch 0 / 10) train acc: 0.148000; val_acc: 0.181667\n",
      "(Iteration 101 / 1800) loss: 3.450880\n",
      "(Epoch 1 / 10) train acc: 0.165000; val_acc: 0.163333\n",
      "(Iteration 201 / 1800) loss: 1.983409\n",
      "(Iteration 301 / 1800) loss: 2.079430\n",
      "(Epoch 2 / 10) train acc: 0.155000; val_acc: 0.163333\n",
      "(Iteration 401 / 1800) loss: 2.079485\n",
      "(Iteration 501 / 1800) loss: 1.989932\n",
      "(Epoch 3 / 10) train acc: 0.165000; val_acc: 0.163333\n",
      "(Iteration 601 / 1800) loss: 2.079472\n",
      "(Iteration 701 / 1800) loss: 1.996169\n",
      "(Epoch 4 / 10) train acc: 0.149000; val_acc: 0.163333\n",
      "(Iteration 801 / 1800) loss: 2.003118\n",
      "(Epoch 5 / 10) train acc: 0.162000; val_acc: 0.163333\n",
      "(Iteration 901 / 1800) loss: 2.029695\n",
      "(Iteration 1001 / 1800) loss: 2.069687\n",
      "(Epoch 6 / 10) train acc: 0.143000; val_acc: 0.163333\n",
      "(Iteration 1101 / 1800) loss: 2.832308\n",
      "(Iteration 1201 / 1800) loss: 2.079466\n",
      "(Epoch 7 / 10) train acc: 0.166000; val_acc: 0.163333\n",
      "(Iteration 1301 / 1800) loss: 4.077155\n",
      "(Iteration 1401 / 1800) loss: 2.018315\n",
      "(Epoch 8 / 10) train acc: 0.145000; val_acc: 0.163333\n",
      "(Iteration 1501 / 1800) loss: 2.071855\n",
      "(Iteration 1601 / 1800) loss: 2.079250\n",
      "(Epoch 9 / 10) train acc: 0.161000; val_acc: 0.163333\n",
      "(Iteration 1701 / 1800) loss: 2.079382\n",
      "(Epoch 10 / 10) train acc: 0.161000; val_acc: 0.163333\n",
      "(Iteration 1 / 1200) loss: 42.987974\n",
      "(Epoch 0 / 10) train acc: 0.147000; val_acc: 0.181667\n",
      "(Iteration 101 / 1200) loss: 2.218614\n",
      "(Epoch 1 / 10) train acc: 0.172000; val_acc: 0.169167\n",
      "(Iteration 201 / 1200) loss: 2.068108\n",
      "(Epoch 2 / 10) train acc: 0.158000; val_acc: 0.167500\n",
      "(Iteration 301 / 1200) loss: 1.965287\n",
      "(Epoch 3 / 10) train acc: 0.166000; val_acc: 0.166667\n",
      "(Iteration 401 / 1200) loss: 2.218638\n",
      "(Epoch 4 / 10) train acc: 0.150000; val_acc: 0.166667\n",
      "(Iteration 501 / 1200) loss: 1.949335\n",
      "(Epoch 5 / 10) train acc: 0.167000; val_acc: 0.166667\n",
      "(Iteration 601 / 1200) loss: 2.788352\n",
      "(Iteration 701 / 1200) loss: 1.972274\n",
      "(Epoch 6 / 10) train acc: 0.150000; val_acc: 0.166667\n",
      "(Iteration 801 / 1200) loss: 2.041439\n",
      "(Epoch 7 / 10) train acc: 0.173000; val_acc: 0.166667\n",
      "(Iteration 901 / 1200) loss: 2.402602\n",
      "(Epoch 8 / 10) train acc: 0.149000; val_acc: 0.166667\n",
      "(Iteration 1001 / 1200) loss: 2.259525\n",
      "(Epoch 9 / 10) train acc: 0.159000; val_acc: 0.166667\n",
      "(Iteration 1101 / 1200) loss: 2.947617\n",
      "(Epoch 10 / 10) train acc: 0.159000; val_acc: 0.166667\n",
      "(Iteration 1 / 900) loss: 46.868747\n",
      "(Epoch 0 / 10) train acc: 0.173000; val_acc: 0.202500\n",
      "(Epoch 1 / 10) train acc: 0.177000; val_acc: 0.172500\n",
      "(Iteration 101 / 900) loss: 1.949605\n",
      "(Epoch 2 / 10) train acc: 0.164000; val_acc: 0.170833\n",
      "(Iteration 201 / 900) loss: 2.067889\n",
      "(Epoch 3 / 10) train acc: 0.170000; val_acc: 0.170833\n",
      "(Iteration 301 / 900) loss: 2.187301\n",
      "(Epoch 4 / 10) train acc: 0.157000; val_acc: 0.170833\n",
      "(Iteration 401 / 900) loss: 2.344752\n",
      "(Epoch 5 / 10) train acc: 0.174000; val_acc: 0.170833\n",
      "(Iteration 501 / 900) loss: 1.982613\n",
      "(Epoch 6 / 10) train acc: 0.155000; val_acc: 0.170833\n",
      "(Iteration 601 / 900) loss: 2.049309\n",
      "(Epoch 7 / 10) train acc: 0.183000; val_acc: 0.170833\n",
      "(Iteration 701 / 900) loss: 2.022174\n",
      "(Epoch 8 / 10) train acc: 0.156000; val_acc: 0.170833\n",
      "(Iteration 801 / 900) loss: 2.136059\n",
      "(Epoch 9 / 10) train acc: 0.162000; val_acc: 0.170833\n",
      "(Epoch 10 / 10) train acc: 0.162000; val_acc: 0.170833\n",
      "(Iteration 1 / 720) loss: 45.994934\n",
      "(Epoch 0 / 10) train acc: 0.184000; val_acc: 0.210833\n",
      "(Epoch 1 / 10) train acc: 0.184000; val_acc: 0.178333\n",
      "(Iteration 101 / 720) loss: 2.195907\n",
      "(Epoch 2 / 10) train acc: 0.168000; val_acc: 0.177500\n",
      "(Iteration 201 / 720) loss: 2.108255\n",
      "(Epoch 3 / 10) train acc: 0.175000; val_acc: 0.177500\n",
      "(Epoch 4 / 10) train acc: 0.160000; val_acc: 0.177500\n",
      "(Iteration 301 / 720) loss: 2.063247\n",
      "(Epoch 5 / 10) train acc: 0.177000; val_acc: 0.177500\n",
      "(Iteration 401 / 720) loss: 2.089646\n",
      "(Epoch 6 / 10) train acc: 0.156000; val_acc: 0.177500\n",
      "(Iteration 501 / 720) loss: 2.793677\n",
      "(Epoch 7 / 10) train acc: 0.184000; val_acc: 0.177500\n",
      "(Epoch 8 / 10) train acc: 0.165000; val_acc: 0.177500\n",
      "(Iteration 601 / 720) loss: 2.422651\n",
      "(Epoch 9 / 10) train acc: 0.171000; val_acc: 0.177500\n",
      "(Iteration 701 / 720) loss: 2.121230\n",
      "(Epoch 10 / 10) train acc: 0.164000; val_acc: 0.177500\n",
      "(Iteration 1 / 600) loss: 45.788111\n",
      "(Epoch 0 / 10) train acc: 0.174000; val_acc: 0.209167\n",
      "(Epoch 1 / 10) train acc: 0.182000; val_acc: 0.182500\n",
      "(Iteration 101 / 600) loss: 2.098335\n",
      "(Epoch 2 / 10) train acc: 0.171000; val_acc: 0.182500\n",
      "(Epoch 3 / 10) train acc: 0.177000; val_acc: 0.182500\n",
      "(Iteration 201 / 600) loss: 2.418830\n",
      "(Epoch 4 / 10) train acc: 0.164000; val_acc: 0.182500\n",
      "(Epoch 5 / 10) train acc: 0.185000; val_acc: 0.182500\n",
      "(Iteration 301 / 600) loss: 2.475643\n",
      "(Epoch 6 / 10) train acc: 0.158000; val_acc: 0.182500\n",
      "(Iteration 401 / 600) loss: 2.073332\n",
      "(Epoch 7 / 10) train acc: 0.185000; val_acc: 0.182500\n",
      "(Epoch 8 / 10) train acc: 0.168000; val_acc: 0.182500\n",
      "(Iteration 501 / 600) loss: 3.577058\n",
      "(Epoch 9 / 10) train acc: 0.169000; val_acc: 0.182500\n",
      "(Epoch 10 / 10) train acc: 0.166000; val_acc: 0.182500\n",
      "(Iteration 1 / 510) loss: 44.736339\n",
      "(Epoch 0 / 10) train acc: 0.174000; val_acc: 0.209167\n",
      "(Epoch 1 / 10) train acc: 0.195000; val_acc: 0.190000\n",
      "(Iteration 101 / 510) loss: 3.086967\n",
      "(Epoch 2 / 10) train acc: 0.170000; val_acc: 0.187500\n",
      "(Epoch 3 / 10) train acc: 0.186000; val_acc: 0.185833\n",
      "(Iteration 201 / 510) loss: 2.222247\n",
      "(Epoch 4 / 10) train acc: 0.168000; val_acc: 0.185833\n",
      "(Epoch 5 / 10) train acc: 0.193000; val_acc: 0.185833\n",
      "(Iteration 301 / 510) loss: 2.682542\n",
      "(Epoch 6 / 10) train acc: 0.169000; val_acc: 0.185833\n",
      "(Epoch 7 / 10) train acc: 0.179000; val_acc: 0.185833\n",
      "(Iteration 401 / 510) loss: 2.720370\n",
      "(Epoch 8 / 10) train acc: 0.171000; val_acc: 0.185833\n",
      "(Epoch 9 / 10) train acc: 0.172000; val_acc: 0.185833\n",
      "(Iteration 501 / 510) loss: 2.120843\n",
      "(Epoch 10 / 10) train acc: 0.167000; val_acc: 0.185833\n",
      "(Iteration 1 / 450) loss: 133.729583\n",
      "(Epoch 0 / 10) train acc: 0.178000; val_acc: 0.195833\n",
      "(Epoch 1 / 10) train acc: 0.192000; val_acc: 0.186667\n",
      "(Epoch 2 / 10) train acc: 0.199000; val_acc: 0.186667\n",
      "(Iteration 101 / 450) loss: 1.973884\n",
      "(Epoch 3 / 10) train acc: 0.198000; val_acc: 0.186667\n",
      "(Epoch 4 / 10) train acc: 0.188000; val_acc: 0.186667\n",
      "(Iteration 201 / 450) loss: 5.019086\n",
      "(Epoch 5 / 10) train acc: 0.184000; val_acc: 0.186667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 6 / 10) train acc: 0.182000; val_acc: 0.186667\n",
      "(Iteration 301 / 450) loss: 2.435051\n",
      "(Epoch 7 / 10) train acc: 0.194000; val_acc: 0.186667\n",
      "(Epoch 8 / 10) train acc: 0.184000; val_acc: 0.186667\n",
      "(Iteration 401 / 450) loss: 2.027385\n",
      "(Epoch 9 / 10) train acc: 0.173000; val_acc: 0.186667\n",
      "(Epoch 10 / 10) train acc: 0.190000; val_acc: 0.186667\n",
      "(Iteration 1 / 400) loss: 48.950434\n",
      "(Epoch 0 / 10) train acc: 0.187000; val_acc: 0.213333\n",
      "(Epoch 1 / 10) train acc: 0.198000; val_acc: 0.198333\n",
      "(Epoch 2 / 10) train acc: 0.181000; val_acc: 0.197500\n",
      "(Iteration 101 / 400) loss: 2.434427\n",
      "(Epoch 3 / 10) train acc: 0.186000; val_acc: 0.196667\n",
      "(Epoch 4 / 10) train acc: 0.171000; val_acc: 0.196667\n",
      "(Epoch 5 / 10) train acc: 0.191000; val_acc: 0.196667\n",
      "(Iteration 201 / 400) loss: 2.764924\n",
      "(Epoch 6 / 10) train acc: 0.166000; val_acc: 0.196667\n",
      "(Epoch 7 / 10) train acc: 0.195000; val_acc: 0.196667\n",
      "(Iteration 301 / 400) loss: 3.221483\n",
      "(Epoch 8 / 10) train acc: 0.178000; val_acc: 0.196667\n",
      "(Epoch 9 / 10) train acc: 0.179000; val_acc: 0.196667\n",
      "(Epoch 10 / 10) train acc: 0.176000; val_acc: 0.196667\n",
      "(Iteration 1 / 360) loss: 47.891391\n",
      "(Epoch 0 / 10) train acc: 0.186000; val_acc: 0.220000\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.202500\n",
      "(Epoch 2 / 10) train acc: 0.185000; val_acc: 0.198333\n",
      "(Iteration 101 / 360) loss: 3.021445\n",
      "(Epoch 3 / 10) train acc: 0.187000; val_acc: 0.198333\n",
      "(Epoch 4 / 10) train acc: 0.173000; val_acc: 0.198333\n",
      "(Epoch 5 / 10) train acc: 0.192000; val_acc: 0.198333\n",
      "(Iteration 201 / 360) loss: 2.435630\n",
      "(Epoch 6 / 10) train acc: 0.166000; val_acc: 0.198333\n",
      "(Epoch 7 / 10) train acc: 0.195000; val_acc: 0.198333\n",
      "(Epoch 8 / 10) train acc: 0.177000; val_acc: 0.198333\n",
      "(Iteration 301 / 360) loss: 3.610810\n",
      "(Epoch 9 / 10) train acc: 0.183000; val_acc: 0.198333\n",
      "(Epoch 10 / 10) train acc: 0.176000; val_acc: 0.198333\n",
      "(Iteration 1 / 320) loss: 47.267379\n",
      "(Epoch 0 / 10) train acc: 0.198000; val_acc: 0.219167\n",
      "(Epoch 1 / 10) train acc: 0.204000; val_acc: 0.207500\n",
      "(Epoch 2 / 10) train acc: 0.170000; val_acc: 0.203333\n",
      "(Epoch 3 / 10) train acc: 0.183000; val_acc: 0.203333\n",
      "(Iteration 101 / 320) loss: 2.370081\n",
      "(Epoch 4 / 10) train acc: 0.172000; val_acc: 0.203333\n",
      "(Epoch 5 / 10) train acc: 0.181000; val_acc: 0.203333\n",
      "(Epoch 6 / 10) train acc: 0.187000; val_acc: 0.203333\n",
      "(Iteration 201 / 320) loss: 2.466898\n",
      "(Epoch 7 / 10) train acc: 0.183000; val_acc: 0.203333\n",
      "(Epoch 8 / 10) train acc: 0.169000; val_acc: 0.203333\n",
      "(Epoch 9 / 10) train acc: 0.176000; val_acc: 0.203333\n",
      "(Iteration 301 / 320) loss: 3.867856\n",
      "(Epoch 10 / 10) train acc: 0.170000; val_acc: 0.203333\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(55)\n",
    "accu3 = []\n",
    "for i in range(1,12):\n",
    "    data = {\n",
    "          'X_train': feat_train,\n",
    "          'y_train': label_train,\n",
    "          'X_val': feat_val,\n",
    "          'y_val': label_val}\n",
    "\n",
    "    # TODO: fill out the hyperparamets\n",
    "    hyperparams = {'lr_decay': 0.1,\n",
    "               'num_epochs': 10,\n",
    "               'batch_size': 10*i,\n",
    "               'learning_rate': 10**-4\n",
    "              }\n",
    "\n",
    "    # TODO: fill out the number of units in your hidden layers\n",
    "    hidden_dim = [10] # this should be a list of units for each hiddent layer\n",
    "\n",
    "    model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim)\n",
    "    solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=100)\n",
    "    np.random.seed(55)\n",
    "    solver.train()\n",
    "    accu3.append(solver.best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 0.22\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAD8CAYAAAB3u9PLAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwP8DrpbWdO4kjyQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89b2c38748>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(1,12), accu3)\n",
    "print(range(1,12)[np.argmax(accu3)],np.max(accu3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#problem 2.3\n",
    "hidden_dim1 = [10]\n",
    "hidden_dim2 = [10,10]\n",
    "hidden_dim3 = [10,10,10]\n",
    "hidden_dim4 = [10,10,10,10]\n",
    "\n",
    "hidden_dims = {'h1':hidden_dim1,\n",
    "              'h2':hidden_dim2,\n",
    "              'h3':hidden_dim3,\n",
    "              'h4':hidden_dim4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 3600) loss: 291.549635\n",
      "(Epoch 0 / 100) train acc: 0.106000; val_acc: 0.125000\n",
      "(Epoch 1 / 100) train acc: 0.126000; val_acc: 0.124167\n",
      "(Epoch 2 / 100) train acc: 0.127000; val_acc: 0.124167\n",
      "(Iteration 101 / 3600) loss: 2.079445\n",
      "(Epoch 3 / 100) train acc: 0.126000; val_acc: 0.124167\n",
      "(Epoch 4 / 100) train acc: 0.115000; val_acc: 0.124167\n",
      "(Epoch 5 / 100) train acc: 0.130000; val_acc: 0.124167\n",
      "(Iteration 201 / 3600) loss: 2.058717\n",
      "(Epoch 6 / 100) train acc: 0.109000; val_acc: 0.124167\n",
      "(Epoch 7 / 100) train acc: 0.126000; val_acc: 0.124167\n",
      "(Epoch 8 / 100) train acc: 0.124000; val_acc: 0.124167\n",
      "(Iteration 301 / 3600) loss: 2.079361\n",
      "(Epoch 9 / 100) train acc: 0.131000; val_acc: 0.124167\n",
      "(Epoch 10 / 100) train acc: 0.116000; val_acc: 0.124167\n",
      "(Epoch 11 / 100) train acc: 0.130000; val_acc: 0.124167\n",
      "(Iteration 401 / 3600) loss: 2.079506\n",
      "(Epoch 12 / 100) train acc: 0.129000; val_acc: 0.124167\n",
      "(Epoch 13 / 100) train acc: 0.115000; val_acc: 0.124167\n",
      "(Iteration 501 / 3600) loss: 2.079404\n",
      "(Epoch 14 / 100) train acc: 0.141000; val_acc: 0.124167\n",
      "(Epoch 15 / 100) train acc: 0.130000; val_acc: 0.124167\n",
      "(Epoch 16 / 100) train acc: 0.085000; val_acc: 0.124167\n",
      "(Iteration 601 / 3600) loss: 2.079347\n",
      "(Epoch 17 / 100) train acc: 0.124000; val_acc: 0.124167\n",
      "(Epoch 18 / 100) train acc: 0.149000; val_acc: 0.124167\n",
      "(Epoch 19 / 100) train acc: 0.122000; val_acc: 0.124167\n",
      "(Iteration 701 / 3600) loss: 2.079283\n",
      "(Epoch 20 / 100) train acc: 0.134000; val_acc: 0.124167\n",
      "(Epoch 21 / 100) train acc: 0.113000; val_acc: 0.124167\n",
      "(Epoch 22 / 100) train acc: 0.129000; val_acc: 0.124167\n",
      "(Iteration 801 / 3600) loss: 2.079344\n",
      "(Epoch 23 / 100) train acc: 0.122000; val_acc: 0.124167\n",
      "(Epoch 24 / 100) train acc: 0.128000; val_acc: 0.124167\n",
      "(Epoch 25 / 100) train acc: 0.093000; val_acc: 0.124167\n",
      "(Iteration 901 / 3600) loss: 2.079485\n",
      "(Epoch 26 / 100) train acc: 0.128000; val_acc: 0.124167\n",
      "(Epoch 27 / 100) train acc: 0.108000; val_acc: 0.124167\n",
      "(Iteration 1001 / 3600) loss: 2.079464\n",
      "(Epoch 28 / 100) train acc: 0.125000; val_acc: 0.124167\n",
      "(Epoch 29 / 100) train acc: 0.117000; val_acc: 0.124167\n",
      "(Epoch 30 / 100) train acc: 0.117000; val_acc: 0.124167\n",
      "(Iteration 1101 / 3600) loss: 2.079301\n",
      "(Epoch 31 / 100) train acc: 0.134000; val_acc: 0.124167\n",
      "(Epoch 32 / 100) train acc: 0.121000; val_acc: 0.124167\n",
      "(Epoch 33 / 100) train acc: 0.129000; val_acc: 0.124167\n",
      "(Iteration 1201 / 3600) loss: 2.079408\n",
      "(Epoch 34 / 100) train acc: 0.116000; val_acc: 0.124167\n",
      "(Epoch 35 / 100) train acc: 0.128000; val_acc: 0.124167\n",
      "(Epoch 36 / 100) train acc: 0.132000; val_acc: 0.124167\n",
      "(Iteration 1301 / 3600) loss: 2.079519\n",
      "(Epoch 37 / 100) train acc: 0.109000; val_acc: 0.124167\n",
      "(Epoch 38 / 100) train acc: 0.152000; val_acc: 0.124167\n",
      "(Iteration 1401 / 3600) loss: 2.079528\n",
      "(Epoch 39 / 100) train acc: 0.106000; val_acc: 0.124167\n",
      "(Epoch 40 / 100) train acc: 0.114000; val_acc: 0.124167\n",
      "(Epoch 41 / 100) train acc: 0.120000; val_acc: 0.124167\n",
      "(Iteration 1501 / 3600) loss: 2.079497\n",
      "(Epoch 42 / 100) train acc: 0.122000; val_acc: 0.124167\n",
      "(Epoch 43 / 100) train acc: 0.120000; val_acc: 0.124167\n",
      "(Epoch 44 / 100) train acc: 0.126000; val_acc: 0.124167\n",
      "(Iteration 1601 / 3600) loss: 2.079399\n",
      "(Epoch 45 / 100) train acc: 0.129000; val_acc: 0.124167\n",
      "(Epoch 46 / 100) train acc: 0.148000; val_acc: 0.124167\n",
      "(Epoch 47 / 100) train acc: 0.120000; val_acc: 0.124167\n",
      "(Iteration 1701 / 3600) loss: 2.079525\n",
      "(Epoch 48 / 100) train acc: 0.110000; val_acc: 0.124167\n",
      "(Epoch 49 / 100) train acc: 0.137000; val_acc: 0.124167\n",
      "(Epoch 50 / 100) train acc: 0.129000; val_acc: 0.124167\n",
      "(Iteration 1801 / 3600) loss: 2.079460\n",
      "(Epoch 51 / 100) train acc: 0.138000; val_acc: 0.124167\n",
      "(Epoch 52 / 100) train acc: 0.127000; val_acc: 0.124167\n",
      "(Iteration 1901 / 3600) loss: 2.079554\n",
      "(Epoch 53 / 100) train acc: 0.105000; val_acc: 0.124167\n",
      "(Epoch 54 / 100) train acc: 0.153000; val_acc: 0.124167\n",
      "(Epoch 55 / 100) train acc: 0.133000; val_acc: 0.124167\n",
      "(Iteration 2001 / 3600) loss: 2.079352\n",
      "(Epoch 56 / 100) train acc: 0.142000; val_acc: 0.124167\n",
      "(Epoch 57 / 100) train acc: 0.121000; val_acc: 0.124167\n",
      "(Epoch 58 / 100) train acc: 0.109000; val_acc: 0.124167\n",
      "(Iteration 2101 / 3600) loss: 2.079473\n",
      "(Epoch 59 / 100) train acc: 0.137000; val_acc: 0.124167\n",
      "(Epoch 60 / 100) train acc: 0.128000; val_acc: 0.124167\n",
      "(Epoch 61 / 100) train acc: 0.128000; val_acc: 0.124167\n",
      "(Iteration 2201 / 3600) loss: 2.079469\n",
      "(Epoch 62 / 100) train acc: 0.120000; val_acc: 0.124167\n",
      "(Epoch 63 / 100) train acc: 0.120000; val_acc: 0.124167\n",
      "(Iteration 2301 / 3600) loss: 2.079799\n",
      "(Epoch 64 / 100) train acc: 0.136000; val_acc: 0.124167\n",
      "(Epoch 65 / 100) train acc: 0.122000; val_acc: 0.124167\n",
      "(Epoch 66 / 100) train acc: 0.114000; val_acc: 0.124167\n",
      "(Iteration 2401 / 3600) loss: 2.079548\n",
      "(Epoch 67 / 100) train acc: 0.136000; val_acc: 0.124167\n",
      "(Epoch 68 / 100) train acc: 0.136000; val_acc: 0.124167\n",
      "(Epoch 69 / 100) train acc: 0.121000; val_acc: 0.124167\n",
      "(Iteration 2501 / 3600) loss: 2.079598\n",
      "(Epoch 70 / 100) train acc: 0.132000; val_acc: 0.124167\n",
      "(Epoch 71 / 100) train acc: 0.135000; val_acc: 0.124167\n",
      "(Epoch 72 / 100) train acc: 0.127000; val_acc: 0.124167\n",
      "(Iteration 2601 / 3600) loss: 2.079346\n",
      "(Epoch 73 / 100) train acc: 0.126000; val_acc: 0.124167\n",
      "(Epoch 74 / 100) train acc: 0.127000; val_acc: 0.124167\n",
      "(Epoch 75 / 100) train acc: 0.109000; val_acc: 0.124167\n",
      "(Iteration 2701 / 3600) loss: 2.079454\n",
      "(Epoch 76 / 100) train acc: 0.140000; val_acc: 0.124167\n",
      "(Epoch 77 / 100) train acc: 0.133000; val_acc: 0.124167\n",
      "(Iteration 2801 / 3600) loss: 2.079537\n",
      "(Epoch 78 / 100) train acc: 0.138000; val_acc: 0.124167\n",
      "(Epoch 79 / 100) train acc: 0.126000; val_acc: 0.124167\n",
      "(Epoch 80 / 100) train acc: 0.131000; val_acc: 0.124167\n",
      "(Iteration 2901 / 3600) loss: 2.079478\n",
      "(Epoch 81 / 100) train acc: 0.113000; val_acc: 0.124167\n",
      "(Epoch 82 / 100) train acc: 0.138000; val_acc: 0.124167\n",
      "(Epoch 83 / 100) train acc: 0.123000; val_acc: 0.124167\n",
      "(Iteration 3001 / 3600) loss: 2.079350\n",
      "(Epoch 84 / 100) train acc: 0.129000; val_acc: 0.124167\n",
      "(Epoch 85 / 100) train acc: 0.108000; val_acc: 0.124167\n",
      "(Epoch 86 / 100) train acc: 0.108000; val_acc: 0.124167\n",
      "(Iteration 3101 / 3600) loss: 2.079503\n",
      "(Epoch 87 / 100) train acc: 0.133000; val_acc: 0.124167\n",
      "(Epoch 88 / 100) train acc: 0.120000; val_acc: 0.124167\n",
      "(Iteration 3201 / 3600) loss: 2.079209\n",
      "(Epoch 89 / 100) train acc: 0.118000; val_acc: 0.124167\n",
      "(Epoch 90 / 100) train acc: 0.118000; val_acc: 0.124167\n",
      "(Epoch 91 / 100) train acc: 0.102000; val_acc: 0.124167\n",
      "(Iteration 3301 / 3600) loss: 2.079111\n",
      "(Epoch 92 / 100) train acc: 0.115000; val_acc: 0.124167\n",
      "(Epoch 93 / 100) train acc: 0.122000; val_acc: 0.124167\n",
      "(Epoch 94 / 100) train acc: 0.126000; val_acc: 0.124167\n",
      "(Iteration 3401 / 3600) loss: 2.079529\n",
      "(Epoch 95 / 100) train acc: 0.125000; val_acc: 0.124167\n",
      "(Epoch 96 / 100) train acc: 0.119000; val_acc: 0.124167\n",
      "(Epoch 97 / 100) train acc: 0.137000; val_acc: 0.124167\n",
      "(Iteration 3501 / 3600) loss: 2.079389\n",
      "(Epoch 98 / 100) train acc: 0.117000; val_acc: 0.124167\n",
      "(Epoch 99 / 100) train acc: 0.136000; val_acc: 0.124167\n",
      "(Epoch 100 / 100) train acc: 0.119000; val_acc: 0.124167\n",
      "(Iteration 1 / 3600) loss: 27.854013\n",
      "(Epoch 0 / 100) train acc: 0.130000; val_acc: 0.123333\n",
      "(Epoch 1 / 100) train acc: 0.124000; val_acc: 0.125000\n",
      "(Epoch 2 / 100) train acc: 0.133000; val_acc: 0.127500\n",
      "(Iteration 101 / 3600) loss: 2.074771\n",
      "(Epoch 3 / 100) train acc: 0.136000; val_acc: 0.127500\n",
      "(Epoch 4 / 100) train acc: 0.122000; val_acc: 0.125833\n",
      "(Epoch 5 / 100) train acc: 0.137000; val_acc: 0.125000\n",
      "(Iteration 201 / 3600) loss: 2.090379\n",
      "(Epoch 6 / 100) train acc: 0.109000; val_acc: 0.125833\n",
      "(Epoch 7 / 100) train acc: 0.140000; val_acc: 0.127500\n",
      "(Epoch 8 / 100) train acc: 0.127000; val_acc: 0.127500\n",
      "(Iteration 301 / 3600) loss: 2.044232\n",
      "(Epoch 9 / 100) train acc: 0.125000; val_acc: 0.130833\n",
      "(Epoch 10 / 100) train acc: 0.128000; val_acc: 0.133333\n",
      "(Epoch 11 / 100) train acc: 0.158000; val_acc: 0.160833\n",
      "(Iteration 401 / 3600) loss: 1.896432\n",
      "(Epoch 12 / 100) train acc: 0.238000; val_acc: 0.239167\n",
      "(Epoch 13 / 100) train acc: 0.213000; val_acc: 0.238333\n",
      "(Iteration 501 / 3600) loss: 1.740720\n",
      "(Epoch 14 / 100) train acc: 0.224000; val_acc: 0.242500\n",
      "(Epoch 15 / 100) train acc: 0.254000; val_acc: 0.240000\n",
      "(Epoch 16 / 100) train acc: 0.263000; val_acc: 0.241667\n",
      "(Iteration 601 / 3600) loss: 1.756180\n",
      "(Epoch 17 / 100) train acc: 0.236000; val_acc: 0.242500\n",
      "(Epoch 18 / 100) train acc: 0.247000; val_acc: 0.244167\n",
      "(Epoch 19 / 100) train acc: 0.249000; val_acc: 0.243333\n",
      "(Iteration 701 / 3600) loss: 1.617353\n",
      "(Epoch 20 / 100) train acc: 0.240000; val_acc: 0.240000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 21 / 100) train acc: 0.249000; val_acc: 0.245000\n",
      "(Epoch 22 / 100) train acc: 0.244000; val_acc: 0.245000\n",
      "(Iteration 801 / 3600) loss: 1.805670\n",
      "(Epoch 23 / 100) train acc: 0.250000; val_acc: 0.240833\n",
      "(Epoch 24 / 100) train acc: 0.251000; val_acc: 0.246667\n",
      "(Epoch 25 / 100) train acc: 0.245000; val_acc: 0.229167\n",
      "(Iteration 901 / 3600) loss: 1.763326\n",
      "(Epoch 26 / 100) train acc: 0.254000; val_acc: 0.245000\n",
      "(Epoch 27 / 100) train acc: 0.244000; val_acc: 0.245000\n",
      "(Iteration 1001 / 3600) loss: 1.739349\n",
      "(Epoch 28 / 100) train acc: 0.240000; val_acc: 0.245000\n",
      "(Epoch 29 / 100) train acc: 0.243000; val_acc: 0.248333\n",
      "(Epoch 30 / 100) train acc: 0.234000; val_acc: 0.246667\n",
      "(Iteration 1101 / 3600) loss: 1.743545\n",
      "(Epoch 31 / 100) train acc: 0.261000; val_acc: 0.252500\n",
      "(Epoch 32 / 100) train acc: 0.242000; val_acc: 0.246667\n",
      "(Epoch 33 / 100) train acc: 0.238000; val_acc: 0.248333\n",
      "(Iteration 1201 / 3600) loss: 1.856513\n",
      "(Epoch 34 / 100) train acc: 0.249000; val_acc: 0.250000\n",
      "(Epoch 35 / 100) train acc: 0.249000; val_acc: 0.250833\n",
      "(Epoch 36 / 100) train acc: 0.254000; val_acc: 0.249167\n",
      "(Iteration 1301 / 3600) loss: 1.753192\n",
      "(Epoch 37 / 100) train acc: 0.263000; val_acc: 0.253333\n",
      "(Epoch 38 / 100) train acc: 0.272000; val_acc: 0.255000\n",
      "(Iteration 1401 / 3600) loss: 1.730041\n",
      "(Epoch 39 / 100) train acc: 0.261000; val_acc: 0.235833\n",
      "(Epoch 40 / 100) train acc: 0.234000; val_acc: 0.258333\n",
      "(Epoch 41 / 100) train acc: 0.247000; val_acc: 0.255833\n",
      "(Iteration 1501 / 3600) loss: 1.702408\n",
      "(Epoch 42 / 100) train acc: 0.274000; val_acc: 0.255000\n",
      "(Epoch 43 / 100) train acc: 0.237000; val_acc: 0.260000\n",
      "(Epoch 44 / 100) train acc: 0.244000; val_acc: 0.237500\n",
      "(Iteration 1601 / 3600) loss: 1.635687\n",
      "(Epoch 45 / 100) train acc: 0.243000; val_acc: 0.258333\n",
      "(Epoch 46 / 100) train acc: 0.274000; val_acc: 0.255833\n",
      "(Epoch 47 / 100) train acc: 0.253000; val_acc: 0.259167\n",
      "(Iteration 1701 / 3600) loss: 1.692980\n",
      "(Epoch 48 / 100) train acc: 0.277000; val_acc: 0.260000\n",
      "(Epoch 49 / 100) train acc: 0.275000; val_acc: 0.261667\n",
      "(Epoch 50 / 100) train acc: 0.263000; val_acc: 0.251667\n",
      "(Iteration 1801 / 3600) loss: 1.686789\n",
      "(Epoch 51 / 100) train acc: 0.282000; val_acc: 0.265000\n",
      "(Epoch 52 / 100) train acc: 0.252000; val_acc: 0.258333\n",
      "(Iteration 1901 / 3600) loss: 1.738030\n",
      "(Epoch 53 / 100) train acc: 0.216000; val_acc: 0.263333\n",
      "(Epoch 54 / 100) train acc: 0.253000; val_acc: 0.257500\n",
      "(Epoch 55 / 100) train acc: 0.264000; val_acc: 0.259167\n",
      "(Iteration 2001 / 3600) loss: 1.725927\n",
      "(Epoch 56 / 100) train acc: 0.249000; val_acc: 0.251667\n",
      "(Epoch 57 / 100) train acc: 0.270000; val_acc: 0.259167\n",
      "(Epoch 58 / 100) train acc: 0.238000; val_acc: 0.266667\n",
      "(Iteration 2101 / 3600) loss: 1.737156\n",
      "(Epoch 59 / 100) train acc: 0.274000; val_acc: 0.270833\n",
      "(Epoch 60 / 100) train acc: 0.258000; val_acc: 0.270000\n",
      "(Epoch 61 / 100) train acc: 0.260000; val_acc: 0.254167\n",
      "(Iteration 2201 / 3600) loss: 1.676705\n",
      "(Epoch 62 / 100) train acc: 0.274000; val_acc: 0.266667\n",
      "(Epoch 63 / 100) train acc: 0.275000; val_acc: 0.289167\n",
      "(Iteration 2301 / 3600) loss: 1.762401\n",
      "(Epoch 64 / 100) train acc: 0.329000; val_acc: 0.311667\n",
      "(Epoch 65 / 100) train acc: 0.288000; val_acc: 0.281667\n",
      "(Epoch 66 / 100) train acc: 0.249000; val_acc: 0.276667\n",
      "(Iteration 2401 / 3600) loss: 1.737811\n",
      "(Epoch 67 / 100) train acc: 0.294000; val_acc: 0.284167\n",
      "(Epoch 68 / 100) train acc: 0.250000; val_acc: 0.255000\n",
      "(Epoch 69 / 100) train acc: 0.274000; val_acc: 0.279167\n",
      "(Iteration 2501 / 3600) loss: 1.640285\n",
      "(Epoch 70 / 100) train acc: 0.287000; val_acc: 0.276667\n",
      "(Epoch 71 / 100) train acc: 0.284000; val_acc: 0.280000\n",
      "(Epoch 72 / 100) train acc: 0.274000; val_acc: 0.276667\n",
      "(Iteration 2601 / 3600) loss: 1.598181\n",
      "(Epoch 73 / 100) train acc: 0.257000; val_acc: 0.276667\n",
      "(Epoch 74 / 100) train acc: 0.260000; val_acc: 0.263333\n",
      "(Epoch 75 / 100) train acc: 0.269000; val_acc: 0.272500\n",
      "(Iteration 2701 / 3600) loss: 1.606668\n",
      "(Epoch 76 / 100) train acc: 0.286000; val_acc: 0.263333\n",
      "(Epoch 77 / 100) train acc: 0.268000; val_acc: 0.270833\n",
      "(Iteration 2801 / 3600) loss: 1.702946\n",
      "(Epoch 78 / 100) train acc: 0.274000; val_acc: 0.303333\n",
      "(Epoch 79 / 100) train acc: 0.277000; val_acc: 0.273333\n",
      "(Epoch 80 / 100) train acc: 0.306000; val_acc: 0.308333\n",
      "(Iteration 2901 / 3600) loss: 1.632372\n",
      "(Epoch 81 / 100) train acc: 0.290000; val_acc: 0.275000\n",
      "(Epoch 82 / 100) train acc: 0.291000; val_acc: 0.272500\n",
      "(Epoch 83 / 100) train acc: 0.287000; val_acc: 0.306667\n",
      "(Iteration 3001 / 3600) loss: 1.689656\n",
      "(Epoch 84 / 100) train acc: 0.277000; val_acc: 0.275000\n",
      "(Epoch 85 / 100) train acc: 0.261000; val_acc: 0.283333\n",
      "(Epoch 86 / 100) train acc: 0.258000; val_acc: 0.305833\n",
      "(Iteration 3101 / 3600) loss: 1.655072\n",
      "(Epoch 87 / 100) train acc: 0.293000; val_acc: 0.277500\n",
      "(Epoch 88 / 100) train acc: 0.263000; val_acc: 0.269167\n",
      "(Iteration 3201 / 3600) loss: 1.558595\n",
      "(Epoch 89 / 100) train acc: 0.266000; val_acc: 0.284167\n",
      "(Epoch 90 / 100) train acc: 0.315000; val_acc: 0.308333\n",
      "(Epoch 91 / 100) train acc: 0.260000; val_acc: 0.285000\n",
      "(Iteration 3301 / 3600) loss: 1.568923\n",
      "(Epoch 92 / 100) train acc: 0.254000; val_acc: 0.280000\n",
      "(Epoch 93 / 100) train acc: 0.289000; val_acc: 0.282500\n",
      "(Epoch 94 / 100) train acc: 0.284000; val_acc: 0.276667\n",
      "(Iteration 3401 / 3600) loss: 1.669391\n",
      "(Epoch 95 / 100) train acc: 0.298000; val_acc: 0.285833\n",
      "(Epoch 96 / 100) train acc: 0.328000; val_acc: 0.311667\n",
      "(Epoch 97 / 100) train acc: 0.297000; val_acc: 0.276667\n",
      "(Iteration 3501 / 3600) loss: 1.656661\n",
      "(Epoch 98 / 100) train acc: 0.303000; val_acc: 0.290833\n",
      "(Epoch 99 / 100) train acc: 0.318000; val_acc: 0.315833\n",
      "(Epoch 100 / 100) train acc: 0.306000; val_acc: 0.290000\n",
      "(Iteration 1 / 3600) loss: 7.891556\n",
      "(Epoch 0 / 100) train acc: 0.094000; val_acc: 0.070000\n",
      "(Epoch 1 / 100) train acc: 0.177000; val_acc: 0.175000\n",
      "(Epoch 2 / 100) train acc: 0.189000; val_acc: 0.185000\n",
      "(Iteration 101 / 3600) loss: 1.945831\n",
      "(Epoch 3 / 100) train acc: 0.186000; val_acc: 0.197500\n",
      "(Epoch 4 / 100) train acc: 0.246000; val_acc: 0.219167\n",
      "(Epoch 5 / 100) train acc: 0.256000; val_acc: 0.259167\n",
      "(Iteration 201 / 3600) loss: 1.628567\n",
      "(Epoch 6 / 100) train acc: 0.304000; val_acc: 0.292500\n",
      "(Epoch 7 / 100) train acc: 0.352000; val_acc: 0.339167\n",
      "(Epoch 8 / 100) train acc: 0.439000; val_acc: 0.436667\n",
      "(Iteration 301 / 3600) loss: 1.437798\n",
      "(Epoch 9 / 100) train acc: 0.477000; val_acc: 0.453333\n",
      "(Epoch 10 / 100) train acc: 0.513000; val_acc: 0.477500\n",
      "(Epoch 11 / 100) train acc: 0.519000; val_acc: 0.507500\n",
      "(Iteration 401 / 3600) loss: 1.286853\n",
      "(Epoch 12 / 100) train acc: 0.583000; val_acc: 0.525833\n",
      "(Epoch 13 / 100) train acc: 0.576000; val_acc: 0.535833\n",
      "(Iteration 501 / 3600) loss: 1.112080\n",
      "(Epoch 14 / 100) train acc: 0.561000; val_acc: 0.535000\n",
      "(Epoch 15 / 100) train acc: 0.591000; val_acc: 0.560833\n",
      "(Epoch 16 / 100) train acc: 0.628000; val_acc: 0.578333\n",
      "(Iteration 601 / 3600) loss: 1.112292\n",
      "(Epoch 17 / 100) train acc: 0.621000; val_acc: 0.589167\n",
      "(Epoch 18 / 100) train acc: 0.602000; val_acc: 0.563333\n",
      "(Epoch 19 / 100) train acc: 0.628000; val_acc: 0.601667\n",
      "(Iteration 701 / 3600) loss: 0.848904\n",
      "(Epoch 20 / 100) train acc: 0.629000; val_acc: 0.622500\n",
      "(Epoch 21 / 100) train acc: 0.643000; val_acc: 0.618333\n",
      "(Epoch 22 / 100) train acc: 0.677000; val_acc: 0.634167\n",
      "(Iteration 801 / 3600) loss: 0.692289\n",
      "(Epoch 23 / 100) train acc: 0.623000; val_acc: 0.615833\n",
      "(Epoch 24 / 100) train acc: 0.640000; val_acc: 0.630000\n",
      "(Epoch 25 / 100) train acc: 0.648000; val_acc: 0.613333\n",
      "(Iteration 901 / 3600) loss: 0.880185\n",
      "(Epoch 26 / 100) train acc: 0.642000; val_acc: 0.658333\n",
      "(Epoch 27 / 100) train acc: 0.660000; val_acc: 0.672500\n",
      "(Iteration 1001 / 3600) loss: 0.817829\n",
      "(Epoch 28 / 100) train acc: 0.684000; val_acc: 0.662500\n",
      "(Epoch 29 / 100) train acc: 0.675000; val_acc: 0.665000\n",
      "(Epoch 30 / 100) train acc: 0.653000; val_acc: 0.662500\n",
      "(Iteration 1101 / 3600) loss: 0.897511\n",
      "(Epoch 31 / 100) train acc: 0.715000; val_acc: 0.683333\n",
      "(Epoch 32 / 100) train acc: 0.697000; val_acc: 0.671667\n",
      "(Epoch 33 / 100) train acc: 0.679000; val_acc: 0.668333\n",
      "(Iteration 1201 / 3600) loss: 0.838470\n",
      "(Epoch 34 / 100) train acc: 0.730000; val_acc: 0.680833\n",
      "(Epoch 35 / 100) train acc: 0.694000; val_acc: 0.694167\n",
      "(Epoch 36 / 100) train acc: 0.743000; val_acc: 0.699167\n",
      "(Iteration 1301 / 3600) loss: 0.887838\n",
      "(Epoch 37 / 100) train acc: 0.723000; val_acc: 0.717500\n",
      "(Epoch 38 / 100) train acc: 0.690000; val_acc: 0.700833\n",
      "(Iteration 1401 / 3600) loss: 0.875819\n",
      "(Epoch 39 / 100) train acc: 0.724000; val_acc: 0.684167\n",
      "(Epoch 40 / 100) train acc: 0.741000; val_acc: 0.715000\n",
      "(Epoch 41 / 100) train acc: 0.749000; val_acc: 0.720833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1501 / 3600) loss: 0.644045\n",
      "(Epoch 42 / 100) train acc: 0.730000; val_acc: 0.730000\n",
      "(Epoch 43 / 100) train acc: 0.742000; val_acc: 0.725833\n",
      "(Epoch 44 / 100) train acc: 0.766000; val_acc: 0.732500\n",
      "(Iteration 1601 / 3600) loss: 0.753686\n",
      "(Epoch 45 / 100) train acc: 0.781000; val_acc: 0.741667\n",
      "(Epoch 46 / 100) train acc: 0.782000; val_acc: 0.740000\n",
      "(Epoch 47 / 100) train acc: 0.770000; val_acc: 0.765833\n",
      "(Iteration 1701 / 3600) loss: 0.806821\n",
      "(Epoch 48 / 100) train acc: 0.808000; val_acc: 0.770000\n",
      "(Epoch 49 / 100) train acc: 0.770000; val_acc: 0.760833\n",
      "(Epoch 50 / 100) train acc: 0.750000; val_acc: 0.736667\n",
      "(Iteration 1801 / 3600) loss: 0.538316\n",
      "(Epoch 51 / 100) train acc: 0.761000; val_acc: 0.747500\n",
      "(Epoch 52 / 100) train acc: 0.783000; val_acc: 0.772500\n",
      "(Iteration 1901 / 3600) loss: 0.672998\n",
      "(Epoch 53 / 100) train acc: 0.800000; val_acc: 0.778333\n",
      "(Epoch 54 / 100) train acc: 0.800000; val_acc: 0.762500\n",
      "(Epoch 55 / 100) train acc: 0.790000; val_acc: 0.768333\n",
      "(Iteration 2001 / 3600) loss: 0.593659\n",
      "(Epoch 56 / 100) train acc: 0.772000; val_acc: 0.765000\n",
      "(Epoch 57 / 100) train acc: 0.815000; val_acc: 0.779167\n",
      "(Epoch 58 / 100) train acc: 0.828000; val_acc: 0.795000\n",
      "(Iteration 2101 / 3600) loss: 0.530597\n",
      "(Epoch 59 / 100) train acc: 0.823000; val_acc: 0.778333\n",
      "(Epoch 60 / 100) train acc: 0.832000; val_acc: 0.790833\n",
      "(Epoch 61 / 100) train acc: 0.833000; val_acc: 0.785833\n",
      "(Iteration 2201 / 3600) loss: 0.555759\n",
      "(Epoch 62 / 100) train acc: 0.836000; val_acc: 0.785833\n",
      "(Epoch 63 / 100) train acc: 0.793000; val_acc: 0.798333\n",
      "(Iteration 2301 / 3600) loss: 0.806285\n",
      "(Epoch 64 / 100) train acc: 0.827000; val_acc: 0.795833\n",
      "(Epoch 65 / 100) train acc: 0.817000; val_acc: 0.797500\n",
      "(Epoch 66 / 100) train acc: 0.834000; val_acc: 0.796667\n",
      "(Iteration 2401 / 3600) loss: 0.649041\n",
      "(Epoch 67 / 100) train acc: 0.836000; val_acc: 0.808333\n",
      "(Epoch 68 / 100) train acc: 0.803000; val_acc: 0.784167\n",
      "(Epoch 69 / 100) train acc: 0.830000; val_acc: 0.810833\n",
      "(Iteration 2501 / 3600) loss: 0.425305\n",
      "(Epoch 70 / 100) train acc: 0.848000; val_acc: 0.802500\n",
      "(Epoch 71 / 100) train acc: 0.852000; val_acc: 0.792500\n",
      "(Epoch 72 / 100) train acc: 0.824000; val_acc: 0.806667\n",
      "(Iteration 2601 / 3600) loss: 0.596309\n",
      "(Epoch 73 / 100) train acc: 0.828000; val_acc: 0.811667\n",
      "(Epoch 74 / 100) train acc: 0.842000; val_acc: 0.802500\n",
      "(Epoch 75 / 100) train acc: 0.825000; val_acc: 0.802500\n",
      "(Iteration 2701 / 3600) loss: 0.617672\n",
      "(Epoch 76 / 100) train acc: 0.832000; val_acc: 0.805833\n",
      "(Epoch 77 / 100) train acc: 0.860000; val_acc: 0.819167\n",
      "(Iteration 2801 / 3600) loss: 0.564231\n",
      "(Epoch 78 / 100) train acc: 0.838000; val_acc: 0.811667\n",
      "(Epoch 79 / 100) train acc: 0.820000; val_acc: 0.803333\n",
      "(Epoch 80 / 100) train acc: 0.853000; val_acc: 0.790000\n",
      "(Iteration 2901 / 3600) loss: 0.521932\n",
      "(Epoch 81 / 100) train acc: 0.833000; val_acc: 0.816667\n",
      "(Epoch 82 / 100) train acc: 0.849000; val_acc: 0.816667\n",
      "(Epoch 83 / 100) train acc: 0.853000; val_acc: 0.815000\n",
      "(Iteration 3001 / 3600) loss: 0.591326\n",
      "(Epoch 84 / 100) train acc: 0.853000; val_acc: 0.818333\n",
      "(Epoch 85 / 100) train acc: 0.879000; val_acc: 0.815000\n",
      "(Epoch 86 / 100) train acc: 0.874000; val_acc: 0.825833\n",
      "(Iteration 3101 / 3600) loss: 0.483123\n",
      "(Epoch 87 / 100) train acc: 0.844000; val_acc: 0.827500\n",
      "(Epoch 88 / 100) train acc: 0.863000; val_acc: 0.822500\n",
      "(Iteration 3201 / 3600) loss: 0.535909\n",
      "(Epoch 89 / 100) train acc: 0.863000; val_acc: 0.829167\n",
      "(Epoch 90 / 100) train acc: 0.861000; val_acc: 0.815000\n",
      "(Epoch 91 / 100) train acc: 0.880000; val_acc: 0.825833\n",
      "(Iteration 3301 / 3600) loss: 0.445376\n",
      "(Epoch 92 / 100) train acc: 0.861000; val_acc: 0.828333\n",
      "(Epoch 93 / 100) train acc: 0.862000; val_acc: 0.817500\n",
      "(Epoch 94 / 100) train acc: 0.872000; val_acc: 0.838333\n",
      "(Iteration 3401 / 3600) loss: 0.382079\n",
      "(Epoch 95 / 100) train acc: 0.866000; val_acc: 0.833333\n",
      "(Epoch 96 / 100) train acc: 0.886000; val_acc: 0.832500\n",
      "(Epoch 97 / 100) train acc: 0.849000; val_acc: 0.833333\n",
      "(Iteration 3501 / 3600) loss: 0.554476\n",
      "(Epoch 98 / 100) train acc: 0.874000; val_acc: 0.822500\n",
      "(Epoch 99 / 100) train acc: 0.884000; val_acc: 0.839167\n",
      "(Epoch 100 / 100) train acc: 0.867000; val_acc: 0.838333\n",
      "(Iteration 1 / 3600) loss: 2.515492\n",
      "(Epoch 0 / 100) train acc: 0.112000; val_acc: 0.114167\n",
      "(Epoch 1 / 100) train acc: 0.159000; val_acc: 0.141667\n",
      "(Epoch 2 / 100) train acc: 0.184000; val_acc: 0.170000\n",
      "(Iteration 101 / 3600) loss: 2.005521\n",
      "(Epoch 3 / 100) train acc: 0.193000; val_acc: 0.181667\n",
      "(Epoch 4 / 100) train acc: 0.172000; val_acc: 0.169167\n",
      "(Epoch 5 / 100) train acc: 0.173000; val_acc: 0.179167\n",
      "(Iteration 201 / 3600) loss: 1.872828\n",
      "(Epoch 6 / 100) train acc: 0.225000; val_acc: 0.205833\n",
      "(Epoch 7 / 100) train acc: 0.238000; val_acc: 0.215833\n",
      "(Epoch 8 / 100) train acc: 0.265000; val_acc: 0.241667\n",
      "(Iteration 301 / 3600) loss: 1.714323\n",
      "(Epoch 9 / 100) train acc: 0.340000; val_acc: 0.294167\n",
      "(Epoch 10 / 100) train acc: 0.345000; val_acc: 0.319167\n",
      "(Epoch 11 / 100) train acc: 0.367000; val_acc: 0.380000\n",
      "(Iteration 401 / 3600) loss: 1.542148\n",
      "(Epoch 12 / 100) train acc: 0.414000; val_acc: 0.401667\n",
      "(Epoch 13 / 100) train acc: 0.469000; val_acc: 0.415000\n",
      "(Iteration 501 / 3600) loss: 1.383340\n",
      "(Epoch 14 / 100) train acc: 0.419000; val_acc: 0.390000\n",
      "(Epoch 15 / 100) train acc: 0.442000; val_acc: 0.446667\n",
      "(Epoch 16 / 100) train acc: 0.442000; val_acc: 0.464167\n",
      "(Iteration 601 / 3600) loss: 1.302252\n",
      "(Epoch 17 / 100) train acc: 0.524000; val_acc: 0.472500\n",
      "(Epoch 18 / 100) train acc: 0.537000; val_acc: 0.510000\n",
      "(Epoch 19 / 100) train acc: 0.535000; val_acc: 0.503333\n",
      "(Iteration 701 / 3600) loss: 1.223650\n",
      "(Epoch 20 / 100) train acc: 0.552000; val_acc: 0.531667\n",
      "(Epoch 21 / 100) train acc: 0.554000; val_acc: 0.541667\n",
      "(Epoch 22 / 100) train acc: 0.534000; val_acc: 0.530833\n",
      "(Iteration 801 / 3600) loss: 0.991475\n",
      "(Epoch 23 / 100) train acc: 0.545000; val_acc: 0.548333\n",
      "(Epoch 24 / 100) train acc: 0.595000; val_acc: 0.539167\n",
      "(Epoch 25 / 100) train acc: 0.560000; val_acc: 0.522500\n",
      "(Iteration 901 / 3600) loss: 1.074748\n",
      "(Epoch 26 / 100) train acc: 0.586000; val_acc: 0.550000\n",
      "(Epoch 27 / 100) train acc: 0.574000; val_acc: 0.570000\n",
      "(Iteration 1001 / 3600) loss: 1.215037\n",
      "(Epoch 28 / 100) train acc: 0.619000; val_acc: 0.553333\n",
      "(Epoch 29 / 100) train acc: 0.569000; val_acc: 0.585000\n",
      "(Epoch 30 / 100) train acc: 0.626000; val_acc: 0.599167\n",
      "(Iteration 1101 / 3600) loss: 0.992502\n",
      "(Epoch 31 / 100) train acc: 0.612000; val_acc: 0.599167\n",
      "(Epoch 32 / 100) train acc: 0.633000; val_acc: 0.590000\n",
      "(Epoch 33 / 100) train acc: 0.579000; val_acc: 0.600833\n",
      "(Iteration 1201 / 3600) loss: 0.979527\n",
      "(Epoch 34 / 100) train acc: 0.663000; val_acc: 0.609167\n",
      "(Epoch 35 / 100) train acc: 0.637000; val_acc: 0.617500\n",
      "(Epoch 36 / 100) train acc: 0.643000; val_acc: 0.635833\n",
      "(Iteration 1301 / 3600) loss: 0.956445\n",
      "(Epoch 37 / 100) train acc: 0.650000; val_acc: 0.637500\n",
      "(Epoch 38 / 100) train acc: 0.630000; val_acc: 0.620833\n",
      "(Iteration 1401 / 3600) loss: 1.018414\n",
      "(Epoch 39 / 100) train acc: 0.656000; val_acc: 0.622500\n",
      "(Epoch 40 / 100) train acc: 0.664000; val_acc: 0.650000\n",
      "(Epoch 41 / 100) train acc: 0.682000; val_acc: 0.644167\n",
      "(Iteration 1501 / 3600) loss: 0.807714\n",
      "(Epoch 42 / 100) train acc: 0.695000; val_acc: 0.680833\n",
      "(Epoch 43 / 100) train acc: 0.687000; val_acc: 0.650000\n",
      "(Epoch 44 / 100) train acc: 0.705000; val_acc: 0.665000\n",
      "(Iteration 1601 / 3600) loss: 0.847068\n",
      "(Epoch 45 / 100) train acc: 0.681000; val_acc: 0.680833\n",
      "(Epoch 46 / 100) train acc: 0.681000; val_acc: 0.640000\n",
      "(Epoch 47 / 100) train acc: 0.707000; val_acc: 0.708333\n",
      "(Iteration 1701 / 3600) loss: 0.923275\n",
      "(Epoch 48 / 100) train acc: 0.759000; val_acc: 0.725833\n",
      "(Epoch 49 / 100) train acc: 0.753000; val_acc: 0.717500\n",
      "(Epoch 50 / 100) train acc: 0.740000; val_acc: 0.708333\n",
      "(Iteration 1801 / 3600) loss: 0.611995\n",
      "(Epoch 51 / 100) train acc: 0.718000; val_acc: 0.705833\n",
      "(Epoch 52 / 100) train acc: 0.733000; val_acc: 0.732500\n",
      "(Iteration 1901 / 3600) loss: 0.694019\n",
      "(Epoch 53 / 100) train acc: 0.771000; val_acc: 0.732500\n",
      "(Epoch 54 / 100) train acc: 0.764000; val_acc: 0.744167\n",
      "(Epoch 55 / 100) train acc: 0.770000; val_acc: 0.745833\n",
      "(Iteration 2001 / 3600) loss: 0.670652\n",
      "(Epoch 56 / 100) train acc: 0.760000; val_acc: 0.745833\n",
      "(Epoch 57 / 100) train acc: 0.750000; val_acc: 0.725000\n",
      "(Epoch 58 / 100) train acc: 0.790000; val_acc: 0.733333\n",
      "(Iteration 2101 / 3600) loss: 0.579508\n",
      "(Epoch 59 / 100) train acc: 0.614000; val_acc: 0.609167\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 60 / 100) train acc: 0.751000; val_acc: 0.735000\n",
      "(Epoch 61 / 100) train acc: 0.786000; val_acc: 0.757500\n",
      "(Iteration 2201 / 3600) loss: 0.679320\n",
      "(Epoch 62 / 100) train acc: 0.788000; val_acc: 0.765000\n",
      "(Epoch 63 / 100) train acc: 0.796000; val_acc: 0.786667\n",
      "(Iteration 2301 / 3600) loss: 0.761620\n",
      "(Epoch 64 / 100) train acc: 0.805000; val_acc: 0.770833\n",
      "(Epoch 65 / 100) train acc: 0.789000; val_acc: 0.767500\n",
      "(Epoch 66 / 100) train acc: 0.816000; val_acc: 0.787500\n",
      "(Iteration 2401 / 3600) loss: 0.652751\n",
      "(Epoch 67 / 100) train acc: 0.809000; val_acc: 0.763333\n",
      "(Epoch 68 / 100) train acc: 0.801000; val_acc: 0.740833\n",
      "(Epoch 69 / 100) train acc: 0.784000; val_acc: 0.785833\n",
      "(Iteration 2501 / 3600) loss: 0.489437\n",
      "(Epoch 70 / 100) train acc: 0.826000; val_acc: 0.770000\n",
      "(Epoch 71 / 100) train acc: 0.805000; val_acc: 0.761667\n",
      "(Epoch 72 / 100) train acc: 0.770000; val_acc: 0.759167\n",
      "(Iteration 2601 / 3600) loss: 0.610557\n",
      "(Epoch 73 / 100) train acc: 0.783000; val_acc: 0.775833\n",
      "(Epoch 74 / 100) train acc: 0.798000; val_acc: 0.770833\n",
      "(Epoch 75 / 100) train acc: 0.779000; val_acc: 0.743333\n",
      "(Iteration 2701 / 3600) loss: 0.638447\n",
      "(Epoch 76 / 100) train acc: 0.808000; val_acc: 0.777500\n",
      "(Epoch 77 / 100) train acc: 0.833000; val_acc: 0.787500\n",
      "(Iteration 2801 / 3600) loss: 0.601396\n",
      "(Epoch 78 / 100) train acc: 0.799000; val_acc: 0.785000\n",
      "(Epoch 79 / 100) train acc: 0.792000; val_acc: 0.785000\n",
      "(Epoch 80 / 100) train acc: 0.829000; val_acc: 0.784167\n",
      "(Iteration 2901 / 3600) loss: 0.501397\n",
      "(Epoch 81 / 100) train acc: 0.824000; val_acc: 0.785000\n",
      "(Epoch 82 / 100) train acc: 0.798000; val_acc: 0.788333\n",
      "(Epoch 83 / 100) train acc: 0.804000; val_acc: 0.779167\n",
      "(Iteration 3001 / 3600) loss: 0.578744\n",
      "(Epoch 84 / 100) train acc: 0.824000; val_acc: 0.784167\n",
      "(Epoch 85 / 100) train acc: 0.824000; val_acc: 0.795833\n",
      "(Epoch 86 / 100) train acc: 0.821000; val_acc: 0.795833\n",
      "(Iteration 3101 / 3600) loss: 0.523714\n",
      "(Epoch 87 / 100) train acc: 0.799000; val_acc: 0.800000\n",
      "(Epoch 88 / 100) train acc: 0.811000; val_acc: 0.769167\n",
      "(Iteration 3201 / 3600) loss: 0.623719\n",
      "(Epoch 89 / 100) train acc: 0.824000; val_acc: 0.800833\n",
      "(Epoch 90 / 100) train acc: 0.797000; val_acc: 0.795833\n",
      "(Epoch 91 / 100) train acc: 0.861000; val_acc: 0.802500\n",
      "(Iteration 3301 / 3600) loss: 0.444038\n",
      "(Epoch 92 / 100) train acc: 0.824000; val_acc: 0.788333\n",
      "(Epoch 93 / 100) train acc: 0.814000; val_acc: 0.772500\n",
      "(Epoch 94 / 100) train acc: 0.788000; val_acc: 0.794167\n",
      "(Iteration 3401 / 3600) loss: 0.411783\n",
      "(Epoch 95 / 100) train acc: 0.848000; val_acc: 0.804167\n",
      "(Epoch 96 / 100) train acc: 0.839000; val_acc: 0.795000\n",
      "(Epoch 97 / 100) train acc: 0.821000; val_acc: 0.803333\n",
      "(Iteration 3501 / 3600) loss: 0.547178\n",
      "(Epoch 98 / 100) train acc: 0.828000; val_acc: 0.810000\n",
      "(Epoch 99 / 100) train acc: 0.838000; val_acc: 0.807500\n",
      "(Epoch 100 / 100) train acc: 0.805000; val_acc: 0.766667\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(55)\n",
    "accu4 = []\n",
    "data = {\n",
    "      'X_train': feat_train,\n",
    "      'y_train': label_train,\n",
    "      'X_val': feat_val,\n",
    "      'y_val': label_val}\n",
    "\n",
    "# TODO: fill out the hyperparamets\n",
    "hyperparams = {'lr_decay': 1,\n",
    "               'num_epochs': 100,\n",
    "               'batch_size': 100,\n",
    "               'learning_rate': 0.001 \n",
    "              }\n",
    "\n",
    "# TODO: fill out the number of units in your hidden layers\n",
    "# this should be a list of units for each hiddent layer\n",
    "\n",
    "for i in range(4):\n",
    "    hidden_dim = hidden_dims['h%d'%(i+1)]\n",
    "\n",
    "    model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim)\n",
    "    solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=100)\n",
    "    np.random.seed(55)\n",
    "    solver.train()\n",
    "    accu4.append(solver.best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 0.839166666667\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD8CAYAAACMwORRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzt3Xl8VPW9//HXhyxA2JeAStiEoCAiQkStvW6ApVqxV21F\na6333l6qLS5oe396teq17a2tC1pLa6m1tbe1gEtrVFoQl7orUQEFTAh7WAMBEhKyf35/ZKBpSMgh\nTHJmJu/n45EHOTPfTD5fzuQ955w5nzPm7oiISGLpEHYBIiISfQp3EZEEpHAXEUlACncRkQSkcBcR\nSUAKdxGRBKRwFxFJQAp3EZEEpHAXEUlAyWH94r59+/qQIUPC+vUiInHpww8/3Onu6c2NCy3chwwZ\nQk5OTli/XkQkLpnZhiDjdFhGRCQBKdxFRBKQwl1EJAEp3EVEEpDCXUQkASncRUQSkMJdRCQBhXae\nu4i0ro27ynjxky1075RC7y6p9EpLpU/Xun97paWQnKRtu0SmcBdJQGWV1Xzjtx+wbmdpk2N6dK4L\n/QPB37tLCr27dKR3l5R/eiE4MKZrx2TMrA1nIUdD4S6SgH740irW7yrlj988ncx+XSkqq6SotO5r\nd2kluyL/FpVVUVRaweY9+/l0816KSiuprKlt9DFTkzrQKxL8BwK/4R5Bny6p9Kp3e2qy9g7ConAX\nSTCLV27nqfc38q1zjues4X0B6Ne9U6CfdXdKK2so2ldJUVnDF4LKg7cXlVaycksxRWWV7CmravLx\nunVMPhj2h7wgHHwhiOwxpKXSvbP2DqJF4S6SQHbuq+C255Yz8tju3DJ5xBH/vJnRtWMyXTsmM6hP\nWqCfqa6pZc/+qib2DP6xx7C9uJzPthazq7SSiurG9w6SOli9Q0SNvRAcurfQKSXpiOfZHijcRRKE\nu3Pbs8spLq/mqf8cS8fktgm95KQO9O3akb5dOwb+mbLK6sgLQRW7SivYXVbJrn2V7C6rpKi07lDR\n7tIq8rbvqxtXVol744+VlpoU+IWgd5dUenZOoUOHxN87ULiLJIg/fbCJxat2cNeXRjGif7ewyzms\ntNRk0lKTyegVbHxNrbM3snfwzy8EDfYWyipZU1j3glBWWdPoY3Uw6HngfYO0VHo18UZyny4dI/el\nkpYaf1EZfxWLyCHW7SzlBy+u5PPD+3Lt54aEXU7UJXWwg1viQZVX1fwj/Jt4Idi1r5J1O0v5cMMe\ndpdVUlPb+O5Bp5QOkReCBu8fRG5ruLfQs3P4p5oGCnczmwI8AiQBj7v7fQ3uHwQ8CfSMjLnN3RdE\nuVYRaURVTS03z1tKanIHHvjKKe3ikEMQnVKSOK5nZ47r2TnQ+Npap6S8OvI+QcXBw0NFpVWH7C1s\n2FXG7tJKSiqqm3y8Hp1T/jn0670Q/MuIvpx4TPdoTbVRzYa7mSUBs4HJQAGwxMyy3X1lvWF3AvPd\n/ZdmNgpYAAxphXpFpIGfv5rPsk17mH3VOI7pEeysGDlUhw5Gj7QUeqSlMLRvl0A/U1Fdw56yf7yZ\n3PCw0YE3ljcVlbFsU93eQVWN8+NOJ4cf7sAEIN/d1wKY2VzgEqB+uDtwoNIewJZoFikijft4425+\n/lo+l546gIvGHBt2Oe1Ox+Qk+ndPov8RnGq6r6Ka5A6tf8gmSLgPADbVWy4ATm8w5h5gkZndAHQB\nJkWlOhFpUmlFNTPnLeWY7p2455KTwi5HAjAzunVKaZPfFeTlo7EDeA3fdbgS+J27ZwAXAv9nZoc8\ntplNN7McM8spLCw88mpF5KAfvrSKDUVlPPTVU+jeRoEh8SNIuBcAA+stZ3DoYZf/AOYDuPu7QCeg\nb8MHcvc57p7l7lnp6c1+eLeINOHlldv50wcb+dbZwzj9+D5hlyMxKEi4LwEyzWyomaUC04DsBmM2\nAhMBzGwkdeGuTXORVlBYUsFtzy5nVAu7UKV9aPaYu7tXm9kMYCF1pzk+4e4rzOxeIMfds4FbgV+b\n2UzqDtlc695UP5mItNSBLtSSimr+NG2sLswlTQp0nnvknPUFDW67q973K4GzoluaiDT01AcbeeWz\nHdx9cex3oUq49LIvEifWFu7jhy+u4l8y+/KNM4eEXY7EOIW7SByoqqll5vxlpCZ34P7L1YUqzdO1\nZUTiwKORLtRffE1dqBKMttxFYtxHG3cz+7V8Lh03gAtPVheqBKNwF4lh/9SFOlVdqBKcDsuIxLAf\nvLiSjUVlzJt+prpQ5Yhoy10kRr28cjtzl2ziunOGMWFo77DLkTijcBeJQQe6UE86rjszJ6kLVY6c\nwl0kxrg7/+/Z5eyrqObhK9SFKi2jZ41IjPnj+xt59bMd3P7FE8lUF6q0kMJdJIasLdzHj16q60K9\nRl2ochQU7iIxoqqmlpnzltIxRZ+FKkdPp0KKxIhHX1nNsoK9/OJr4wJ/bJtIU7TlLhIDPtxQ91mo\nl43LUBeqRIXCXSRk+yqquWX+Uo7r2Zl7po4KuxxJEDosIxKyH7zwjy7UtvrwZEl82nIXCdHCFduY\nl7OJ69WFKlGmcBcJyY6Scm5/7hNGD+jOzepClShTuIuEwN35f88sp1RdqNJKAj2jzGyKmeWaWb6Z\n3dbI/bPMbGnkK8/M9kS/VJHE8Yf3N/JabiH/feFIhvdTF6pEX7NvqJpZEjAbmAwUAEvMLDvyodgA\nuPvMeuNvAE5thVpFEsKawn386KWVnD0inWvOHBx2OZKggmy5TwDy3X2tu1cCc4FLDjP+SuBP0ShO\nJNEc6ELtlJLE/ZePwUxdqNI6goT7AGBTveWCyG2HMLPBwFDg1aMvTSTx/OyV1Swv2Mt9l56sLlRp\nVUHCvbFNC29i7DTgGXevafSBzKabWY6Z5RQWFgatUSQhfLihiNmv5XP5+AymjFYXqrSuIOFeAAys\nt5wBbGli7DQOc0jG3ee4e5a7Z6WnpwevUiTO7auoZua8ZRzXszN3X6wuVGl9QcJ9CZBpZkPNLJW6\nAM9uOMjMTgB6Ae9Gt0SR+HfvCyso2F3GrCvGqgtV2kSz4e7u1cAMYCGwCpjv7ivM7F4zm1pv6JXA\nXHdv6pCNSLu0cMU25ucUcP25wzhtiLpQpW0EuraMuy8AFjS47a4Gy/dEryyRxLCjuJzbnl3O6AHd\nuWmiulCl7agtTqSVuDv/9exyyipr1IUqbU7PNpFW8of3NvB6biF3XKQuVGl7CneRVrCmcB8/WrCK\nc0ak8/Uz1IUqbU/hLhJlVTW13Dx3KZ3VhSoh0od1iETZI4tX88nmvTx29Tj6qQtVQqItd5Eo+nBD\nEb94PZ+vqAtVQqZwF4mSfRXV3DxvKQN6debuqSeFXY60czosIxIl/5O9gs279zP/W2fStaP+tCRc\n2nIXiYK/fbqVpz8s4NvnDidLXagSAxTuIkdpR3HdZ6GePKAHN03KDLscEUDhLnJU3J3vPbOc/VU1\nzLpiLClJ+pOS2KBnoshR+L/3NvD3vELuuHAkw/t1DbsckYMU7iItlL+jhB+9tIpzT0jnanWhSoxR\nuIu0QGV1LTfPW0paahI/vUxdqBJ7dL6WSAs88koen24u5rGrx6sLVWKSttxFjlDO+iJ++foavpqV\nwZTRx4RdjkijFO4iR6CkvIqZ85eS0SuNuy5WF6rELh2WETkC//PCSjbv3s/T16kLVWKbttxFAvrb\np1t55sMCvnPecMYPVheqxLZA4W5mU8ws18zyzey2JsZ81cxWmtkKM3squmWKhGt7cTm3PfcJYzJ6\ncONEdaFK7Gt2v9LMkoDZwGSgAFhiZtnuvrLemEzgduAsd99tZv1aq2CRtnagC7VcXagSR4I8SycA\n+e6+1t0rgbnAJQ3G/Ccw2913A7j7juiWKRKe37+7gTfyCrnjolEMS1cXqsSHIOE+ANhUb7kgclt9\nI4ARZva2mb1nZlMaeyAzm25mOWaWU1hY2LKKRdpQ/o4S/nfBKs47IZ2rTx8UdjkigQUJ98Za77zB\ncjKQCZwLXAk8bmY9D/kh9znunuXuWenp6Udaq0ibqqyu5aa5S+nSMZmf6LNQJc4ECfcCYGC95Qxg\nSyNjnnf3KndfB+RSF/YicevhxXms2FLMjy89mX7d1IUq8SVIuC8BMs1sqJmlAtOA7AZj/gKcB2Bm\nfak7TLM2moWKtKUl64t47O9ruCJrIF84SV2oEn+aDXd3rwZmAAuBVcB8d19hZvea2dTIsIXALjNb\nCbwGfM/dd7VW0SKtqaS8ipnz6rpQv3/xqLDLEWmRQC127r4AWNDgtrvqfe/ALZEvkbh2T/ZKtuzZ\nz9PXfU5dqBK3dMKuSD1//WQrz35UwIzzhjN+cK+wyxFpMYW7SMT24nJu//MnnJLRgxvUhSpxTuEu\nAtTWOt99ehnlVTU8pC5USQB6BosAv393PW+u3smd6kKVBKFwl3Zv9fYSfvzXzzj/xH58TV2okiAU\n7tKu1e9Cve+yk9WFKglD53lJuzZrcR4rtxYz5+vj1YUqCUVb7tJufbCurgt12mkDuUBdqJJgFO7S\nLhVHulAH9U7j+19SF6okHh2WkXbpnuwVbN1b14XaRV2okoC05S7tzoJPtvLcR5uZcX6mulAlYSnc\npV3Ztrec/z7QhXr+8LDLEWk1CndpN2prne89s4yKqlp9FqokPD27pd148t26LtQ7LhrJ8epClQSn\ncJd2IW97CfepC1XaEYW7JLzK6lpunruUrh2T+cll+ixUaR90DpgkvIderutC/fU1WaR36xh2OSJt\nQlvuktDeX7uLX72xhisnDGTyqP5hlyPSZgKFu5lNMbNcM8s3s9sauf9aMys0s6WRr29Gv1SRI1Nc\nXsUt85cxuHcad16kLlRpX5o9LGNmScBsYDJQACwxs2x3X9lg6Dx3n9EKNYq0yD3Pr2BbcTlPX3em\nulCl3Qmy5T4ByHf3te5eCcwFLmndskSOzkvLt/Lcx5uZcd5wxg1SF6q0P0HCfQCwqd5yQeS2hi4z\ns+Vm9oyZDYxKdSItcLALdWBPZqgLVdqpIOHe2Hlj3mD5BWCIu48BFgNPNvpAZtPNLMfMcgoLC4+s\nUpEADnwWamV1LQ+rC1XasSDP/AKg/pZ4BrCl/gB33+XuFZHFXwPjG3sgd5/j7lnunpWent6SekUO\n63fvrOet/J3c+aWRDO3bJexyREITJNyXAJlmNtTMUoFpQHb9AWZ2bL3FqcCq6JUoEkze9hLu+9tn\nTDyxH1dNUBeqtG/NnkLg7tVmNgNYCCQBT7j7CjO7F8hx92zgRjObClQDRcC1rVizyCEqqmu4ae5S\nunVM5j51oYoE61B19wXAgga33VXv+9uB26NbmkhwD72cx6qtxTyuLlQRQB2qkgDeW7uLOW+s5coJ\ng5ikLlQRQOEuca64vIpbD3ahjgy7HJGYobY9iWt3R7pQn1EXqsg/0Za7xK0Xlm3hzx9v5obzh3Oq\nulBF/onCXeLS1r37uePPnzB2YE9mnKcuVJGGFO4Sdw50oVbVOLOuGEuyulBFDqG/Cok7v31nPW/n\n7+L7XxqlLlSRJijcJa7kbivhJ3/7jEkj+3HlBF2fTqQpCneJG3VdqB/TvZO6UEWao3PHJG48tCiP\nz7aV8JtvZNG3q7pQRQ5HW+4SF95ds4s5b67lqtMHMXGkulBFmqNwl5i3d38Vt85fypA+XdSFKhKQ\nDstIzLv7+U/ZXlLBs9d/jrRUPWVFgtCWu8S07GVb+MvSLdxw/nDGDuwZdjkicUPhLjFry5793Kku\nVJEWUbhLTDrQhVpd6zysLlSRI6a/GIlJT7y9jnfW1HWhDlEXqsgRU7hLzPlsWzE//Vsuk0b2Z9pp\n6kIVaQmFu8SUiuoabp67lO6dk7nvspPVhSrSQoHC3cymmFmumeWb2W2HGXe5mbmZZUWvRGlPHox0\nof708jHqQhU5Cs2Gu5klAbOBLwKjgCvNbFQj47oBNwLvR7tIaR/eWbOTX7+5lq+dPojzT1QXqsjR\nCLLlPgHId/e17l4JzAUuaWTcD4CfAuVRrE/aib37q/ju/GUM6dOFO9SFKnLUgoT7AGBTveWCyG0H\nmdmpwEB3fzGKtUk7clekC3XWFWPVhSoSBUHCvbF3tPzgnWYdgFnArc0+kNl0M8sxs5zCwsLgVUpC\ne37pZp5fuoUbz89UF6pIlAQJ9wKg/vloGcCWesvdgNHA62a2HjgDyG7sTVV3n+PuWe6elZ6e3vKq\nJWFs2bOfO//yKacO6sl3zhsWdjkiCSNIuC8BMs1sqJmlAtOA7AN3uvted+/r7kPcfQjwHjDV3XNa\npWJJGLW1zq3zl1GjLlSRqGv2r8ndq4EZwEJgFTDf3VeY2b1mNrW1C5TE9cTb63h37S7u+tIoBvdR\nF6pINAV658rdFwALGtx2VxNjzz36siTRrdpa14U6eVR/rlAXqkjUaT9Y2lx5VQ0z5y2le+cU7rtU\nXagirUHnnEmbe3BRLp9tK+GJa7Pooy5UkVahLXdpU+/k7+Txt9Zx9RnqQhVpTQp3aTN7y6q49ell\nDO3ThTsuPOQKFiISRTosI23m+89/SmHks1A7pyaFXY5IQtOWu7SJ55duJnvZFm6cmMkp6kIVaXUK\nd2l1myNdqOMG9eTb56oLVaQtKNylVdXWOt+dv4zaWmeWulBF2oz+0qRV/eatSBfqxepCFWlLCndp\nNau2FnP/wlwuGNWfr2apC1WkLSncpVWUVx34LNQUfqwuVJE2p1MhpVU8sDCX3O0l/Pba09SFKhIC\nbblL1L1drwv1vBP7hV2OSLukcJeo2ltWxa3zl3F8urpQRcKkwzISVXc+/yk791Xw3DXqQhUJk7bc\nJWqeX7qZF5Zt4aaJmYzJUBeqSJgU7hIVB7pQxw/uxfXqQhUJnQ7LyFHbVFTGTXM/rutC/aq6UEVi\ngcJdWmx7cTk/fzWfuUs2Ymbcf/kYBvVJC7ssESFguJvZFOARIAl43N3va3D/dcB3gBpgHzDd3VdG\nuVaJEUWllTz29zU8+c56amqdK04byA3nZ3JMj05hlyYiEc2Gu5klAbOByUABsMTMshuE91Pu/lhk\n/FTgIWBKK9QrISopr+LxN9fxm7fWUVpZzb+OHcDNk0Zoa10kBgXZcp8A5Lv7WgAzmwtcAhwMd3cv\nrje+C+DRLFLCtb+yhiffXc9jf1/DnrIqvjj6GG6ZPILM/t3CLk1EmhAk3AcAm+otFwCnNxxkZt8B\nbgFSgfMbeyAzmw5MBxg0aNCR1iptrLK6lrlLNvLoq/kUllRwzoh0vnvBCZyc0SPs0kSkGUHCvbEr\nPh2yZe7us4HZZnYVcCfwjUbGzAHmAGRlZWnrPkZV19Ty3MebeWTxajbv2c+EIb2ZfdU4JgztHXZp\nIhJQkHAvAOpfrzUD2HKY8XOBXx5NURKO2lpnwadbeejlPNYWlnLygB7876Unc3ZmX13VUSTOBAn3\nJUCmmQ0FNgPTgKvqDzCzTHdfHVm8CFiNxA1357XcHdy/MI9VW4sZ0b8rj109ni+c1F+hLhKnmg13\nd682sxnAQupOhXzC3VeY2b1AjrtnAzPMbBJQBeymkUMyEpveWbOTBxbm8tHGPQzqncasK05h6ikD\nSOqgUBeJZ+YezqHvrKwsz8nJCeV3C3y8cTcPLMrl7fxdHNO9EzdOzOQrWRmkqLtUJKaZ2YfuntXc\nOHWotjOrthbz4KI8Fq/aTp8uqdx50UiuPmMwnVJ0BUeRRKJwbyfW7Sxl1st5vLB8C107JvPdC0bw\nb2cNpUtHPQVEEpH+shPc5j37+dni1TzzUQGpSR24/pxhTD/7eHqmpYZdmoi0IoV7gtpRUs4vXlvD\nU+9vBODrZwzmO+cNJ72bPs9UpD1QuCeYPWWV/OqNtfzu7fVU1tTylfEZ3DAxkwE9O4ddmoi0IYV7\ngthXUc1v31rHnDfXsq+imovHHMfMySMY2rdL2KWJSAgU7nGuvKqGP7y3gV+8voai0komj+rPrReM\n4MRjuoddmoiESOEep6pqapmfs4lHX8lnW3E5nx/el1svGMGpg3qFXZqIxACFe5ypqXWyl21m1sur\n2VhUxvjBvZh1xVjOHNYn7NJEJIYo3OOEu7NwxTYeXJTH6h37GHVsd564NovzTuin67+IyCEU7jHO\n3fl7XiEPLsrjk817GZbehdlXjeOLo4+hg67/IiJNULjHsA/WFfHAwlw+WF/EgJ6duf/yMfzrqQNI\n1vVfRKQZCvcY9EnBXu5flMsbeYWkd+vIvZecxBWnDaRjsq7/IiLBKNxjSN72Eh5alMffVmyjZ1oK\nt3/xRK45cwidUxXqInJkFO4xYOOuMh5enMefl26mS2oyN0/K5D8+P5RunVLCLk1E4pTCPUTb9pbz\ns1dXM3/JJpI6GNP/5XiuO2cYvbrool4icnQU7iHYta+CX76+ht+/twF358oJg5hx/nD6d+8Udmki\nkiAU7m1o7/4qHn9zLU+8tY79VTVcOi6DmyZmMrB3WtiliUiCCRTuZjYFeIS6z1B93N3va3D/LcA3\ngWqgEPh3d98Q5VrjVlllNb97Zz2/+vta9u6v4qKTj2Xm5BEM79c17NJEJEE1G+5mlgTMBiYDBcAS\nM8t295X1hn0MZLl7mZldD/wUuKI1Co4nFdU1PPX+Rma/toad+yo4/8R+3DJ5BKMH9Ai7NBFJcEG2\n3CcA+e6+FsDM5gKXAAfD3d1fqzf+PeDqaBYZb6prann2owJ+9ko+m/fs54zje/Orr49j/ODeYZcm\nIu1EkHAfAGyqt1wAnH6Y8f8B/PVoiopXtbXOi59sZdbLeazbWcopA3vyk8vGcNbwPrr+i4i0qSDh\n3lgqeaMDza4GsoBzmrh/OjAdYNCgQQFLjH3uzuJVO3hwUS6fbSvhhP7d+PU1WUwaqYt6iUg4goR7\nATCw3nIGsKXhIDObBNwBnOPuFY09kLvPAeYAZGVlNfoCEW/ezt/J/QtzWbppD0P6pPHItLFcPOY4\nXdRLREIVJNyXAJlmNhTYDEwDrqo/wMxOBX4FTHH3HVGvMgZ9uGE3DyzM5d21uziuRyfuu/RkLhuf\nQYou6iUiMaDZcHf3ajObASyk7lTIJ9x9hZndC+S4ezZwP9AVeDpyGGKju09txbpDs2LLXh5clMer\nn+2gb9dU7r54FFdOGESnFF3/RURiR6Dz3N19AbCgwW131ft+UpTrijlrCvfx0Mt5vLR8K907JfO9\nL5zAtZ8bQpeO6gMTkdijZGrGpqIyHnllNc99VECnlCRmnDec/zz7eHp01kW9RCR2KdybsKO4nJ+/\nls+fPtiImfFvZw3l+nOH0bdrx7BLExFplsK9gd2llTz2xhqefGc91TXOV7IGcuPE4Rzbo3PYpYmI\nBKZwjygpr+I3b63jN2+uY19lNV8eO4CbJ2UyuE+XsEsTETli7T7cy6tq+P276/nl62vYXVbFF07q\nzy2TT+CEY7qFXZqISIu123CvrK5l3pKNPPpqPjtKKjh7RDrfvWAEYzJ6hl2aiMhRa3fhXlPr/Pnj\nzTy8OI+C3fs5bUgvHr3yVE4/vk/YpYmIRE27CffaWuevn27joZdzWVNYyugB3fnhl0dzzoh0Xf9F\nRBJOwoe7u/N6biEPLMplxZZihvfryi+/No4po49RqItIwkrocH9v7S4eWJhLzobdDOzdmQe/cgpf\nPnUASbqol4gkuIQM92Wb9vDAolzeXL2T/t078sMvj+arWQNJTdZFvUSkfUiocM/dVsKDi3JZtHI7\nvbukcudFI7n6jMG6qJeItDsJEe7rd5Yya3Ee2cu20DU1mVsmj+DfPz+Urrqol4i0U3Gdflv27OfR\nV1czP6eAlCTjW2cP47pzjqdnWmrYpYmIhCouw72wpIJfvJ7PH9/bCMDXzxjMt88bRr9unUKuTEQk\nNsRduM9bspF7sldSUV3D5eMzuHFiJhm90sIuS0QkpsRduA/sncbEkf2YOXkEw9K7hl2OiEhMirtw\n/9ywvnxuWN+wyxARiWk68VtEJAEFCnczm2JmuWaWb2a3NXL/2Wb2kZlVm9nl0S9TRESORLPhbmZJ\nwGzgi8Ao4EozG9Vg2EbgWuCpaBcoIiJHLsgx9wlAvruvBTCzucAlwMoDA9x9feS+2laoUUREjlCQ\nwzIDgE31lgsit4mISIwKEu6NXULRW/LLzGy6meWYWU5hYWFLHkJERAIIEu4FwMB6yxnAlpb8Mnef\n4+5Z7p6Vnp7ekocQEZEAgoT7EiDTzIaaWSowDchu3bJERORomHvzR1jM7ELgYSAJeMLdf2Rm9wI5\n7p5tZqcBfwZ6AeXANnc/qZnHLAQ2tLDuvsDOFv5srNFcYk+izAM0l1h1NHMZ7O7NHvoIFO6xxsxy\n3D0r7DqiQXOJPYkyD9BcYlVbzEUdqiIiCUjhLiKSgOI13OeEXUAUaS6xJ1HmAZpLrGr1ucTlMXcR\nETm8eN1yFxGRw4jZcDezJ8xsh5l92sT9ZmY/i1ypcrmZjWvrGoMKMJdzzWyvmS2NfN3V1jUGZWYD\nzew1M1tlZivM7KZGxsT8ugk4j7hYL2bWycw+MLNlkbn8TyNjOprZvMg6ed/MhrR9pc0LOJdrzayw\n3nr5Zhi1BmFmSWb2sZm92Mh9rbtO3D0mv4CzgXHAp03cfyHwV+ouj3AG8H7YNR/FXM4FXgy7zoBz\nORYYF/m+G5AHjIq3dRNwHnGxXiL/z10j36cA7wNnNBjzbeCxyPfTgHlh130Uc7kW+HnYtQaczy3U\nXS33kOdRa6+TmN1yd/c3gKLDDLkE+L3XeQ/oaWbHtk11RybAXOKGu291948i35cAqzj0QnIxv24C\nziMuRP6f90UWUyJfDd9MuwR4MvL9M8BEM2vsulGhCjiXuGBmGcBFwONNDGnVdRKz4R5Aol2t8szI\nruhfzeyw3b2xIrIbeSp1W1f1xdW6Ocw8IE7WS2T3fymwA3jZ3ZtcJ+5eDewF+rRtlcEEmAvAZZFD\nfs+Y2cBG7o8FDwP/BTR1KfRWXSfxHO5Ru1plDPiIupbiU4BHgb+EXE+zzKwr8Cxws7sXN7y7kR+J\nyXXTzDziZr24e427j6Xuwn4TzGx0gyFxs04CzOUFYIi7jwEW84+t35hhZl8Cdrj7h4cb1shtUVsn\n8RzuUbszzP94AAABjUlEQVRaZdjcvfjArqi7LwBSzCxmPwXczFKoC8Q/uvtzjQyJi3XT3Dzibb0A\nuPse4HVgSoO7Dq4TM0sGehDjhwqbmou773L3isjir4HxbVxaEGcBU81sPTAXON/M/tBgTKuuk3gO\n92zgmsiZGWcAe919a9hFtYSZHXPgWJuZTaBuvewKt6rGRer8DbDK3R9qYljMr5sg84iX9WJm6WbW\nM/J9Z2AS8FmDYdnANyLfXw686pF38mJJkLk0eP9mKnXvl8QUd7/d3TPcfQh1b5a+6u5XNxjWqusk\nyMfshcLM/kTd2Qp9zawAuJu6N1dw98eABdSdlZEPlAH/Fk6lzQswl8uB682sGtgPTIvFP7yIs4Cv\nA59EjosC/DcwCOJq3QSZR7ysl2OBJ63u8447APPd/UWrd+VW6l7I/s/M8qnbOpwWXrmHFWQuN5rZ\nVKCaurlcG1q1R6gt14k6VEVEElA8H5YREZEmKNxFRBKQwl1EJAEp3EVEEpDCXUQkASncRUQSkMJd\nRCQBKdxFRBLQ/wd+ia7R/q0ADQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89b2a6bfd0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(1,5), accu4)\n",
    "print(range(1,5)[np.argmax(accu4)],np.max(accu4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hidden_dim1 = [10,10,5]\n",
    "hidden_dim2 = [10,10,10]\n",
    "hidden_dim3 = [20,10,5]\n",
    "hidden_dim4 = [20,20,10]\n",
    "hidden_dim5 = [20,20,20]\n",
    "\n",
    "hidden_dims = {'h1':hidden_dim1,\n",
    "              'h2':hidden_dim2,\n",
    "              'h3':hidden_dim3,\n",
    "              'h4':hidden_dim4,\n",
    "              'h5':hidden_dim5}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 1 / 3600) loss: 10.488189\n",
      "(Epoch 0 / 100) train acc: 0.134000; val_acc: 0.125833\n",
      "(Epoch 1 / 100) train acc: 0.188000; val_acc: 0.170833\n",
      "(Epoch 2 / 100) train acc: 0.237000; val_acc: 0.243333\n",
      "(Iteration 101 / 3600) loss: 1.850281\n",
      "(Epoch 3 / 100) train acc: 0.325000; val_acc: 0.316667\n",
      "(Epoch 4 / 100) train acc: 0.323000; val_acc: 0.323333\n",
      "(Epoch 5 / 100) train acc: 0.447000; val_acc: 0.414167\n",
      "(Iteration 201 / 3600) loss: 1.684501\n",
      "(Epoch 6 / 100) train acc: 0.349000; val_acc: 0.351667\n",
      "(Epoch 7 / 100) train acc: 0.338000; val_acc: 0.345833\n",
      "(Epoch 8 / 100) train acc: 0.406000; val_acc: 0.407500\n",
      "(Iteration 301 / 3600) loss: 1.655931\n",
      "(Epoch 9 / 100) train acc: 0.418000; val_acc: 0.404167\n",
      "(Epoch 10 / 100) train acc: 0.383000; val_acc: 0.398333\n",
      "(Epoch 11 / 100) train acc: 0.444000; val_acc: 0.433333\n",
      "(Iteration 401 / 3600) loss: 1.554860\n",
      "(Epoch 12 / 100) train acc: 0.486000; val_acc: 0.484167\n",
      "(Epoch 13 / 100) train acc: 0.505000; val_acc: 0.463333\n",
      "(Iteration 501 / 3600) loss: 1.394536\n",
      "(Epoch 14 / 100) train acc: 0.514000; val_acc: 0.500833\n",
      "(Epoch 15 / 100) train acc: 0.519000; val_acc: 0.496667\n",
      "(Epoch 16 / 100) train acc: 0.496000; val_acc: 0.495000\n",
      "(Iteration 601 / 3600) loss: 1.191470\n",
      "(Epoch 17 / 100) train acc: 0.566000; val_acc: 0.553333\n",
      "(Epoch 18 / 100) train acc: 0.535000; val_acc: 0.501667\n",
      "(Epoch 19 / 100) train acc: 0.524000; val_acc: 0.514167\n",
      "(Iteration 701 / 3600) loss: 0.938541\n",
      "(Epoch 20 / 100) train acc: 0.614000; val_acc: 0.556667\n",
      "(Epoch 21 / 100) train acc: 0.570000; val_acc: 0.553333\n",
      "(Epoch 22 / 100) train acc: 0.640000; val_acc: 0.586667\n",
      "(Iteration 801 / 3600) loss: 0.721940\n",
      "(Epoch 23 / 100) train acc: 0.533000; val_acc: 0.516667\n",
      "(Epoch 24 / 100) train acc: 0.605000; val_acc: 0.586667\n",
      "(Epoch 25 / 100) train acc: 0.649000; val_acc: 0.630833\n",
      "(Iteration 901 / 3600) loss: 0.829989\n",
      "(Epoch 26 / 100) train acc: 0.636000; val_acc: 0.624167\n",
      "(Epoch 27 / 100) train acc: 0.645000; val_acc: 0.640833\n",
      "(Iteration 1001 / 3600) loss: 0.895091\n",
      "(Epoch 28 / 100) train acc: 0.660000; val_acc: 0.638333\n",
      "(Epoch 29 / 100) train acc: 0.664000; val_acc: 0.613333\n",
      "(Epoch 30 / 100) train acc: 0.629000; val_acc: 0.636667\n",
      "(Iteration 1101 / 3600) loss: 0.668580\n",
      "(Epoch 31 / 100) train acc: 0.684000; val_acc: 0.650833\n",
      "(Epoch 32 / 100) train acc: 0.653000; val_acc: 0.639167\n",
      "(Epoch 33 / 100) train acc: 0.678000; val_acc: 0.665000\n",
      "(Iteration 1201 / 3600) loss: 0.696127\n",
      "(Epoch 34 / 100) train acc: 0.683000; val_acc: 0.675833\n",
      "(Epoch 35 / 100) train acc: 0.654000; val_acc: 0.636667\n",
      "(Epoch 36 / 100) train acc: 0.698000; val_acc: 0.673333\n",
      "(Iteration 1301 / 3600) loss: 0.898712\n",
      "(Epoch 37 / 100) train acc: 0.696000; val_acc: 0.683333\n",
      "(Epoch 38 / 100) train acc: 0.697000; val_acc: 0.678333\n",
      "(Iteration 1401 / 3600) loss: 0.984828\n",
      "(Epoch 39 / 100) train acc: 0.723000; val_acc: 0.685000\n",
      "(Epoch 40 / 100) train acc: 0.694000; val_acc: 0.655833\n",
      "(Epoch 41 / 100) train acc: 0.695000; val_acc: 0.684167\n",
      "(Iteration 1501 / 3600) loss: 0.700146\n",
      "(Epoch 42 / 100) train acc: 0.702000; val_acc: 0.685000\n",
      "(Epoch 43 / 100) train acc: 0.699000; val_acc: 0.685000\n",
      "(Epoch 44 / 100) train acc: 0.718000; val_acc: 0.690833\n",
      "(Iteration 1601 / 3600) loss: 0.751994\n",
      "(Epoch 45 / 100) train acc: 0.720000; val_acc: 0.702500\n",
      "(Epoch 46 / 100) train acc: 0.754000; val_acc: 0.727500\n",
      "(Epoch 47 / 100) train acc: 0.746000; val_acc: 0.707500\n",
      "(Iteration 1701 / 3600) loss: 0.748337\n",
      "(Epoch 48 / 100) train acc: 0.736000; val_acc: 0.705833\n",
      "(Epoch 49 / 100) train acc: 0.743000; val_acc: 0.739167\n",
      "(Epoch 50 / 100) train acc: 0.764000; val_acc: 0.728333\n",
      "(Iteration 1801 / 3600) loss: 0.577469\n",
      "(Epoch 51 / 100) train acc: 0.749000; val_acc: 0.730000\n",
      "(Epoch 52 / 100) train acc: 0.757000; val_acc: 0.730000\n",
      "(Iteration 1901 / 3600) loss: 0.676015\n",
      "(Epoch 53 / 100) train acc: 0.816000; val_acc: 0.758333\n",
      "(Epoch 54 / 100) train acc: 0.803000; val_acc: 0.766667\n",
      "(Epoch 55 / 100) train acc: 0.816000; val_acc: 0.760833\n",
      "(Iteration 2001 / 3600) loss: 0.445532\n",
      "(Epoch 56 / 100) train acc: 0.816000; val_acc: 0.775000\n",
      "(Epoch 57 / 100) train acc: 0.823000; val_acc: 0.784167\n",
      "(Epoch 58 / 100) train acc: 0.827000; val_acc: 0.776667\n",
      "(Iteration 2101 / 3600) loss: 0.639022\n",
      "(Epoch 59 / 100) train acc: 0.854000; val_acc: 0.794167\n",
      "(Epoch 60 / 100) train acc: 0.860000; val_acc: 0.797500\n",
      "(Epoch 61 / 100) train acc: 0.862000; val_acc: 0.809167\n",
      "(Iteration 2201 / 3600) loss: 0.479602\n",
      "(Epoch 62 / 100) train acc: 0.855000; val_acc: 0.815000\n",
      "(Epoch 63 / 100) train acc: 0.879000; val_acc: 0.844167\n",
      "(Iteration 2301 / 3600) loss: 0.662478\n",
      "(Epoch 64 / 100) train acc: 0.872000; val_acc: 0.841667\n",
      "(Epoch 65 / 100) train acc: 0.817000; val_acc: 0.774167\n",
      "(Epoch 66 / 100) train acc: 0.892000; val_acc: 0.838333\n",
      "(Iteration 2401 / 3600) loss: 0.472343\n",
      "(Epoch 67 / 100) train acc: 0.890000; val_acc: 0.849167\n",
      "(Epoch 68 / 100) train acc: 0.757000; val_acc: 0.734167\n",
      "(Epoch 69 / 100) train acc: 0.884000; val_acc: 0.851667\n",
      "(Iteration 2501 / 3600) loss: 0.297075\n",
      "(Epoch 70 / 100) train acc: 0.875000; val_acc: 0.854167\n",
      "(Epoch 71 / 100) train acc: 0.746000; val_acc: 0.721667\n",
      "(Epoch 72 / 100) train acc: 0.890000; val_acc: 0.855000\n",
      "(Iteration 2601 / 3600) loss: 0.471155\n",
      "(Epoch 73 / 100) train acc: 0.891000; val_acc: 0.862500\n",
      "(Epoch 74 / 100) train acc: 0.893000; val_acc: 0.855000\n",
      "(Epoch 75 / 100) train acc: 0.902000; val_acc: 0.863333\n",
      "(Iteration 2701 / 3600) loss: 0.391823\n",
      "(Epoch 76 / 100) train acc: 0.887000; val_acc: 0.870833\n",
      "(Epoch 77 / 100) train acc: 0.908000; val_acc: 0.863333\n",
      "(Iteration 2801 / 3600) loss: 0.358138\n",
      "(Epoch 78 / 100) train acc: 0.864000; val_acc: 0.826667\n",
      "(Epoch 79 / 100) train acc: 0.900000; val_acc: 0.872500\n",
      "(Epoch 80 / 100) train acc: 0.917000; val_acc: 0.866667\n",
      "(Iteration 2901 / 3600) loss: 0.231950\n",
      "(Epoch 81 / 100) train acc: 0.911000; val_acc: 0.875000\n",
      "(Epoch 82 / 100) train acc: 0.895000; val_acc: 0.879167\n",
      "(Epoch 83 / 100) train acc: 0.904000; val_acc: 0.874167\n",
      "(Iteration 3001 / 3600) loss: 0.283932\n",
      "(Epoch 84 / 100) train acc: 0.896000; val_acc: 0.877500\n",
      "(Epoch 85 / 100) train acc: 0.917000; val_acc: 0.875000\n",
      "(Epoch 86 / 100) train acc: 0.939000; val_acc: 0.880000\n",
      "(Iteration 3101 / 3600) loss: 0.268659\n",
      "(Epoch 87 / 100) train acc: 0.901000; val_acc: 0.879167\n",
      "(Epoch 88 / 100) train acc: 0.921000; val_acc: 0.881667\n",
      "(Iteration 3201 / 3600) loss: 0.518863\n",
      "(Epoch 89 / 100) train acc: 0.923000; val_acc: 0.889167\n",
      "(Epoch 90 / 100) train acc: 0.916000; val_acc: 0.885833\n",
      "(Epoch 91 / 100) train acc: 0.924000; val_acc: 0.876667\n",
      "(Iteration 3301 / 3600) loss: 0.375537\n",
      "(Epoch 92 / 100) train acc: 0.910000; val_acc: 0.880000\n",
      "(Epoch 93 / 100) train acc: 0.918000; val_acc: 0.887500\n",
      "(Epoch 94 / 100) train acc: 0.912000; val_acc: 0.874167\n",
      "(Iteration 3401 / 3600) loss: 0.250462\n",
      "(Epoch 95 / 100) train acc: 0.919000; val_acc: 0.888333\n",
      "(Epoch 96 / 100) train acc: 0.909000; val_acc: 0.890833\n",
      "(Epoch 97 / 100) train acc: 0.906000; val_acc: 0.881667\n",
      "(Iteration 3501 / 3600) loss: 0.296280\n",
      "(Epoch 98 / 100) train acc: 0.925000; val_acc: 0.888333\n",
      "(Epoch 99 / 100) train acc: 0.921000; val_acc: 0.894167\n",
      "(Epoch 100 / 100) train acc: 0.922000; val_acc: 0.890000\n",
      "(Iteration 1 / 3600) loss: 7.891556\n",
      "(Epoch 0 / 100) train acc: 0.094000; val_acc: 0.070000\n",
      "(Epoch 1 / 100) train acc: 0.177000; val_acc: 0.175000\n",
      "(Epoch 2 / 100) train acc: 0.189000; val_acc: 0.185000\n",
      "(Iteration 101 / 3600) loss: 1.945831\n",
      "(Epoch 3 / 100) train acc: 0.186000; val_acc: 0.197500\n",
      "(Epoch 4 / 100) train acc: 0.246000; val_acc: 0.219167\n",
      "(Epoch 5 / 100) train acc: 0.256000; val_acc: 0.259167\n",
      "(Iteration 201 / 3600) loss: 1.628567\n",
      "(Epoch 6 / 100) train acc: 0.304000; val_acc: 0.292500\n",
      "(Epoch 7 / 100) train acc: 0.352000; val_acc: 0.339167\n",
      "(Epoch 8 / 100) train acc: 0.439000; val_acc: 0.436667\n",
      "(Iteration 301 / 3600) loss: 1.437798\n",
      "(Epoch 9 / 100) train acc: 0.477000; val_acc: 0.453333\n",
      "(Epoch 10 / 100) train acc: 0.513000; val_acc: 0.477500\n",
      "(Epoch 11 / 100) train acc: 0.519000; val_acc: 0.507500\n",
      "(Iteration 401 / 3600) loss: 1.286853\n",
      "(Epoch 12 / 100) train acc: 0.583000; val_acc: 0.525833\n",
      "(Epoch 13 / 100) train acc: 0.576000; val_acc: 0.535833\n",
      "(Iteration 501 / 3600) loss: 1.112080\n",
      "(Epoch 14 / 100) train acc: 0.561000; val_acc: 0.535000\n",
      "(Epoch 15 / 100) train acc: 0.591000; val_acc: 0.560833\n",
      "(Epoch 16 / 100) train acc: 0.628000; val_acc: 0.578333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iteration 601 / 3600) loss: 1.112292\n",
      "(Epoch 17 / 100) train acc: 0.621000; val_acc: 0.589167\n",
      "(Epoch 18 / 100) train acc: 0.602000; val_acc: 0.563333\n",
      "(Epoch 19 / 100) train acc: 0.628000; val_acc: 0.601667\n",
      "(Iteration 701 / 3600) loss: 0.848904\n",
      "(Epoch 20 / 100) train acc: 0.629000; val_acc: 0.622500\n",
      "(Epoch 21 / 100) train acc: 0.643000; val_acc: 0.618333\n",
      "(Epoch 22 / 100) train acc: 0.677000; val_acc: 0.634167\n",
      "(Iteration 801 / 3600) loss: 0.692289\n",
      "(Epoch 23 / 100) train acc: 0.623000; val_acc: 0.615833\n",
      "(Epoch 24 / 100) train acc: 0.640000; val_acc: 0.630000\n",
      "(Epoch 25 / 100) train acc: 0.648000; val_acc: 0.613333\n",
      "(Iteration 901 / 3600) loss: 0.880185\n",
      "(Epoch 26 / 100) train acc: 0.642000; val_acc: 0.658333\n",
      "(Epoch 27 / 100) train acc: 0.660000; val_acc: 0.672500\n",
      "(Iteration 1001 / 3600) loss: 0.817829\n",
      "(Epoch 28 / 100) train acc: 0.684000; val_acc: 0.662500\n",
      "(Epoch 29 / 100) train acc: 0.675000; val_acc: 0.665000\n",
      "(Epoch 30 / 100) train acc: 0.653000; val_acc: 0.662500\n",
      "(Iteration 1101 / 3600) loss: 0.897511\n",
      "(Epoch 31 / 100) train acc: 0.715000; val_acc: 0.683333\n",
      "(Epoch 32 / 100) train acc: 0.697000; val_acc: 0.671667\n",
      "(Epoch 33 / 100) train acc: 0.679000; val_acc: 0.668333\n",
      "(Iteration 1201 / 3600) loss: 0.838470\n",
      "(Epoch 34 / 100) train acc: 0.730000; val_acc: 0.680833\n",
      "(Epoch 35 / 100) train acc: 0.694000; val_acc: 0.694167\n",
      "(Epoch 36 / 100) train acc: 0.743000; val_acc: 0.699167\n",
      "(Iteration 1301 / 3600) loss: 0.887838\n",
      "(Epoch 37 / 100) train acc: 0.723000; val_acc: 0.717500\n",
      "(Epoch 38 / 100) train acc: 0.690000; val_acc: 0.700833\n",
      "(Iteration 1401 / 3600) loss: 0.875819\n",
      "(Epoch 39 / 100) train acc: 0.724000; val_acc: 0.684167\n",
      "(Epoch 40 / 100) train acc: 0.741000; val_acc: 0.715000\n",
      "(Epoch 41 / 100) train acc: 0.749000; val_acc: 0.720833\n",
      "(Iteration 1501 / 3600) loss: 0.644045\n",
      "(Epoch 42 / 100) train acc: 0.730000; val_acc: 0.730000\n",
      "(Epoch 43 / 100) train acc: 0.742000; val_acc: 0.725833\n",
      "(Epoch 44 / 100) train acc: 0.766000; val_acc: 0.732500\n",
      "(Iteration 1601 / 3600) loss: 0.753686\n",
      "(Epoch 45 / 100) train acc: 0.781000; val_acc: 0.741667\n",
      "(Epoch 46 / 100) train acc: 0.782000; val_acc: 0.740000\n",
      "(Epoch 47 / 100) train acc: 0.770000; val_acc: 0.765833\n",
      "(Iteration 1701 / 3600) loss: 0.806821\n",
      "(Epoch 48 / 100) train acc: 0.808000; val_acc: 0.770000\n",
      "(Epoch 49 / 100) train acc: 0.770000; val_acc: 0.760833\n",
      "(Epoch 50 / 100) train acc: 0.750000; val_acc: 0.736667\n",
      "(Iteration 1801 / 3600) loss: 0.538316\n",
      "(Epoch 51 / 100) train acc: 0.761000; val_acc: 0.747500\n",
      "(Epoch 52 / 100) train acc: 0.783000; val_acc: 0.772500\n",
      "(Iteration 1901 / 3600) loss: 0.672998\n",
      "(Epoch 53 / 100) train acc: 0.800000; val_acc: 0.778333\n",
      "(Epoch 54 / 100) train acc: 0.800000; val_acc: 0.762500\n",
      "(Epoch 55 / 100) train acc: 0.790000; val_acc: 0.768333\n",
      "(Iteration 2001 / 3600) loss: 0.593659\n",
      "(Epoch 56 / 100) train acc: 0.772000; val_acc: 0.765000\n",
      "(Epoch 57 / 100) train acc: 0.815000; val_acc: 0.779167\n",
      "(Epoch 58 / 100) train acc: 0.828000; val_acc: 0.795000\n",
      "(Iteration 2101 / 3600) loss: 0.530597\n",
      "(Epoch 59 / 100) train acc: 0.823000; val_acc: 0.778333\n",
      "(Epoch 60 / 100) train acc: 0.832000; val_acc: 0.790833\n",
      "(Epoch 61 / 100) train acc: 0.833000; val_acc: 0.785833\n",
      "(Iteration 2201 / 3600) loss: 0.555759\n",
      "(Epoch 62 / 100) train acc: 0.836000; val_acc: 0.785833\n",
      "(Epoch 63 / 100) train acc: 0.793000; val_acc: 0.798333\n",
      "(Iteration 2301 / 3600) loss: 0.806285\n",
      "(Epoch 64 / 100) train acc: 0.827000; val_acc: 0.795833\n",
      "(Epoch 65 / 100) train acc: 0.817000; val_acc: 0.797500\n",
      "(Epoch 66 / 100) train acc: 0.834000; val_acc: 0.796667\n",
      "(Iteration 2401 / 3600) loss: 0.649041\n",
      "(Epoch 67 / 100) train acc: 0.836000; val_acc: 0.808333\n",
      "(Epoch 68 / 100) train acc: 0.803000; val_acc: 0.784167\n",
      "(Epoch 69 / 100) train acc: 0.830000; val_acc: 0.810833\n",
      "(Iteration 2501 / 3600) loss: 0.425305\n",
      "(Epoch 70 / 100) train acc: 0.848000; val_acc: 0.802500\n",
      "(Epoch 71 / 100) train acc: 0.852000; val_acc: 0.792500\n",
      "(Epoch 72 / 100) train acc: 0.824000; val_acc: 0.806667\n",
      "(Iteration 2601 / 3600) loss: 0.596309\n",
      "(Epoch 73 / 100) train acc: 0.828000; val_acc: 0.811667\n",
      "(Epoch 74 / 100) train acc: 0.842000; val_acc: 0.802500\n",
      "(Epoch 75 / 100) train acc: 0.825000; val_acc: 0.802500\n",
      "(Iteration 2701 / 3600) loss: 0.617672\n",
      "(Epoch 76 / 100) train acc: 0.832000; val_acc: 0.805833\n",
      "(Epoch 77 / 100) train acc: 0.860000; val_acc: 0.819167\n",
      "(Iteration 2801 / 3600) loss: 0.564231\n",
      "(Epoch 78 / 100) train acc: 0.838000; val_acc: 0.811667\n",
      "(Epoch 79 / 100) train acc: 0.820000; val_acc: 0.803333\n",
      "(Epoch 80 / 100) train acc: 0.853000; val_acc: 0.790000\n",
      "(Iteration 2901 / 3600) loss: 0.521932\n",
      "(Epoch 81 / 100) train acc: 0.833000; val_acc: 0.816667\n",
      "(Epoch 82 / 100) train acc: 0.849000; val_acc: 0.816667\n",
      "(Epoch 83 / 100) train acc: 0.853000; val_acc: 0.815000\n",
      "(Iteration 3001 / 3600) loss: 0.591326\n",
      "(Epoch 84 / 100) train acc: 0.853000; val_acc: 0.818333\n",
      "(Epoch 85 / 100) train acc: 0.879000; val_acc: 0.815000\n",
      "(Epoch 86 / 100) train acc: 0.874000; val_acc: 0.825833\n",
      "(Iteration 3101 / 3600) loss: 0.483123\n",
      "(Epoch 87 / 100) train acc: 0.844000; val_acc: 0.827500\n",
      "(Epoch 88 / 100) train acc: 0.863000; val_acc: 0.822500\n",
      "(Iteration 3201 / 3600) loss: 0.535909\n",
      "(Epoch 89 / 100) train acc: 0.863000; val_acc: 0.829167\n",
      "(Epoch 90 / 100) train acc: 0.861000; val_acc: 0.815000\n",
      "(Epoch 91 / 100) train acc: 0.880000; val_acc: 0.825833\n",
      "(Iteration 3301 / 3600) loss: 0.445376\n",
      "(Epoch 92 / 100) train acc: 0.861000; val_acc: 0.828333\n",
      "(Epoch 93 / 100) train acc: 0.862000; val_acc: 0.817500\n",
      "(Epoch 94 / 100) train acc: 0.872000; val_acc: 0.838333\n",
      "(Iteration 3401 / 3600) loss: 0.382079\n",
      "(Epoch 95 / 100) train acc: 0.866000; val_acc: 0.833333\n",
      "(Epoch 96 / 100) train acc: 0.886000; val_acc: 0.832500\n",
      "(Epoch 97 / 100) train acc: 0.849000; val_acc: 0.833333\n",
      "(Iteration 3501 / 3600) loss: 0.554476\n",
      "(Epoch 98 / 100) train acc: 0.874000; val_acc: 0.822500\n",
      "(Epoch 99 / 100) train acc: 0.884000; val_acc: 0.839167\n",
      "(Epoch 100 / 100) train acc: 0.867000; val_acc: 0.838333\n",
      "(Iteration 1 / 3600) loss: 5.958040\n",
      "(Epoch 0 / 100) train acc: 0.126000; val_acc: 0.129167\n",
      "(Epoch 1 / 100) train acc: 0.137000; val_acc: 0.143333\n",
      "(Epoch 2 / 100) train acc: 0.202000; val_acc: 0.183333\n",
      "(Iteration 101 / 3600) loss: 2.018877\n",
      "(Epoch 3 / 100) train acc: 0.201000; val_acc: 0.199167\n",
      "(Epoch 4 / 100) train acc: 0.213000; val_acc: 0.201667\n",
      "(Epoch 5 / 100) train acc: 0.227000; val_acc: 0.215833\n",
      "(Iteration 201 / 3600) loss: 1.985827\n",
      "(Epoch 6 / 100) train acc: 0.276000; val_acc: 0.253333\n",
      "(Epoch 7 / 100) train acc: 0.236000; val_acc: 0.239167\n",
      "(Epoch 8 / 100) train acc: 0.271000; val_acc: 0.240000\n",
      "(Iteration 301 / 3600) loss: 1.887389\n",
      "(Epoch 9 / 100) train acc: 0.277000; val_acc: 0.281667\n",
      "(Epoch 10 / 100) train acc: 0.263000; val_acc: 0.256667\n",
      "(Epoch 11 / 100) train acc: 0.294000; val_acc: 0.293333\n",
      "(Iteration 401 / 3600) loss: 1.818252\n",
      "(Epoch 12 / 100) train acc: 0.311000; val_acc: 0.293333\n",
      "(Epoch 13 / 100) train acc: 0.294000; val_acc: 0.303333\n",
      "(Iteration 501 / 3600) loss: 1.813811\n",
      "(Epoch 14 / 100) train acc: 0.303000; val_acc: 0.293333\n",
      "(Epoch 15 / 100) train acc: 0.287000; val_acc: 0.310833\n",
      "(Epoch 16 / 100) train acc: 0.360000; val_acc: 0.317500\n",
      "(Iteration 601 / 3600) loss: 1.725693\n",
      "(Epoch 17 / 100) train acc: 0.371000; val_acc: 0.368333\n",
      "(Epoch 18 / 100) train acc: 0.430000; val_acc: 0.426667\n",
      "(Epoch 19 / 100) train acc: 0.458000; val_acc: 0.455000\n",
      "(Iteration 701 / 3600) loss: 1.650884\n",
      "(Epoch 20 / 100) train acc: 0.473000; val_acc: 0.457500\n",
      "(Epoch 21 / 100) train acc: 0.461000; val_acc: 0.442500\n",
      "(Epoch 22 / 100) train acc: 0.454000; val_acc: 0.458333\n",
      "(Iteration 801 / 3600) loss: 1.429015\n",
      "(Epoch 23 / 100) train acc: 0.483000; val_acc: 0.467500\n",
      "(Epoch 24 / 100) train acc: 0.453000; val_acc: 0.460833\n",
      "(Epoch 25 / 100) train acc: 0.488000; val_acc: 0.461667\n",
      "(Iteration 901 / 3600) loss: 1.401003\n",
      "(Epoch 26 / 100) train acc: 0.476000; val_acc: 0.460833\n",
      "(Epoch 27 / 100) train acc: 0.501000; val_acc: 0.480000\n",
      "(Iteration 1001 / 3600) loss: 1.498972\n",
      "(Epoch 28 / 100) train acc: 0.457000; val_acc: 0.470000\n",
      "(Epoch 29 / 100) train acc: 0.500000; val_acc: 0.483333\n",
      "(Epoch 30 / 100) train acc: 0.472000; val_acc: 0.483333\n",
      "(Iteration 1101 / 3600) loss: 1.409205\n",
      "(Epoch 31 / 100) train acc: 0.483000; val_acc: 0.467500\n",
      "(Epoch 32 / 100) train acc: 0.489000; val_acc: 0.495000\n",
      "(Epoch 33 / 100) train acc: 0.477000; val_acc: 0.475833\n",
      "(Iteration 1201 / 3600) loss: 1.235827\n",
      "(Epoch 34 / 100) train acc: 0.530000; val_acc: 0.500833\n",
      "(Epoch 35 / 100) train acc: 0.576000; val_acc: 0.535833\n",
      "(Epoch 36 / 100) train acc: 0.583000; val_acc: 0.526667\n",
      "(Iteration 1301 / 3600) loss: 1.477381\n",
      "(Epoch 37 / 100) train acc: 0.579000; val_acc: 0.545833\n",
      "(Epoch 38 / 100) train acc: 0.520000; val_acc: 0.537500\n",
      "(Iteration 1401 / 3600) loss: 1.487743\n",
      "(Epoch 39 / 100) train acc: 0.550000; val_acc: 0.505000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 40 / 100) train acc: 0.538000; val_acc: 0.540000\n",
      "(Epoch 41 / 100) train acc: 0.521000; val_acc: 0.505000\n",
      "(Iteration 1501 / 3600) loss: 1.219697\n",
      "(Epoch 42 / 100) train acc: 0.561000; val_acc: 0.533333\n",
      "(Epoch 43 / 100) train acc: 0.514000; val_acc: 0.512500\n",
      "(Epoch 44 / 100) train acc: 0.571000; val_acc: 0.540000\n",
      "(Iteration 1601 / 3600) loss: 1.381749\n",
      "(Epoch 45 / 100) train acc: 0.578000; val_acc: 0.560000\n",
      "(Epoch 46 / 100) train acc: 0.576000; val_acc: 0.559167\n",
      "(Epoch 47 / 100) train acc: 0.565000; val_acc: 0.550833\n",
      "(Iteration 1701 / 3600) loss: 1.261394\n",
      "(Epoch 48 / 100) train acc: 0.596000; val_acc: 0.565833\n",
      "(Epoch 49 / 100) train acc: 0.562000; val_acc: 0.572500\n",
      "(Epoch 50 / 100) train acc: 0.600000; val_acc: 0.570833\n",
      "(Iteration 1801 / 3600) loss: 1.119292\n",
      "(Epoch 51 / 100) train acc: 0.483000; val_acc: 0.514167\n",
      "(Epoch 52 / 100) train acc: 0.561000; val_acc: 0.533333\n",
      "(Iteration 1901 / 3600) loss: 1.205870\n",
      "(Epoch 53 / 100) train acc: 0.603000; val_acc: 0.579167\n",
      "(Epoch 54 / 100) train acc: 0.569000; val_acc: 0.579167\n",
      "(Epoch 55 / 100) train acc: 0.602000; val_acc: 0.587500\n",
      "(Iteration 2001 / 3600) loss: 1.160161\n",
      "(Epoch 56 / 100) train acc: 0.569000; val_acc: 0.572500\n",
      "(Epoch 57 / 100) train acc: 0.614000; val_acc: 0.576667\n",
      "(Epoch 58 / 100) train acc: 0.621000; val_acc: 0.584167\n",
      "(Iteration 2101 / 3600) loss: 1.042071\n",
      "(Epoch 59 / 100) train acc: 0.585000; val_acc: 0.581667\n",
      "(Epoch 60 / 100) train acc: 0.600000; val_acc: 0.585833\n",
      "(Epoch 61 / 100) train acc: 0.623000; val_acc: 0.590833\n",
      "(Iteration 2201 / 3600) loss: 1.176515\n",
      "(Epoch 62 / 100) train acc: 0.622000; val_acc: 0.599167\n",
      "(Epoch 63 / 100) train acc: 0.627000; val_acc: 0.595833\n",
      "(Iteration 2301 / 3600) loss: 1.115415\n",
      "(Epoch 64 / 100) train acc: 0.658000; val_acc: 0.600000\n",
      "(Epoch 65 / 100) train acc: 0.577000; val_acc: 0.504167\n",
      "(Epoch 66 / 100) train acc: 0.624000; val_acc: 0.595833\n",
      "(Iteration 2401 / 3600) loss: 1.149028\n",
      "(Epoch 67 / 100) train acc: 0.592000; val_acc: 0.559167\n",
      "(Epoch 68 / 100) train acc: 0.636000; val_acc: 0.604167\n",
      "(Epoch 69 / 100) train acc: 0.609000; val_acc: 0.601667\n",
      "(Iteration 2501 / 3600) loss: 1.141368\n",
      "(Epoch 70 / 100) train acc: 0.646000; val_acc: 0.617500\n",
      "(Epoch 71 / 100) train acc: 0.660000; val_acc: 0.606667\n",
      "(Epoch 72 / 100) train acc: 0.626000; val_acc: 0.605000\n",
      "(Iteration 2601 / 3600) loss: 1.312163\n",
      "(Epoch 73 / 100) train acc: 0.625000; val_acc: 0.612500\n",
      "(Epoch 74 / 100) train acc: 0.659000; val_acc: 0.624167\n",
      "(Epoch 75 / 100) train acc: 0.654000; val_acc: 0.609167\n",
      "(Iteration 2701 / 3600) loss: 1.056749\n",
      "(Epoch 76 / 100) train acc: 0.640000; val_acc: 0.617500\n",
      "(Epoch 77 / 100) train acc: 0.659000; val_acc: 0.630000\n",
      "(Iteration 2801 / 3600) loss: 1.018518\n",
      "(Epoch 78 / 100) train acc: 0.648000; val_acc: 0.616667\n",
      "(Epoch 79 / 100) train acc: 0.651000; val_acc: 0.623333\n",
      "(Epoch 80 / 100) train acc: 0.601000; val_acc: 0.575000\n",
      "(Iteration 2901 / 3600) loss: 1.027129\n",
      "(Epoch 81 / 100) train acc: 0.666000; val_acc: 0.634167\n",
      "(Epoch 82 / 100) train acc: 0.610000; val_acc: 0.590000\n",
      "(Epoch 83 / 100) train acc: 0.599000; val_acc: 0.574167\n",
      "(Iteration 3001 / 3600) loss: 1.096179\n",
      "(Epoch 84 / 100) train acc: 0.671000; val_acc: 0.630000\n",
      "(Epoch 85 / 100) train acc: 0.631000; val_acc: 0.614167\n",
      "(Epoch 86 / 100) train acc: 0.625000; val_acc: 0.603333\n",
      "(Iteration 3101 / 3600) loss: 0.965472\n",
      "(Epoch 87 / 100) train acc: 0.664000; val_acc: 0.641667\n",
      "(Epoch 88 / 100) train acc: 0.631000; val_acc: 0.631667\n",
      "(Iteration 3201 / 3600) loss: 1.590721\n",
      "(Epoch 89 / 100) train acc: 0.661000; val_acc: 0.642500\n",
      "(Epoch 90 / 100) train acc: 0.669000; val_acc: 0.634167\n",
      "(Epoch 91 / 100) train acc: 0.601000; val_acc: 0.565833\n",
      "(Iteration 3301 / 3600) loss: 1.035905\n",
      "(Epoch 92 / 100) train acc: 0.646000; val_acc: 0.640000\n",
      "(Epoch 93 / 100) train acc: 0.662000; val_acc: 0.640833\n",
      "(Epoch 94 / 100) train acc: 0.638000; val_acc: 0.614167\n",
      "(Iteration 3401 / 3600) loss: 1.025077\n",
      "(Epoch 95 / 100) train acc: 0.651000; val_acc: 0.620833\n",
      "(Epoch 96 / 100) train acc: 0.658000; val_acc: 0.634167\n",
      "(Epoch 97 / 100) train acc: 0.627000; val_acc: 0.637500\n",
      "(Iteration 3501 / 3600) loss: 0.944659\n",
      "(Epoch 98 / 100) train acc: 0.645000; val_acc: 0.633333\n",
      "(Epoch 99 / 100) train acc: 0.691000; val_acc: 0.644167\n",
      "(Epoch 100 / 100) train acc: 0.654000; val_acc: 0.627500\n",
      "(Iteration 1 / 3600) loss: 28.556532\n",
      "(Epoch 0 / 100) train acc: 0.157000; val_acc: 0.159167\n",
      "(Epoch 1 / 100) train acc: 0.338000; val_acc: 0.336667\n",
      "(Epoch 2 / 100) train acc: 0.324000; val_acc: 0.350833\n",
      "(Iteration 101 / 3600) loss: 1.750271\n",
      "(Epoch 3 / 100) train acc: 0.363000; val_acc: 0.384167\n",
      "(Epoch 4 / 100) train acc: 0.397000; val_acc: 0.392500\n",
      "(Epoch 5 / 100) train acc: 0.390000; val_acc: 0.399167\n",
      "(Iteration 201 / 3600) loss: 1.545952\n",
      "(Epoch 6 / 100) train acc: 0.364000; val_acc: 0.420833\n",
      "(Epoch 7 / 100) train acc: 0.430000; val_acc: 0.422500\n",
      "(Epoch 8 / 100) train acc: 0.477000; val_acc: 0.441667\n",
      "(Iteration 301 / 3600) loss: 1.627056\n",
      "(Epoch 9 / 100) train acc: 0.450000; val_acc: 0.455000\n",
      "(Epoch 10 / 100) train acc: 0.467000; val_acc: 0.475833\n",
      "(Epoch 11 / 100) train acc: 0.444000; val_acc: 0.457500\n",
      "(Iteration 401 / 3600) loss: 1.528585\n",
      "(Epoch 12 / 100) train acc: 0.494000; val_acc: 0.489167\n",
      "(Epoch 13 / 100) train acc: 0.461000; val_acc: 0.494167\n",
      "(Iteration 501 / 3600) loss: 1.537335\n",
      "(Epoch 14 / 100) train acc: 0.443000; val_acc: 0.432500\n",
      "(Epoch 15 / 100) train acc: 0.562000; val_acc: 0.576667\n",
      "(Epoch 16 / 100) train acc: 0.628000; val_acc: 0.589167\n",
      "(Iteration 601 / 3600) loss: 1.153297\n",
      "(Epoch 17 / 100) train acc: 0.624000; val_acc: 0.601667\n",
      "(Epoch 18 / 100) train acc: 0.658000; val_acc: 0.620000\n",
      "(Epoch 19 / 100) train acc: 0.673000; val_acc: 0.641667\n",
      "(Iteration 701 / 3600) loss: 0.900062\n",
      "(Epoch 20 / 100) train acc: 0.638000; val_acc: 0.639167\n",
      "(Epoch 21 / 100) train acc: 0.523000; val_acc: 0.524167\n",
      "(Epoch 22 / 100) train acc: 0.723000; val_acc: 0.680000\n",
      "(Iteration 801 / 3600) loss: 0.618675\n",
      "(Epoch 23 / 100) train acc: 0.517000; val_acc: 0.505000\n",
      "(Epoch 24 / 100) train acc: 0.734000; val_acc: 0.703333\n",
      "(Epoch 25 / 100) train acc: 0.709000; val_acc: 0.681667\n",
      "(Iteration 901 / 3600) loss: 0.758017\n",
      "(Epoch 26 / 100) train acc: 0.737000; val_acc: 0.722500\n",
      "(Epoch 27 / 100) train acc: 0.756000; val_acc: 0.733333\n",
      "(Iteration 1001 / 3600) loss: 0.760983\n",
      "(Epoch 28 / 100) train acc: 0.764000; val_acc: 0.745000\n",
      "(Epoch 29 / 100) train acc: 0.802000; val_acc: 0.786667\n",
      "(Epoch 30 / 100) train acc: 0.738000; val_acc: 0.735833\n",
      "(Iteration 1101 / 3600) loss: 0.614749\n",
      "(Epoch 31 / 100) train acc: 0.690000; val_acc: 0.715000\n",
      "(Epoch 32 / 100) train acc: 0.814000; val_acc: 0.809167\n",
      "(Epoch 33 / 100) train acc: 0.807000; val_acc: 0.776667\n",
      "(Iteration 1201 / 3600) loss: 0.599960\n",
      "(Epoch 34 / 100) train acc: 0.857000; val_acc: 0.812500\n",
      "(Epoch 35 / 100) train acc: 0.810000; val_acc: 0.785833\n",
      "(Epoch 36 / 100) train acc: 0.866000; val_acc: 0.832500\n",
      "(Iteration 1301 / 3600) loss: 0.747804\n",
      "(Epoch 37 / 100) train acc: 0.864000; val_acc: 0.838333\n",
      "(Epoch 38 / 100) train acc: 0.743000; val_acc: 0.781667\n",
      "(Iteration 1401 / 3600) loss: 0.747181\n",
      "(Epoch 39 / 100) train acc: 0.851000; val_acc: 0.831667\n",
      "(Epoch 40 / 100) train acc: 0.853000; val_acc: 0.840000\n",
      "(Epoch 41 / 100) train acc: 0.847000; val_acc: 0.845000\n",
      "(Iteration 1501 / 3600) loss: 0.382451\n",
      "(Epoch 42 / 100) train acc: 0.869000; val_acc: 0.846667\n",
      "(Epoch 43 / 100) train acc: 0.890000; val_acc: 0.855000\n",
      "(Epoch 44 / 100) train acc: 0.889000; val_acc: 0.863333\n",
      "(Iteration 1601 / 3600) loss: 0.467794\n",
      "(Epoch 45 / 100) train acc: 0.886000; val_acc: 0.850000\n",
      "(Epoch 46 / 100) train acc: 0.899000; val_acc: 0.864167\n",
      "(Epoch 47 / 100) train acc: 0.848000; val_acc: 0.853333\n",
      "(Iteration 1701 / 3600) loss: 0.627563\n",
      "(Epoch 48 / 100) train acc: 0.886000; val_acc: 0.855833\n",
      "(Epoch 49 / 100) train acc: 0.835000; val_acc: 0.847500\n",
      "(Epoch 50 / 100) train acc: 0.865000; val_acc: 0.849167\n",
      "(Iteration 1801 / 3600) loss: 0.286874\n",
      "(Epoch 51 / 100) train acc: 0.841000; val_acc: 0.842500\n",
      "(Epoch 52 / 100) train acc: 0.892000; val_acc: 0.864167\n",
      "(Iteration 1901 / 3600) loss: 0.390913\n",
      "(Epoch 53 / 100) train acc: 0.894000; val_acc: 0.873333\n",
      "(Epoch 54 / 100) train acc: 0.538000; val_acc: 0.511667\n",
      "(Epoch 55 / 100) train acc: 0.897000; val_acc: 0.881667\n",
      "(Iteration 2001 / 3600) loss: 0.254404\n",
      "(Epoch 56 / 100) train acc: 0.842000; val_acc: 0.820000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 57 / 100) train acc: 0.898000; val_acc: 0.878333\n",
      "(Epoch 58 / 100) train acc: 0.915000; val_acc: 0.885833\n",
      "(Iteration 2101 / 3600) loss: 0.353443\n",
      "(Epoch 59 / 100) train acc: 0.879000; val_acc: 0.860000\n",
      "(Epoch 60 / 100) train acc: 0.904000; val_acc: 0.877500\n",
      "(Epoch 61 / 100) train acc: 0.883000; val_acc: 0.870833\n",
      "(Iteration 2201 / 3600) loss: 0.391708\n",
      "(Epoch 62 / 100) train acc: 0.902000; val_acc: 0.876667\n",
      "(Epoch 63 / 100) train acc: 0.890000; val_acc: 0.879167\n",
      "(Iteration 2301 / 3600) loss: 0.497693\n",
      "(Epoch 64 / 100) train acc: 0.901000; val_acc: 0.887500\n",
      "(Epoch 65 / 100) train acc: 0.903000; val_acc: 0.880833\n",
      "(Epoch 66 / 100) train acc: 0.913000; val_acc: 0.874167\n",
      "(Iteration 2401 / 3600) loss: 0.558116\n",
      "(Epoch 67 / 100) train acc: 0.912000; val_acc: 0.889167\n",
      "(Epoch 68 / 100) train acc: 0.914000; val_acc: 0.880833\n",
      "(Epoch 69 / 100) train acc: 0.913000; val_acc: 0.890833\n",
      "(Iteration 2501 / 3600) loss: 0.336330\n",
      "(Epoch 70 / 100) train acc: 0.908000; val_acc: 0.887500\n",
      "(Epoch 71 / 100) train acc: 0.903000; val_acc: 0.879167\n",
      "(Epoch 72 / 100) train acc: 0.522000; val_acc: 0.511667\n",
      "(Iteration 2601 / 3600) loss: 0.406304\n",
      "(Epoch 73 / 100) train acc: 0.911000; val_acc: 0.888333\n",
      "(Epoch 74 / 100) train acc: 0.922000; val_acc: 0.885000\n",
      "(Epoch 75 / 100) train acc: 0.909000; val_acc: 0.887500\n",
      "(Iteration 2701 / 3600) loss: 0.300477\n",
      "(Epoch 76 / 100) train acc: 0.900000; val_acc: 0.882500\n",
      "(Epoch 77 / 100) train acc: 0.914000; val_acc: 0.890833\n",
      "(Iteration 2801 / 3600) loss: 0.289582\n",
      "(Epoch 78 / 100) train acc: 0.918000; val_acc: 0.887500\n",
      "(Epoch 79 / 100) train acc: 0.905000; val_acc: 0.885833\n",
      "(Epoch 80 / 100) train acc: 0.938000; val_acc: 0.898333\n",
      "(Iteration 2901 / 3600) loss: 0.331051\n",
      "(Epoch 81 / 100) train acc: 0.918000; val_acc: 0.895833\n",
      "(Epoch 82 / 100) train acc: 0.900000; val_acc: 0.875000\n",
      "(Epoch 83 / 100) train acc: 0.907000; val_acc: 0.890000\n",
      "(Iteration 3001 / 3600) loss: 0.312687\n",
      "(Epoch 84 / 100) train acc: 0.933000; val_acc: 0.898333\n",
      "(Epoch 85 / 100) train acc: 0.932000; val_acc: 0.895000\n",
      "(Epoch 86 / 100) train acc: 0.932000; val_acc: 0.875833\n",
      "(Iteration 3101 / 3600) loss: 0.335713\n",
      "(Epoch 87 / 100) train acc: 0.912000; val_acc: 0.879167\n",
      "(Epoch 88 / 100) train acc: 0.918000; val_acc: 0.899167\n",
      "(Iteration 3201 / 3600) loss: 0.238597\n",
      "(Epoch 89 / 100) train acc: 0.938000; val_acc: 0.900000\n",
      "(Epoch 90 / 100) train acc: 0.897000; val_acc: 0.890000\n",
      "(Epoch 91 / 100) train acc: 0.920000; val_acc: 0.890000\n",
      "(Iteration 3301 / 3600) loss: 0.257817\n",
      "(Epoch 92 / 100) train acc: 0.935000; val_acc: 0.893333\n",
      "(Epoch 93 / 100) train acc: 0.825000; val_acc: 0.801667\n",
      "(Epoch 94 / 100) train acc: 0.932000; val_acc: 0.898333\n",
      "(Iteration 3401 / 3600) loss: 0.158647\n",
      "(Epoch 95 / 100) train acc: 0.913000; val_acc: 0.891667\n",
      "(Epoch 96 / 100) train acc: 0.930000; val_acc: 0.895833\n",
      "(Epoch 97 / 100) train acc: 0.915000; val_acc: 0.902500\n",
      "(Iteration 3501 / 3600) loss: 0.368618\n",
      "(Epoch 98 / 100) train acc: 0.929000; val_acc: 0.902500\n",
      "(Epoch 99 / 100) train acc: 0.944000; val_acc: 0.891667\n",
      "(Epoch 100 / 100) train acc: 0.934000; val_acc: 0.894167\n",
      "(Iteration 1 / 3600) loss: 14.632046\n",
      "(Epoch 0 / 100) train acc: 0.155000; val_acc: 0.148333\n",
      "(Epoch 1 / 100) train acc: 0.380000; val_acc: 0.359167\n",
      "(Epoch 2 / 100) train acc: 0.438000; val_acc: 0.445000\n",
      "(Iteration 101 / 3600) loss: 1.317042\n",
      "(Epoch 3 / 100) train acc: 0.537000; val_acc: 0.501667\n",
      "(Epoch 4 / 100) train acc: 0.564000; val_acc: 0.505000\n",
      "(Epoch 5 / 100) train acc: 0.610000; val_acc: 0.580833\n",
      "(Iteration 201 / 3600) loss: 1.003451\n",
      "(Epoch 6 / 100) train acc: 0.580000; val_acc: 0.562500\n",
      "(Epoch 7 / 100) train acc: 0.578000; val_acc: 0.529167\n",
      "(Epoch 8 / 100) train acc: 0.711000; val_acc: 0.643333\n",
      "(Iteration 301 / 3600) loss: 1.097042\n",
      "(Epoch 9 / 100) train acc: 0.716000; val_acc: 0.657500\n",
      "(Epoch 10 / 100) train acc: 0.611000; val_acc: 0.579167\n",
      "(Epoch 11 / 100) train acc: 0.622000; val_acc: 0.600833\n",
      "(Iteration 401 / 3600) loss: 0.668737\n",
      "(Epoch 12 / 100) train acc: 0.759000; val_acc: 0.697500\n",
      "(Epoch 13 / 100) train acc: 0.692000; val_acc: 0.628333\n",
      "(Iteration 501 / 3600) loss: 0.772019\n",
      "(Epoch 14 / 100) train acc: 0.786000; val_acc: 0.739167\n",
      "(Epoch 15 / 100) train acc: 0.778000; val_acc: 0.745000\n",
      "(Epoch 16 / 100) train acc: 0.784000; val_acc: 0.741667\n",
      "(Iteration 601 / 3600) loss: 0.698202\n",
      "(Epoch 17 / 100) train acc: 0.815000; val_acc: 0.768333\n",
      "(Epoch 18 / 100) train acc: 0.825000; val_acc: 0.794167\n",
      "(Epoch 19 / 100) train acc: 0.832000; val_acc: 0.765833\n",
      "(Iteration 701 / 3600) loss: 0.436500\n",
      "(Epoch 20 / 100) train acc: 0.822000; val_acc: 0.767500\n",
      "(Epoch 21 / 100) train acc: 0.762000; val_acc: 0.725833\n",
      "(Epoch 22 / 100) train acc: 0.849000; val_acc: 0.777500\n",
      "(Iteration 801 / 3600) loss: 0.403484\n",
      "(Epoch 23 / 100) train acc: 0.823000; val_acc: 0.791667\n",
      "(Epoch 24 / 100) train acc: 0.859000; val_acc: 0.801667\n",
      "(Epoch 25 / 100) train acc: 0.869000; val_acc: 0.798333\n",
      "(Iteration 901 / 3600) loss: 0.482737\n",
      "(Epoch 26 / 100) train acc: 0.829000; val_acc: 0.799167\n",
      "(Epoch 27 / 100) train acc: 0.873000; val_acc: 0.820833\n",
      "(Iteration 1001 / 3600) loss: 0.488453\n",
      "(Epoch 28 / 100) train acc: 0.864000; val_acc: 0.828333\n",
      "(Epoch 29 / 100) train acc: 0.858000; val_acc: 0.811667\n",
      "(Epoch 30 / 100) train acc: 0.875000; val_acc: 0.835000\n",
      "(Iteration 1101 / 3600) loss: 0.465973\n",
      "(Epoch 31 / 100) train acc: 0.877000; val_acc: 0.837500\n",
      "(Epoch 32 / 100) train acc: 0.865000; val_acc: 0.855833\n",
      "(Epoch 33 / 100) train acc: 0.893000; val_acc: 0.839167\n",
      "(Iteration 1201 / 3600) loss: 0.408843\n",
      "(Epoch 34 / 100) train acc: 0.891000; val_acc: 0.854167\n",
      "(Epoch 35 / 100) train acc: 0.870000; val_acc: 0.833333\n",
      "(Epoch 36 / 100) train acc: 0.908000; val_acc: 0.860833\n",
      "(Iteration 1301 / 3600) loss: 0.452563\n",
      "(Epoch 37 / 100) train acc: 0.905000; val_acc: 0.863333\n",
      "(Epoch 38 / 100) train acc: 0.888000; val_acc: 0.855833\n",
      "(Iteration 1401 / 3600) loss: 0.491953\n",
      "(Epoch 39 / 100) train acc: 0.901000; val_acc: 0.844167\n",
      "(Epoch 40 / 100) train acc: 0.884000; val_acc: 0.866667\n",
      "(Epoch 41 / 100) train acc: 0.912000; val_acc: 0.869167\n",
      "(Iteration 1501 / 3600) loss: 0.293849\n",
      "(Epoch 42 / 100) train acc: 0.905000; val_acc: 0.869167\n",
      "(Epoch 43 / 100) train acc: 0.917000; val_acc: 0.864167\n",
      "(Epoch 44 / 100) train acc: 0.926000; val_acc: 0.872500\n",
      "(Iteration 1601 / 3600) loss: 0.570634\n",
      "(Epoch 45 / 100) train acc: 0.910000; val_acc: 0.847500\n",
      "(Epoch 46 / 100) train acc: 0.921000; val_acc: 0.876667\n",
      "(Epoch 47 / 100) train acc: 0.881000; val_acc: 0.861667\n",
      "(Iteration 1701 / 3600) loss: 0.423014\n",
      "(Epoch 48 / 100) train acc: 0.933000; val_acc: 0.872500\n",
      "(Epoch 49 / 100) train acc: 0.907000; val_acc: 0.877500\n",
      "(Epoch 50 / 100) train acc: 0.736000; val_acc: 0.718333\n",
      "(Iteration 1801 / 3600) loss: 0.722410\n",
      "(Epoch 51 / 100) train acc: 0.916000; val_acc: 0.878333\n",
      "(Epoch 52 / 100) train acc: 0.921000; val_acc: 0.882500\n",
      "(Iteration 1901 / 3600) loss: 0.328563\n",
      "(Epoch 53 / 100) train acc: 0.932000; val_acc: 0.884167\n",
      "(Epoch 54 / 100) train acc: 0.923000; val_acc: 0.882500\n",
      "(Epoch 55 / 100) train acc: 0.940000; val_acc: 0.885000\n",
      "(Iteration 2001 / 3600) loss: 0.154575\n",
      "(Epoch 56 / 100) train acc: 0.899000; val_acc: 0.857500\n",
      "(Epoch 57 / 100) train acc: 0.933000; val_acc: 0.885833\n",
      "(Epoch 58 / 100) train acc: 0.929000; val_acc: 0.882500\n",
      "(Iteration 2101 / 3600) loss: 0.247331\n",
      "(Epoch 59 / 100) train acc: 0.927000; val_acc: 0.881667\n",
      "(Epoch 60 / 100) train acc: 0.928000; val_acc: 0.888333\n",
      "(Epoch 61 / 100) train acc: 0.927000; val_acc: 0.893333\n",
      "(Iteration 2201 / 3600) loss: 0.228337\n",
      "(Epoch 62 / 100) train acc: 0.933000; val_acc: 0.885833\n",
      "(Epoch 63 / 100) train acc: 0.919000; val_acc: 0.899167\n",
      "(Iteration 2301 / 3600) loss: 0.397929\n",
      "(Epoch 64 / 100) train acc: 0.936000; val_acc: 0.891667\n",
      "(Epoch 65 / 100) train acc: 0.936000; val_acc: 0.894167\n",
      "(Epoch 66 / 100) train acc: 0.912000; val_acc: 0.882500\n",
      "(Iteration 2401 / 3600) loss: 0.344699\n",
      "(Epoch 67 / 100) train acc: 0.931000; val_acc: 0.893333\n",
      "(Epoch 68 / 100) train acc: 0.934000; val_acc: 0.890833\n",
      "(Epoch 69 / 100) train acc: 0.935000; val_acc: 0.890000\n",
      "(Iteration 2501 / 3600) loss: 0.160432\n",
      "(Epoch 70 / 100) train acc: 0.938000; val_acc: 0.893333\n",
      "(Epoch 71 / 100) train acc: 0.933000; val_acc: 0.889167\n",
      "(Epoch 72 / 100) train acc: 0.929000; val_acc: 0.887500\n",
      "(Iteration 2601 / 3600) loss: 0.204027\n",
      "(Epoch 73 / 100) train acc: 0.939000; val_acc: 0.900833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Epoch 74 / 100) train acc: 0.944000; val_acc: 0.885833\n",
      "(Epoch 75 / 100) train acc: 0.942000; val_acc: 0.897500\n",
      "(Iteration 2701 / 3600) loss: 0.232356\n",
      "(Epoch 76 / 100) train acc: 0.923000; val_acc: 0.895833\n",
      "(Epoch 77 / 100) train acc: 0.936000; val_acc: 0.899167\n",
      "(Iteration 2801 / 3600) loss: 0.167208\n",
      "(Epoch 78 / 100) train acc: 0.933000; val_acc: 0.890000\n",
      "(Epoch 79 / 100) train acc: 0.939000; val_acc: 0.900000\n",
      "(Epoch 80 / 100) train acc: 0.937000; val_acc: 0.890000\n",
      "(Iteration 2901 / 3600) loss: 0.105098\n",
      "(Epoch 81 / 100) train acc: 0.936000; val_acc: 0.906667\n",
      "(Epoch 82 / 100) train acc: 0.933000; val_acc: 0.893333\n",
      "(Epoch 83 / 100) train acc: 0.931000; val_acc: 0.894167\n",
      "(Iteration 3001 / 3600) loss: 0.285609\n",
      "(Epoch 84 / 100) train acc: 0.942000; val_acc: 0.897500\n",
      "(Epoch 85 / 100) train acc: 0.935000; val_acc: 0.897500\n",
      "(Epoch 86 / 100) train acc: 0.959000; val_acc: 0.901667\n",
      "(Iteration 3101 / 3600) loss: 0.225823\n",
      "(Epoch 87 / 100) train acc: 0.948000; val_acc: 0.891667\n",
      "(Epoch 88 / 100) train acc: 0.934000; val_acc: 0.900833\n",
      "(Iteration 3201 / 3600) loss: 0.230549\n",
      "(Epoch 89 / 100) train acc: 0.958000; val_acc: 0.899167\n",
      "(Epoch 90 / 100) train acc: 0.930000; val_acc: 0.900000\n",
      "(Epoch 91 / 100) train acc: 0.953000; val_acc: 0.900833\n",
      "(Iteration 3301 / 3600) loss: 0.162001\n",
      "(Epoch 92 / 100) train acc: 0.954000; val_acc: 0.902500\n",
      "(Epoch 93 / 100) train acc: 0.942000; val_acc: 0.894167\n",
      "(Epoch 94 / 100) train acc: 0.956000; val_acc: 0.905833\n",
      "(Iteration 3401 / 3600) loss: 0.148451\n",
      "(Epoch 95 / 100) train acc: 0.951000; val_acc: 0.900000\n",
      "(Epoch 96 / 100) train acc: 0.950000; val_acc: 0.900833\n",
      "(Epoch 97 / 100) train acc: 0.934000; val_acc: 0.900833\n",
      "(Iteration 3501 / 3600) loss: 0.135108\n",
      "(Epoch 98 / 100) train acc: 0.954000; val_acc: 0.900833\n",
      "(Epoch 99 / 100) train acc: 0.963000; val_acc: 0.902500\n",
      "(Epoch 100 / 100) train acc: 0.944000; val_acc: 0.895833\n"
     ]
    }
   ],
   "source": [
    "np.random.seed(55)\n",
    "accu5 = []\n",
    "data = {\n",
    "      'X_train': feat_train,\n",
    "      'y_train': label_train,\n",
    "      'X_val': feat_val,\n",
    "      'y_val': label_val}\n",
    "\n",
    "# TODO: fill out the hyperparamets\n",
    "hyperparams = {'lr_decay': 1,\n",
    "               'num_epochs': 100,\n",
    "               'batch_size': 100,\n",
    "               'learning_rate': 0.001 \n",
    "              }\n",
    "\n",
    "# TODO: fill out the number of units in your hidden layers\n",
    "# this should be a list of units for each hiddent layer\n",
    "\n",
    "for i in range(5):\n",
    "    hidden_dim = hidden_dims['h%d'%(i+1)]\n",
    "\n",
    "    model = FullyConnectedNet(input_dim=75,\n",
    "                          hidden_dim=hidden_dim)\n",
    "    solver = Solver(model, data,\n",
    "                update_rule='sgd',\n",
    "                optim_config={\n",
    "                  'learning_rate': hyperparams['learning_rate'],\n",
    "                },\n",
    "                lr_decay=hyperparams['lr_decay'],\n",
    "                num_epochs=hyperparams['num_epochs'], \n",
    "                batch_size=hyperparams['batch_size'],\n",
    "                print_every=100)\n",
    "    np.random.seed(55)\n",
    "    solver.train()\n",
    "    accu5.append(solver.best_val_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 0.906666666667\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f89b27634a8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(1,6), accu5)\n",
    "print(range(1,6)[np.argmax(accu5)],np.max(accu5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

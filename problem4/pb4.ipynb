{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils import data\n",
    "from mds189 import Mds189\n",
    "import numpy as np\n",
    "from skimage import io, transform\n",
    "import ipdb\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision import transforms\n",
    "import torchvision.models as models\n",
    "from PIL import Image\n",
    "import time\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions for loading images.\n",
    "def pil_loader(path):\n",
    "    # open path as file to avoid ResourceWarning (https://github.com/python-pillow/Pillow/issues/835)\n",
    "    with open(path, 'rb') as f:\n",
    "        img = Image.open(f)\n",
    "        return img.convert('RGB')\n",
    "\n",
    "def accimage_loader(path):\n",
    "    import accimage\n",
    "    try:\n",
    "        return accimage.Image(path)\n",
    "    except IOError:\n",
    "        # Potentially a decoding problem, fall back to PIL.Image\n",
    "        return pil_loader(path)\n",
    "\n",
    "def default_loader(path):\n",
    "    from torchvision import get_image_backend\n",
    "    if get_image_backend() == 'accimage':\n",
    "        return accimage_loader(path)\n",
    "    else:\n",
    "        return pil_loader(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag for whether you're training or not\n",
    "is_train = True\n",
    "is_key_frame = True # TODO: set this to false to train on the video frames, instead of the key frames\n",
    "model_to_load = 'model.ckpt' # This is the model to load during testing, if you want to eval a previously-trained model.\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "# Parameters for data loader\n",
    "params = {'batch_size': 64,  # TODO: fill in the batch size. often, these are things like 32,64,128,or 256\n",
    "          'shuffle': True,\n",
    "          'num_workers': 30 \n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# TODO: Hyper-parameters\n",
    "num_epochs = 15\n",
    "learning_rate = 0.0001\n",
    "# NOTE: depending on your optimizer, you may want to tune other hyperparameters as well\n",
    "\n",
    "# Datasets\n",
    "# TODO: put the path to your train, test, validation txt files\n",
    "if is_key_frame:\n",
    "    label_file_train =  './data/keyframe_data_train.txt'\n",
    "    label_file_val  =  './data/keyframe_data_val.txt'\n",
    "    # NOTE: the kaggle competition test data is only for the video frames, not the key frames\n",
    "    # this is why we don't have an equivalent label_file_test with keyframes\n",
    "else:\n",
    "    label_file_train = './data/videoframe_data_train.txt'\n",
    "    label_file_val = './data/videoframe_data_val.txt'\n",
    "    label_file_test = './data/videoframe_data_test.txt'\n",
    "\n",
    "# TODO: you should normalize based on the average image in the training set. This shows \n",
    "# an example of doing normalization\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "# TODO: if you want to pad or resize your images, you can put the parameters for that below.\n",
    "\n",
    "# Generators\n",
    "# NOTE: if you don't want to pad or resize your images, you should delete the Pad and Resize\n",
    "# transforms from all three _dataset definitions.\n",
    "train_dataset = Mds189(label_file_train,loader=default_loader,transform=transforms.Compose([\n",
    "                                               #transforms.Pad(1),    # TODO: if you want to pad your images\n",
    "                                               #transforms.Resize(256), # TODO: if you want to resize your images\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize(mean, std)\n",
    "                                           ]))\n",
    "train_loader = data.DataLoader(train_dataset, **params)\n",
    "\n",
    "val_dataset = Mds189(label_file_val,loader=default_loader,transform=transforms.Compose([\n",
    "                                               #transforms.Pad(1),\n",
    "                                               #transforms.Resize(256),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize(mean, std)\n",
    "                                           ]))\n",
    "val_loader = data.DataLoader(val_dataset, **params)\n",
    "\n",
    "if not is_key_frame:\n",
    "    test_dataset = Mds189(label_file_test,loader=default_loader,transform=transforms.Compose([\n",
    "                                                   #transforms.Pad(1),\n",
    "                                                   #transforms.Resize(256),\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize(mean, std)\n",
    "                                               ]))\n",
    "    test_loader = data.DataLoader(test_dataset, **params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: one way of defining your model architecture is to fill in a class like NeuralNet()\n",
    "# NOTE: you should not overwrite the models you try whose performance you're keeping track of.\n",
    "#       one thing you could do is have many different model forward passes in class NeuralNet()\n",
    "#       and then depending on which model you want to train/evaluate, you call that model's\n",
    "#       forward pass. this strategy will save you a lot of time in the long run. the last thing\n",
    "#       you want to do is have to recode the layer structure for a model (whose performance\n",
    "#       you're reporting) because you forgot to e.g., compute the confusion matrix on its results\n",
    "#       or visualize the error modes of your (best) model\n",
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        # you can define some common layers, for example: \n",
    "        #self.conv1 = nn.Conv2d(3, 448, 224) # you should review the definition of nn.Conv2d online\n",
    "        #self.pool = nn.MaxPool2d(2, 2)\n",
    "        #super(Net, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(3, 6, 5, stride=1),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(6, 16, 5, stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(16, 32, 5, stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(32, 64, 5, stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            nn.Conv2d(64, 128, 5, stride=1, padding=0),\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(2, 2),\n",
    "            \n",
    "        )\n",
    " \n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(3840, 1280),\n",
    "            nn.Linear(1280, 840),\n",
    "            nn.Linear(840, 120),\n",
    "            nn.Linear(120, 84)\n",
    "        )\n",
    "        \n",
    "        self.sm = nn.Linear(84, 8)\n",
    "        # note: input_dimensions and output_dimensions are not defined, they\n",
    "        # are placeholders to show you what arguments to pass to nn.Linear \n",
    "        #self.fc1 = nn.Linear(3*448*244, 8)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # now you can use the layers you defined, to write the forward pass, i.e.,\n",
    "        # network architecture for your model\n",
    "        #x = self.pool(F.relu(self.conv1(x))) # x -> convolution -> ReLU -> max pooling\n",
    "        # Tensors need to be reshaped before going into an fc layer\n",
    "        # the -1 will correspond to the batch size\n",
    "        #x = x.view(x.size(0),-1) \n",
    "        #x = F.relu(self.fc1(x)) # x -> fc (affine) layer -> relu\n",
    "        out = self.conv(x)\n",
    "        out = out.view(out.size(0), -1)\n",
    "        out = self.fc(out)\n",
    "        return F.log_softmax(self.sm(out), dim=1)\n",
    "\n",
    "        #return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training..\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    w.join()\n",
      "Traceback (most recent call last):\n",
      "    w.join()\n",
      "    w.join()\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    w.join()\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "AssertionError: can only join a child process\n",
      "AssertionError: can only join a child process\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    w.join()\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "Traceback (most recent call last):\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    w.join()\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    w.join()\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "AssertionError: can only join a child process\n",
      "    w.join()\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    self._shutdown_workers()\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    w.join()\n",
      "    w.join()\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Traceback (most recent call last):\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "AssertionError: can only join a child process\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    w.join()\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    w.join()\n",
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "AssertionError: can only join a child process\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "AssertionError: can only join a child process\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n",
      "AssertionError: can only join a child process\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Exception ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x7f0e13708668>>\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 717, in __del__\n",
      "    self._shutdown_workers()\n",
      "    self._shutdown_workers()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 713, in _shutdown_workers\n",
      "    w.join()\n",
      "    w.join()\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "  File \"/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/multiprocessing/process.py\", line 122, in join\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AssertionError: can only join a child process\n",
      "    assert self._parent_pid == os.getpid(), 'can only join a child process'\n",
      "AssertionError: can only join a child process\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/15], Step [4/46], Loss: 133.0071\n",
      "Epoch [1/15], Step [8/46], Loss: 132.0062\n",
      "Epoch [1/15], Step [12/46], Loss: 131.0213\n",
      "Epoch [1/15], Step [16/46], Loss: 130.1960\n",
      "Epoch [1/15], Step [20/46], Loss: 132.3183\n",
      "Epoch [1/15], Step [24/46], Loss: 130.8331\n",
      "Epoch [1/15], Step [28/46], Loss: 130.5978\n",
      "Epoch [1/15], Step [32/46], Loss: 131.3198\n",
      "Epoch [1/15], Step [36/46], Loss: 136.5039\n",
      "Epoch [1/15], Step [40/46], Loss: 127.9607\n",
      "Epoch [1/15], Step [44/46], Loss: 129.9454\n",
      "epoch 1\n",
      "Epoch [2/15], Step [4/46], Loss: 132.2227\n",
      "Epoch [2/15], Step [8/46], Loss: 129.6964\n",
      "Epoch [2/15], Step [12/46], Loss: 129.2626\n",
      "Epoch [2/15], Step [16/46], Loss: 129.3888\n",
      "Epoch [2/15], Step [20/46], Loss: 126.4728\n",
      "Epoch [2/15], Step [24/46], Loss: 129.2369\n",
      "Epoch [2/15], Step [28/46], Loss: 127.7464\n",
      "Epoch [2/15], Step [32/46], Loss: 115.1990\n",
      "Epoch [2/15], Step [36/46], Loss: 120.6806\n",
      "Epoch [2/15], Step [40/46], Loss: 114.6075\n",
      "Epoch [2/15], Step [44/46], Loss: 133.2165\n",
      "epoch 2\n",
      "Epoch [3/15], Step [4/46], Loss: 115.8913\n",
      "Epoch [3/15], Step [8/46], Loss: 105.4892\n",
      "Epoch [3/15], Step [12/46], Loss: 103.0956\n",
      "Epoch [3/15], Step [16/46], Loss: 90.6927\n",
      "Epoch [3/15], Step [20/46], Loss: 110.3280\n",
      "Epoch [3/15], Step [24/46], Loss: 100.6006\n",
      "Epoch [3/15], Step [28/46], Loss: 94.7006\n",
      "Epoch [3/15], Step [32/46], Loss: 104.6944\n",
      "Epoch [3/15], Step [36/46], Loss: 93.6801\n",
      "Epoch [3/15], Step [40/46], Loss: 89.6197\n",
      "Epoch [3/15], Step [44/46], Loss: 90.2380\n",
      "epoch 3\n",
      "Epoch [4/15], Step [4/46], Loss: 116.4998\n",
      "Epoch [4/15], Step [8/46], Loss: 93.5551\n",
      "Epoch [4/15], Step [12/46], Loss: 83.7032\n",
      "Epoch [4/15], Step [16/46], Loss: 89.0615\n",
      "Epoch [4/15], Step [20/46], Loss: 107.4451\n",
      "Epoch [4/15], Step [24/46], Loss: 89.6190\n",
      "Epoch [4/15], Step [28/46], Loss: 96.6793\n",
      "Epoch [4/15], Step [32/46], Loss: 75.0043\n",
      "Epoch [4/15], Step [36/46], Loss: 81.1185\n",
      "Epoch [4/15], Step [40/46], Loss: 85.3657\n",
      "Epoch [4/15], Step [44/46], Loss: 105.0885\n",
      "epoch 4\n",
      "Epoch [5/15], Step [4/46], Loss: 90.5826\n",
      "Epoch [5/15], Step [8/46], Loss: 76.6323\n",
      "Epoch [5/15], Step [12/46], Loss: 78.6729\n",
      "Epoch [5/15], Step [16/46], Loss: 89.0678\n",
      "Epoch [5/15], Step [20/46], Loss: 64.2872\n",
      "Epoch [5/15], Step [24/46], Loss: 80.7356\n",
      "Epoch [5/15], Step [28/46], Loss: 72.3777\n",
      "Epoch [5/15], Step [32/46], Loss: 77.9249\n",
      "Epoch [5/15], Step [36/46], Loss: 75.3092\n",
      "Epoch [5/15], Step [40/46], Loss: 79.9542\n",
      "Epoch [5/15], Step [44/46], Loss: 75.1350\n",
      "epoch 5\n",
      "Epoch [6/15], Step [4/46], Loss: 64.2249\n",
      "Epoch [6/15], Step [8/46], Loss: 63.1781\n",
      "Epoch [6/15], Step [12/46], Loss: 67.0967\n",
      "Epoch [6/15], Step [16/46], Loss: 77.1462\n",
      "Epoch [6/15], Step [20/46], Loss: 78.1914\n",
      "Epoch [6/15], Step [24/46], Loss: 65.1434\n",
      "Epoch [6/15], Step [28/46], Loss: 51.0899\n",
      "Epoch [6/15], Step [32/46], Loss: 56.3637\n",
      "Epoch [6/15], Step [36/46], Loss: 67.0139\n",
      "Epoch [6/15], Step [40/46], Loss: 59.7800\n",
      "Epoch [6/15], Step [44/46], Loss: 71.3208\n",
      "epoch 6\n",
      "Epoch [7/15], Step [4/46], Loss: 58.3076\n",
      "Epoch [7/15], Step [8/46], Loss: 83.7683\n",
      "Epoch [7/15], Step [12/46], Loss: 59.5039\n",
      "Epoch [7/15], Step [16/46], Loss: 53.4869\n",
      "Epoch [7/15], Step [20/46], Loss: 55.9476\n",
      "Epoch [7/15], Step [24/46], Loss: 43.2329\n",
      "Epoch [7/15], Step [28/46], Loss: 54.4197\n",
      "Epoch [7/15], Step [32/46], Loss: 66.3960\n",
      "Epoch [7/15], Step [36/46], Loss: 61.8062\n",
      "Epoch [7/15], Step [40/46], Loss: 46.1078\n",
      "Epoch [7/15], Step [44/46], Loss: 53.4797\n",
      "epoch 7\n",
      "Epoch [8/15], Step [4/46], Loss: 71.3022\n",
      "Epoch [8/15], Step [8/46], Loss: 51.3352\n",
      "Epoch [8/15], Step [12/46], Loss: 43.5330\n",
      "Epoch [8/15], Step [16/46], Loss: 40.5455\n",
      "Epoch [8/15], Step [20/46], Loss: 61.3148\n",
      "Epoch [8/15], Step [24/46], Loss: 54.2679\n",
      "Epoch [8/15], Step [28/46], Loss: 71.6888\n",
      "Epoch [8/15], Step [32/46], Loss: 52.2024\n",
      "Epoch [8/15], Step [36/46], Loss: 54.1032\n",
      "Epoch [8/15], Step [40/46], Loss: 58.5619\n",
      "Epoch [8/15], Step [44/46], Loss: 57.5530\n",
      "epoch 8\n",
      "Epoch [9/15], Step [4/46], Loss: 54.5174\n",
      "Epoch [9/15], Step [8/46], Loss: 45.4376\n",
      "Epoch [9/15], Step [12/46], Loss: 55.2273\n",
      "Epoch [9/15], Step [16/46], Loss: 37.5488\n",
      "Epoch [9/15], Step [20/46], Loss: 40.4453\n",
      "Epoch [9/15], Step [24/46], Loss: 48.9130\n",
      "Epoch [9/15], Step [28/46], Loss: 42.3136\n",
      "Epoch [9/15], Step [32/46], Loss: 43.7940\n",
      "Epoch [9/15], Step [36/46], Loss: 43.0638\n",
      "Epoch [9/15], Step [40/46], Loss: 38.7638\n",
      "Epoch [9/15], Step [44/46], Loss: 30.4350\n",
      "epoch 9\n",
      "Epoch [10/15], Step [4/46], Loss: 37.6745\n",
      "Epoch [10/15], Step [8/46], Loss: 35.1573\n",
      "Epoch [10/15], Step [12/46], Loss: 52.9150\n",
      "Epoch [10/15], Step [16/46], Loss: 36.6360\n",
      "Epoch [10/15], Step [20/46], Loss: 35.5774\n",
      "Epoch [10/15], Step [24/46], Loss: 36.8343\n",
      "Epoch [10/15], Step [28/46], Loss: 41.9150\n",
      "Epoch [10/15], Step [32/46], Loss: 31.7336\n",
      "Epoch [10/15], Step [36/46], Loss: 44.4950\n",
      "Epoch [10/15], Step [40/46], Loss: 45.5958\n",
      "Epoch [10/15], Step [44/46], Loss: 34.9560\n",
      "epoch 10\n",
      "Epoch [11/15], Step [4/46], Loss: 24.4662\n",
      "Epoch [11/15], Step [8/46], Loss: 34.6801\n",
      "Epoch [11/15], Step [12/46], Loss: 34.7712\n",
      "Epoch [11/15], Step [16/46], Loss: 22.1789\n",
      "Epoch [11/15], Step [20/46], Loss: 30.2911\n",
      "Epoch [11/15], Step [24/46], Loss: 35.0296\n",
      "Epoch [11/15], Step [28/46], Loss: 38.0907\n",
      "Epoch [11/15], Step [32/46], Loss: 39.9595\n",
      "Epoch [11/15], Step [36/46], Loss: 22.4699\n",
      "Epoch [11/15], Step [40/46], Loss: 27.2219\n",
      "Epoch [11/15], Step [44/46], Loss: 30.4019\n",
      "epoch 11\n",
      "Epoch [12/15], Step [4/46], Loss: 37.2355\n",
      "Epoch [12/15], Step [8/46], Loss: 24.9405\n",
      "Epoch [12/15], Step [12/46], Loss: 23.3571\n",
      "Epoch [12/15], Step [16/46], Loss: 22.8447\n",
      "Epoch [12/15], Step [20/46], Loss: 17.7667\n",
      "Epoch [12/15], Step [24/46], Loss: 24.4686\n",
      "Epoch [12/15], Step [28/46], Loss: 24.2209\n",
      "Epoch [12/15], Step [32/46], Loss: 33.1042\n",
      "Epoch [12/15], Step [36/46], Loss: 21.9356\n",
      "Epoch [12/15], Step [40/46], Loss: 27.2013\n",
      "Epoch [12/15], Step [44/46], Loss: 35.3154\n",
      "epoch 12\n",
      "Epoch [13/15], Step [4/46], Loss: 24.5358\n",
      "Epoch [13/15], Step [8/46], Loss: 23.6060\n",
      "Epoch [13/15], Step [12/46], Loss: 27.3680\n",
      "Epoch [13/15], Step [16/46], Loss: 18.1184\n",
      "Epoch [13/15], Step [20/46], Loss: 23.8860\n",
      "Epoch [13/15], Step [24/46], Loss: 9.8064\n",
      "Epoch [13/15], Step [28/46], Loss: 41.4029\n",
      "Epoch [13/15], Step [32/46], Loss: 24.5030\n",
      "Epoch [13/15], Step [36/46], Loss: 33.3416\n",
      "Epoch [13/15], Step [40/46], Loss: 16.5478\n",
      "Epoch [13/15], Step [44/46], Loss: 25.4063\n",
      "epoch 13\n",
      "Epoch [14/15], Step [4/46], Loss: 19.7008\n",
      "Epoch [14/15], Step [8/46], Loss: 21.8477\n",
      "Epoch [14/15], Step [12/46], Loss: 17.4273\n",
      "Epoch [14/15], Step [16/46], Loss: 13.8916\n",
      "Epoch [14/15], Step [20/46], Loss: 16.9781\n",
      "Epoch [14/15], Step [24/46], Loss: 18.7465\n",
      "Epoch [14/15], Step [28/46], Loss: 30.4114\n",
      "Epoch [14/15], Step [32/46], Loss: 26.1047\n",
      "Epoch [14/15], Step [36/46], Loss: 11.1246\n",
      "Epoch [14/15], Step [40/46], Loss: 25.2759\n",
      "Epoch [14/15], Step [44/46], Loss: 12.9859\n",
      "epoch 14\n",
      "Epoch [15/15], Step [4/46], Loss: 10.7203\n",
      "Epoch [15/15], Step [8/46], Loss: 22.3764\n",
      "Epoch [15/15], Step [12/46], Loss: 10.0287\n",
      "Epoch [15/15], Step [16/46], Loss: 21.8781\n",
      "Epoch [15/15], Step [20/46], Loss: 10.0594\n",
      "Epoch [15/15], Step [24/46], Loss: 21.3815\n",
      "Epoch [15/15], Step [28/46], Loss: 16.1800\n",
      "Epoch [15/15], Step [32/46], Loss: 8.9548\n",
      "Epoch [15/15], Step [36/46], Loss: 12.3436\n",
      "Epoch [15/15], Step [40/46], Loss: 14.0540\n",
      "Epoch [15/15], Step [44/46], Loss: 18.5094\n",
      "Time: 25872.9881131649\n",
      "Beginning Testing..\n",
      "Accuracy of the network on the 975 test images: 73.02564102564102 %\n",
      "reach: 0.6266666666666667\n",
      "squat: 0.68\n",
      "inline: 0.86\n",
      "lunge: 0.8133333333333334\n",
      "hamstrings: 0.68\n",
      "stretch: 0.6933333333333334\n",
      "deadbug: 0.7933333333333333\n",
      "pushup: 0.5066666666666667\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet().to(device)\n",
    "loss_list = []\n",
    "# if we're only testing, we don't want to train for any epochs, and we want to load a model\n",
    "if not is_train:\n",
    "    num_epochs = 0\n",
    "    model.load_state_dict(torch.load('model.ckpt'))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss(reduce=False, size_average=False) #TODO: define your loss here. hint: should just require calling a built-in pytorch layer.\n",
    "# NOTE: you can use a different optimizer besides Adam, like RMSProp or SGD, if you'd like\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "# Loop over epochs\n",
    "print('Beginning training..')\n",
    "total_step = len(train_loader)\n",
    "loss_list_tra = []\n",
    "loss_list_val = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    print('epoch {}'.format(epoch))\n",
    "    for i, (local_batch,local_labels) in enumerate(train_loader):\n",
    "        # Transfer to GPU\n",
    "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model.forward(local_ims)\n",
    "        loss = criterion(outputs, local_labels).sum()\n",
    "        # TODO: maintain a list of your losses as a function of number of steps\n",
    "        #       because we ask you to plot this information\n",
    "        # NOTE: if you use Google Colab's tensorboard-like feature to visualize\n",
    "        #       the loss, you do not need to plot it here. just take a screenshot\n",
    "        #       of the loss curve and include it in your write-up.\n",
    "        #print(loss.item())\n",
    "        loss_list_tra.append(loss.item()*46)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        loss_val = 0\n",
    "        for (val_batch,val_labels) in val_loader:\n",
    "            # Transfer to GPU\n",
    "            val_ims, val_labels = val_batch.to(device), val_labels.to(device)\n",
    "\n",
    "            outputs_val = model.forward(val_ims)\n",
    "            loss_val += criterion(outputs_val, val_labels).sum().item()\n",
    "        loss_list_val.append(loss_val)\n",
    "        \n",
    "        if (i+1) % 4 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "end = time.time()\n",
    "print('Time: {}'.format(end - start))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "print('Beginning Testing..')\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_list = []\n",
    "    groundtruth_list = []\n",
    "    for (local_batch,local_labels) in val_loader:\n",
    "        # Transfer to GPU\n",
    "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "        outputs = model.forward(local_ims)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += local_labels.size(0)\n",
    "        predicted_list.extend(predicted)\n",
    "        groundtruth_list.extend(local_labels)\n",
    "        correct += (predicted == local_labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'\n",
    "          .format(total, 100 * correct / total))\n",
    "\n",
    "# Look at some things about the model results..\n",
    "# convert the predicted_list and groundtruth_list Tensors to lists\n",
    "pl = [p.cpu().numpy().tolist() for p in predicted_list]\n",
    "gt = [p.cpu().numpy().tolist() for p in groundtruth_list]\n",
    "\n",
    "# TODO: use pl and gt to produce your confusion matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_m = confusion_matrix(gt, pl)\n",
    "\n",
    "# view the per-movement accuracy\n",
    "label_map = ['reach','squat','inline','lunge','hamstrings','stretch','deadbug','pushup']\n",
    "for id in range(len(label_map)):\n",
    "    print('{}: {}'.format(label_map[id],sum([p and g for (p,g) in zip(np.array(pl)==np.array(gt),np.array(gt)==id)])/(sum(np.array(gt)==id)+0.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss1 = [l1*1/3 for l1 in loss_list_tra]\n",
    "loss2 = [l2 for l2 in loss_list_val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "975"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader)\n",
    "len(val_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0e135ea4a8>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.plot(range(len(loss1)),loss1)\n",
    "plt.plot(range(len(loss2)),loss2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training..\n",
      "epoch 0\n",
      "Epoch [1/25], Step [4/46], Loss: 132.4995\n",
      "Epoch [1/25], Step [8/46], Loss: 134.0070\n",
      "Epoch [1/25], Step [12/46], Loss: 130.6303\n",
      "Epoch [1/25], Step [16/46], Loss: 126.4795\n",
      "Epoch [1/25], Step [20/46], Loss: 127.5393\n",
      "Epoch [1/25], Step [24/46], Loss: 130.1460\n",
      "Epoch [1/25], Step [28/46], Loss: 123.8864\n",
      "Epoch [1/25], Step [32/46], Loss: 127.3067\n",
      "Epoch [1/25], Step [36/46], Loss: 128.8626\n",
      "Epoch [1/25], Step [40/46], Loss: 127.3737\n",
      "Epoch [1/25], Step [44/46], Loss: 123.7723\n",
      "epoch 1\n",
      "Epoch [2/25], Step [4/46], Loss: 114.7587\n",
      "Epoch [2/25], Step [8/46], Loss: 109.1737\n",
      "Epoch [2/25], Step [12/46], Loss: 118.1739\n",
      "Epoch [2/25], Step [16/46], Loss: 119.3957\n",
      "Epoch [2/25], Step [20/46], Loss: 115.7085\n",
      "Epoch [2/25], Step [24/46], Loss: 117.9288\n",
      "Epoch [2/25], Step [28/46], Loss: 119.3055\n",
      "Epoch [2/25], Step [32/46], Loss: 98.0466\n",
      "Epoch [2/25], Step [36/46], Loss: 117.0755\n",
      "Epoch [2/25], Step [40/46], Loss: 106.6386\n",
      "Epoch [2/25], Step [44/46], Loss: 101.9718\n",
      "epoch 2\n",
      "Epoch [3/25], Step [4/46], Loss: 92.5640\n",
      "Epoch [3/25], Step [8/46], Loss: 106.0878\n",
      "Epoch [3/25], Step [12/46], Loss: 101.5078\n",
      "Epoch [3/25], Step [16/46], Loss: 87.3255\n",
      "Epoch [3/25], Step [20/46], Loss: 88.0803\n",
      "Epoch [3/25], Step [24/46], Loss: 98.5418\n",
      "Epoch [3/25], Step [28/46], Loss: 96.8800\n",
      "Epoch [3/25], Step [32/46], Loss: 99.2174\n",
      "Epoch [3/25], Step [36/46], Loss: 93.9094\n",
      "Epoch [3/25], Step [40/46], Loss: 92.1958\n",
      "Epoch [3/25], Step [44/46], Loss: 86.3714\n",
      "epoch 3\n",
      "Epoch [4/25], Step [4/46], Loss: 102.8628\n",
      "Epoch [4/25], Step [8/46], Loss: 83.7784\n",
      "Epoch [4/25], Step [12/46], Loss: 92.8235\n",
      "Epoch [4/25], Step [16/46], Loss: 88.4404\n",
      "Epoch [4/25], Step [20/46], Loss: 71.0641\n",
      "Epoch [4/25], Step [24/46], Loss: 78.9953\n",
      "Epoch [4/25], Step [28/46], Loss: 69.0261\n",
      "Epoch [4/25], Step [32/46], Loss: 86.2349\n",
      "Epoch [4/25], Step [36/46], Loss: 84.3864\n",
      "Epoch [4/25], Step [40/46], Loss: 72.4195\n",
      "Epoch [4/25], Step [44/46], Loss: 78.4620\n",
      "epoch 4\n",
      "Epoch [5/25], Step [4/46], Loss: 78.3905\n",
      "Epoch [5/25], Step [8/46], Loss: 75.4904\n",
      "Epoch [5/25], Step [12/46], Loss: 71.1877\n",
      "Epoch [5/25], Step [16/46], Loss: 90.7377\n",
      "Epoch [5/25], Step [20/46], Loss: 67.4530\n",
      "Epoch [5/25], Step [24/46], Loss: 80.6337\n",
      "Epoch [5/25], Step [28/46], Loss: 69.9568\n",
      "Epoch [5/25], Step [32/46], Loss: 76.2928\n",
      "Epoch [5/25], Step [36/46], Loss: 73.2421\n",
      "Epoch [5/25], Step [40/46], Loss: 64.2128\n",
      "Epoch [5/25], Step [44/46], Loss: 84.3861\n",
      "epoch 5\n",
      "Epoch [6/25], Step [4/46], Loss: 60.9264\n",
      "Epoch [6/25], Step [8/46], Loss: 45.7936\n",
      "Epoch [6/25], Step [12/46], Loss: 60.9130\n",
      "Epoch [6/25], Step [16/46], Loss: 71.1041\n",
      "Epoch [6/25], Step [20/46], Loss: 57.1779\n",
      "Epoch [6/25], Step [24/46], Loss: 68.1725\n",
      "Epoch [6/25], Step [28/46], Loss: 57.6843\n",
      "Epoch [6/25], Step [32/46], Loss: 59.9210\n",
      "Epoch [6/25], Step [36/46], Loss: 59.6698\n",
      "Epoch [6/25], Step [40/46], Loss: 80.1309\n",
      "Epoch [6/25], Step [44/46], Loss: 68.9314\n",
      "epoch 6\n",
      "Epoch [7/25], Step [4/46], Loss: 48.7298\n",
      "Epoch [7/25], Step [8/46], Loss: 44.5040\n",
      "Epoch [7/25], Step [12/46], Loss: 59.3672\n",
      "Epoch [7/25], Step [16/46], Loss: 63.8228\n",
      "Epoch [7/25], Step [20/46], Loss: 55.1170\n",
      "Epoch [7/25], Step [24/46], Loss: 58.8083\n",
      "Epoch [7/25], Step [28/46], Loss: 49.5146\n",
      "Epoch [7/25], Step [32/46], Loss: 52.2458\n",
      "Epoch [7/25], Step [36/46], Loss: 61.6051\n",
      "Epoch [7/25], Step [40/46], Loss: 56.8993\n",
      "Epoch [7/25], Step [44/46], Loss: 43.2720\n",
      "epoch 7\n",
      "Epoch [8/25], Step [4/46], Loss: 64.0649\n",
      "Epoch [8/25], Step [8/46], Loss: 39.1282\n",
      "Epoch [8/25], Step [12/46], Loss: 45.0225\n",
      "Epoch [8/25], Step [16/46], Loss: 48.9633\n",
      "Epoch [8/25], Step [20/46], Loss: 28.1534\n",
      "Epoch [8/25], Step [24/46], Loss: 40.8061\n",
      "Epoch [8/25], Step [28/46], Loss: 49.8029\n",
      "Epoch [8/25], Step [32/46], Loss: 42.5183\n",
      "Epoch [8/25], Step [36/46], Loss: 42.1709\n",
      "Epoch [8/25], Step [40/46], Loss: 44.7166\n",
      "Epoch [8/25], Step [44/46], Loss: 41.5428\n",
      "epoch 8\n",
      "Epoch [9/25], Step [4/46], Loss: 42.4250\n",
      "Epoch [9/25], Step [8/46], Loss: 41.6753\n",
      "Epoch [9/25], Step [12/46], Loss: 28.0405\n",
      "Epoch [9/25], Step [16/46], Loss: 34.2960\n",
      "Epoch [9/25], Step [20/46], Loss: 48.7266\n",
      "Epoch [9/25], Step [24/46], Loss: 44.5121\n",
      "Epoch [9/25], Step [28/46], Loss: 45.7215\n",
      "Epoch [9/25], Step [32/46], Loss: 37.2952\n",
      "Epoch [9/25], Step [36/46], Loss: 25.8433\n",
      "Epoch [9/25], Step [40/46], Loss: 40.4169\n",
      "Epoch [9/25], Step [44/46], Loss: 33.2711\n",
      "epoch 9\n",
      "Epoch [10/25], Step [4/46], Loss: 30.6207\n",
      "Epoch [10/25], Step [8/46], Loss: 23.3963\n",
      "Epoch [10/25], Step [12/46], Loss: 26.3032\n",
      "Epoch [10/25], Step [16/46], Loss: 21.5023\n",
      "Epoch [10/25], Step [20/46], Loss: 26.9471\n",
      "Epoch [10/25], Step [24/46], Loss: 34.9881\n",
      "Epoch [10/25], Step [28/46], Loss: 44.6897\n",
      "Epoch [10/25], Step [32/46], Loss: 26.1738\n",
      "Epoch [10/25], Step [36/46], Loss: 47.0798\n",
      "Epoch [10/25], Step [40/46], Loss: 20.9859\n",
      "Epoch [10/25], Step [44/46], Loss: 22.0662\n",
      "epoch 10\n",
      "Epoch [11/25], Step [4/46], Loss: 37.4305\n",
      "Epoch [11/25], Step [8/46], Loss: 32.7744\n",
      "Epoch [11/25], Step [12/46], Loss: 24.0170\n",
      "Epoch [11/25], Step [16/46], Loss: 30.0973\n",
      "Epoch [11/25], Step [20/46], Loss: 25.8682\n",
      "Epoch [11/25], Step [24/46], Loss: 30.0976\n",
      "Epoch [11/25], Step [28/46], Loss: 31.2282\n",
      "Epoch [11/25], Step [32/46], Loss: 22.9799\n",
      "Epoch [11/25], Step [36/46], Loss: 31.3036\n",
      "Epoch [11/25], Step [40/46], Loss: 24.7928\n",
      "Epoch [11/25], Step [44/46], Loss: 31.8116\n",
      "epoch 11\n",
      "Epoch [12/25], Step [4/46], Loss: 21.7358\n",
      "Epoch [12/25], Step [8/46], Loss: 16.9318\n",
      "Epoch [12/25], Step [12/46], Loss: 20.3521\n",
      "Epoch [12/25], Step [16/46], Loss: 24.4326\n",
      "Epoch [12/25], Step [20/46], Loss: 24.0245\n",
      "Epoch [12/25], Step [24/46], Loss: 15.3298\n",
      "Epoch [12/25], Step [28/46], Loss: 14.7619\n",
      "Epoch [12/25], Step [32/46], Loss: 29.4670\n",
      "Epoch [12/25], Step [36/46], Loss: 21.6379\n",
      "Epoch [12/25], Step [40/46], Loss: 19.0772\n",
      "Epoch [12/25], Step [44/46], Loss: 18.9531\n",
      "epoch 12\n",
      "Epoch [13/25], Step [4/46], Loss: 11.1362\n",
      "Epoch [13/25], Step [8/46], Loss: 12.7851\n",
      "Epoch [13/25], Step [12/46], Loss: 15.7380\n",
      "Epoch [13/25], Step [16/46], Loss: 18.8982\n",
      "Epoch [13/25], Step [20/46], Loss: 13.6959\n",
      "Epoch [13/25], Step [24/46], Loss: 16.8925\n",
      "Epoch [13/25], Step [28/46], Loss: 18.5273\n",
      "Epoch [13/25], Step [32/46], Loss: 16.6269\n",
      "Epoch [13/25], Step [36/46], Loss: 15.1978\n",
      "Epoch [13/25], Step [40/46], Loss: 25.2973\n",
      "Epoch [13/25], Step [44/46], Loss: 21.4746\n",
      "epoch 13\n",
      "Epoch [14/25], Step [4/46], Loss: 13.4920\n",
      "Epoch [14/25], Step [8/46], Loss: 6.7449\n",
      "Epoch [14/25], Step [12/46], Loss: 16.0674\n",
      "Epoch [14/25], Step [16/46], Loss: 11.3566\n",
      "Epoch [14/25], Step [20/46], Loss: 9.8322\n",
      "Epoch [14/25], Step [24/46], Loss: 8.9811\n",
      "Epoch [14/25], Step [28/46], Loss: 7.1239\n",
      "Epoch [14/25], Step [32/46], Loss: 5.1700\n",
      "Epoch [14/25], Step [36/46], Loss: 13.6816\n",
      "Epoch [14/25], Step [40/46], Loss: 6.4951\n",
      "Epoch [14/25], Step [44/46], Loss: 12.2863\n",
      "epoch 14\n",
      "Epoch [15/25], Step [4/46], Loss: 11.3753\n",
      "Epoch [15/25], Step [8/46], Loss: 10.0264\n",
      "Epoch [15/25], Step [12/46], Loss: 8.5983\n",
      "Epoch [15/25], Step [16/46], Loss: 6.4858\n",
      "Epoch [15/25], Step [20/46], Loss: 9.6319\n",
      "Epoch [15/25], Step [24/46], Loss: 6.2110\n",
      "Epoch [15/25], Step [28/46], Loss: 8.8233\n",
      "Epoch [15/25], Step [32/46], Loss: 4.1388\n",
      "Epoch [15/25], Step [36/46], Loss: 16.8638\n",
      "Epoch [15/25], Step [40/46], Loss: 8.9405\n",
      "Epoch [15/25], Step [44/46], Loss: 3.9937\n",
      "epoch 15\n",
      "Epoch [16/25], Step [4/46], Loss: 4.9624\n",
      "Epoch [16/25], Step [8/46], Loss: 2.6316\n",
      "Epoch [16/25], Step [12/46], Loss: 1.6395\n",
      "Epoch [16/25], Step [16/46], Loss: 4.7913\n",
      "Epoch [16/25], Step [20/46], Loss: 5.7513\n",
      "Epoch [16/25], Step [24/46], Loss: 6.8072\n",
      "Epoch [16/25], Step [28/46], Loss: 3.5168\n",
      "Epoch [16/25], Step [32/46], Loss: 6.6947\n",
      "Epoch [16/25], Step [36/46], Loss: 5.6460\n",
      "Epoch [16/25], Step [40/46], Loss: 1.0358\n",
      "Epoch [16/25], Step [44/46], Loss: 4.1379\n",
      "epoch 16\n",
      "Epoch [17/25], Step [4/46], Loss: 3.0663\n",
      "Epoch [17/25], Step [8/46], Loss: 2.4385\n",
      "Epoch [17/25], Step [12/46], Loss: 2.5032\n",
      "Epoch [17/25], Step [16/46], Loss: 8.7303\n",
      "Epoch [17/25], Step [20/46], Loss: 1.9713\n",
      "Epoch [17/25], Step [24/46], Loss: 3.9316\n",
      "Epoch [17/25], Step [28/46], Loss: 3.7258\n",
      "Epoch [17/25], Step [32/46], Loss: 5.1367\n",
      "Epoch [17/25], Step [36/46], Loss: 9.0546\n",
      "Epoch [17/25], Step [40/46], Loss: 2.2138\n",
      "Epoch [17/25], Step [44/46], Loss: 7.7419\n",
      "epoch 17\n",
      "Epoch [18/25], Step [4/46], Loss: 1.1101\n",
      "Epoch [18/25], Step [8/46], Loss: 2.8661\n",
      "Epoch [18/25], Step [12/46], Loss: 3.1056\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Step [16/46], Loss: 5.7774\n",
      "Epoch [18/25], Step [20/46], Loss: 0.7774\n",
      "Epoch [18/25], Step [24/46], Loss: 3.8626\n",
      "Epoch [18/25], Step [28/46], Loss: 1.3217\n",
      "Epoch [18/25], Step [32/46], Loss: 5.4888\n",
      "Epoch [18/25], Step [36/46], Loss: 7.5961\n",
      "Epoch [18/25], Step [40/46], Loss: 5.5560\n",
      "Epoch [18/25], Step [44/46], Loss: 3.2472\n",
      "epoch 18\n",
      "Epoch [19/25], Step [4/46], Loss: 6.4233\n",
      "Epoch [19/25], Step [8/46], Loss: 8.4379\n",
      "Epoch [19/25], Step [12/46], Loss: 3.5740\n",
      "Epoch [19/25], Step [16/46], Loss: 1.1230\n",
      "Epoch [19/25], Step [20/46], Loss: 6.8138\n",
      "Epoch [19/25], Step [24/46], Loss: 12.1419\n",
      "Epoch [19/25], Step [28/46], Loss: 7.6875\n",
      "Epoch [19/25], Step [32/46], Loss: 2.0277\n",
      "Epoch [19/25], Step [36/46], Loss: 3.6486\n",
      "Epoch [19/25], Step [40/46], Loss: 4.2888\n",
      "Epoch [19/25], Step [44/46], Loss: 2.5950\n",
      "epoch 19\n",
      "Epoch [20/25], Step [4/46], Loss: 1.1897\n",
      "Epoch [20/25], Step [8/46], Loss: 0.4890\n",
      "Epoch [20/25], Step [12/46], Loss: 5.2773\n",
      "Epoch [20/25], Step [16/46], Loss: 5.9116\n",
      "Epoch [20/25], Step [20/46], Loss: 4.9075\n",
      "Epoch [20/25], Step [24/46], Loss: 9.4107\n",
      "Epoch [20/25], Step [28/46], Loss: 1.8200\n",
      "Epoch [20/25], Step [32/46], Loss: 2.5633\n",
      "Epoch [20/25], Step [36/46], Loss: 0.6463\n",
      "Epoch [20/25], Step [40/46], Loss: 1.5981\n",
      "Epoch [20/25], Step [44/46], Loss: 1.1759\n",
      "epoch 20\n",
      "Epoch [21/25], Step [4/46], Loss: 0.3460\n",
      "Epoch [21/25], Step [8/46], Loss: 1.8155\n",
      "Epoch [21/25], Step [12/46], Loss: 0.3853\n",
      "Epoch [21/25], Step [16/46], Loss: 0.8845\n",
      "Epoch [21/25], Step [20/46], Loss: 1.7345\n",
      "Epoch [21/25], Step [24/46], Loss: 3.2105\n",
      "Epoch [21/25], Step [28/46], Loss: 4.6143\n",
      "Epoch [21/25], Step [32/46], Loss: 0.6432\n",
      "Epoch [21/25], Step [36/46], Loss: 3.6781\n",
      "Epoch [21/25], Step [40/46], Loss: 0.7290\n",
      "Epoch [21/25], Step [44/46], Loss: 2.1575\n",
      "epoch 21\n",
      "Epoch [22/25], Step [4/46], Loss: 1.1671\n",
      "Epoch [22/25], Step [8/46], Loss: 1.1555\n",
      "Epoch [22/25], Step [12/46], Loss: 4.1990\n",
      "Epoch [22/25], Step [16/46], Loss: 2.5709\n",
      "Epoch [22/25], Step [20/46], Loss: 0.2419\n",
      "Epoch [22/25], Step [24/46], Loss: 1.6968\n",
      "Epoch [22/25], Step [28/46], Loss: 0.6196\n",
      "Epoch [22/25], Step [32/46], Loss: 0.5805\n",
      "Epoch [22/25], Step [36/46], Loss: 4.7532\n",
      "Epoch [22/25], Step [40/46], Loss: 0.6979\n",
      "Epoch [22/25], Step [44/46], Loss: 3.5268\n",
      "epoch 22\n",
      "Epoch [23/25], Step [4/46], Loss: 1.3545\n",
      "Epoch [23/25], Step [8/46], Loss: 1.2203\n",
      "Epoch [23/25], Step [12/46], Loss: 0.7046\n",
      "Epoch [23/25], Step [16/46], Loss: 1.9142\n",
      "Epoch [23/25], Step [20/46], Loss: 1.5504\n",
      "Epoch [23/25], Step [24/46], Loss: 3.2195\n",
      "Epoch [23/25], Step [28/46], Loss: 4.3563\n",
      "Epoch [23/25], Step [32/46], Loss: 8.7002\n",
      "Epoch [23/25], Step [36/46], Loss: 0.4020\n",
      "Epoch [23/25], Step [40/46], Loss: 2.2535\n",
      "Epoch [23/25], Step [44/46], Loss: 4.5264\n",
      "epoch 23\n",
      "Epoch [24/25], Step [4/46], Loss: 8.8796\n",
      "Epoch [24/25], Step [8/46], Loss: 0.8121\n",
      "Epoch [24/25], Step [12/46], Loss: 0.8409\n",
      "Epoch [24/25], Step [16/46], Loss: 3.8531\n",
      "Epoch [24/25], Step [20/46], Loss: 1.2936\n",
      "Epoch [24/25], Step [24/46], Loss: 1.0142\n",
      "Epoch [24/25], Step [28/46], Loss: 1.2615\n",
      "Epoch [24/25], Step [32/46], Loss: 2.3065\n",
      "Epoch [24/25], Step [36/46], Loss: 2.0448\n",
      "Epoch [24/25], Step [40/46], Loss: 4.3021\n",
      "Epoch [24/25], Step [44/46], Loss: 3.2068\n",
      "epoch 24\n",
      "Epoch [25/25], Step [4/46], Loss: 0.7289\n",
      "Epoch [25/25], Step [8/46], Loss: 0.4895\n",
      "Epoch [25/25], Step [12/46], Loss: 1.0088\n",
      "Epoch [25/25], Step [16/46], Loss: 1.9633\n",
      "Epoch [25/25], Step [20/46], Loss: 2.6078\n",
      "Epoch [25/25], Step [24/46], Loss: 1.5386\n",
      "Epoch [25/25], Step [28/46], Loss: 0.7365\n",
      "Epoch [25/25], Step [32/46], Loss: 2.3858\n",
      "Epoch [25/25], Step [36/46], Loss: 2.5256\n",
      "Epoch [25/25], Step [40/46], Loss: 0.2378\n",
      "Epoch [25/25], Step [44/46], Loss: 0.7357\n",
      "Time: 14700.832482099533\n",
      "Beginning Testing..\n",
      "Accuracy of the network on the 975 test images: 73.43589743589743 %\n",
      "reach: 0.7466666666666667\n",
      "squat: 0.76\n",
      "inline: 0.7666666666666667\n",
      "lunge: 0.7666666666666667\n",
      "hamstrings: 0.7133333333333334\n",
      "stretch: 0.7533333333333333\n",
      "deadbug: 0.68\n",
      "pushup: 0.68\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet().to(device)\n",
    "loss_list = []\n",
    "# if we're only testing, we don't want to train for any epochs, and we want to load a model\n",
    "if not is_train:\n",
    "    num_epochs = 0\n",
    "    model.load_state_dict(torch.load('model.ckpt'))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss(reduce=False, size_average=False) #TODO: define your loss here. hint: should just require calling a built-in pytorch layer.\n",
    "# NOTE: you can use a different optimizer besides Adam, like RMSProp or SGD, if you'd like\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "# Loop over epochs\n",
    "print('Beginning training..')\n",
    "total_step = len(train_loader)\n",
    "loss_list = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    print('epoch {}'.format(epoch))\n",
    "    for i, (local_batch,local_labels) in enumerate(train_loader):\n",
    "        # Transfer to GPU\n",
    "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model.forward(local_ims)\n",
    "        loss = criterion(outputs, local_labels).sum()\n",
    "        # TODO: maintain a list of your losses as a function of number of steps\n",
    "        #       because we ask you to plot this information\n",
    "        # NOTE: if you use Google Colab's tensorboard-like feature to visualize\n",
    "        #       the loss, you do not need to plot it here. just take a screenshot\n",
    "        #       of the loss curve and include it in your write-up.\n",
    "        #print(loss.item())\n",
    "        loss_list.append(loss.item())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 4 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "end = time.time()\n",
    "print('Time: {}'.format(end - start))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "print('Beginning Testing..')\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_list = []\n",
    "    groundtruth_list = []\n",
    "    for (local_batch,local_labels) in val_loader:\n",
    "        # Transfer to GPU\n",
    "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "        outputs = model.forward(local_ims)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += local_labels.size(0)\n",
    "        predicted_list.extend(predicted)\n",
    "        groundtruth_list.extend(local_labels)\n",
    "        correct += (predicted == local_labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'\n",
    "          .format(total, 100 * correct / total))\n",
    "\n",
    "# Look at some things about the model results..\n",
    "# convert the predicted_list and groundtruth_list Tensors to lists\n",
    "pl = [p.cpu().numpy().tolist() for p in predicted_list]\n",
    "gt = [p.cpu().numpy().tolist() for p in groundtruth_list]\n",
    "\n",
    "# TODO: use pl and gt to produce your confusion matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_m = confusion_matrix(gt, pl)\n",
    "\n",
    "# view the per-movement accuracy\n",
    "label_map = ['reach','squat','inline','lunge','hamstrings','stretch','deadbug','pushup']\n",
    "for id in range(len(label_map)):\n",
    "    print('{}: {}'.format(label_map[id],sum([p and g for (p,g) in zip(np.array(pl)==np.array(gt),np.array(gt)==id)])/(sum(np.array(gt)==id)+0.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 61,   1,  11,   1,   0,   0,   0,   1],\n",
       "       [  1,  55,   7,   4,   4,   2,   1,   1],\n",
       "       [ 12,   5, 118,   3,   9,   2,   0,   1],\n",
       "       [  6,   6,  10, 112,   2,   6,   1,   7],\n",
       "       [  5,   1,   6,   4, 107,  11,  10,   6],\n",
       "       [  6,   2,   3,   3,   8, 116,   5,   7],\n",
       "       [  0,   2,   4,   7,  12,  12, 107,   6],\n",
       "       [  2,   1,   3,   2,   1,   5,   4,  57]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# flag for whether you're training or not\n",
    "is_train = True\n",
    "is_key_frame = False # TODO: set this to false to train on the video frames, instead of the key frames\n",
    "model_to_load = 'model.ckpt' # This is the model to load during testing, if you want to eval a previously-trained model.\n",
    "\n",
    "# CUDA for PyTorch\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
    "#cudnn.benchmark = True\n",
    "\n",
    "# Parameters for data loader\n",
    "params = {'batch_size': 64,  # TODO: fill in the batch size. often, these are things like 32,64,128,or 256\n",
    "          'shuffle': True,\n",
    "          'num_workers': 30 \n",
    "          }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Hyper-parameters\n",
    "num_epochs = 25\n",
    "learning_rate = 0.0001\n",
    "# NOTE: depending on your optimizer, you may want to tune other hyperparameters as well\n",
    "\n",
    "# Datasets\n",
    "# TODO: put the path to your train, test, validation txt files\n",
    "if is_key_frame:\n",
    "    label_file_train =  './data/keyframe_data_train.txt'\n",
    "    label_file_val  =  './data/keyframe_data_val.txt'\n",
    "    # NOTE: the kaggle competition test data is only for the video frames, not the key frames\n",
    "    # this is why we don't have an equivalent label_file_test with keyframes\n",
    "else:\n",
    "    label_file_train = './data/videoframe_data_train.txt'\n",
    "    label_file_val = './data/videoframe_data_val.txt'\n",
    "    label_file_test = './data/videoframe_data_test.txt'\n",
    "\n",
    "# TODO: you should normalize based on the average image in the training set. This shows \n",
    "# an example of doing normalization\n",
    "mean = [0.5, 0.5, 0.5]\n",
    "std = [0.5, 0.5, 0.5]\n",
    "# TODO: if you want to pad or resize your images, you can put the parameters for that below.\n",
    "\n",
    "# Generators\n",
    "# NOTE: if you don't want to pad or resize your images, you should delete the Pad and Resize\n",
    "# transforms from all three _dataset definitions.\n",
    "train_dataset = Mds189(label_file_train,loader=default_loader,transform=transforms.Compose([\n",
    "                                               #transforms.Pad(1),    # TODO: if you want to pad your images\n",
    "                                               #transforms.Resize(256), # TODO: if you want to resize your images\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize(mean, std)\n",
    "                                           ]))\n",
    "train_loader = data.DataLoader(train_dataset, **params)\n",
    "\n",
    "val_dataset = Mds189(label_file_val,loader=default_loader,transform=transforms.Compose([\n",
    "                                               #transforms.Pad(1),\n",
    "                                               #transforms.Resize(256),\n",
    "                                               transforms.ToTensor(),\n",
    "                                               transforms.Normalize(mean, std)\n",
    "                                           ]))\n",
    "val_loader = data.DataLoader(val_dataset, **params)\n",
    "\n",
    "if not is_key_frame:\n",
    "    test_dataset = Mds189(label_file_test,loader=default_loader,transform=transforms.Compose([\n",
    "                                                   #transforms.Pad(1),\n",
    "                                                   #transforms.Resize(256),\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize(mean, std)\n",
    "                                               ]))\n",
    "    test_loader = data.DataLoader(test_dataset, **params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning training..\n",
      "epoch 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wenbo2/anaconda3/envs/fastai/lib/python3.6/site-packages/torch/nn/_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='none' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Step [4/590], Loss: 132.7683\n",
      "Epoch [1/25], Step [8/590], Loss: 131.1727\n",
      "Epoch [1/25], Step [12/590], Loss: 131.6251\n",
      "Epoch [1/25], Step [16/590], Loss: 130.6518\n",
      "Epoch [1/25], Step [20/590], Loss: 132.3721\n",
      "Epoch [1/25], Step [24/590], Loss: 131.3639\n",
      "Epoch [1/25], Step [28/590], Loss: 130.8371\n",
      "Epoch [1/25], Step [32/590], Loss: 131.3049\n",
      "Epoch [1/25], Step [36/590], Loss: 128.1117\n",
      "Epoch [1/25], Step [40/590], Loss: 131.8998\n",
      "Epoch [1/25], Step [44/590], Loss: 128.5308\n",
      "Epoch [1/25], Step [48/590], Loss: 128.3570\n",
      "Epoch [1/25], Step [52/590], Loss: 132.8541\n",
      "Epoch [1/25], Step [56/590], Loss: 128.6715\n",
      "Epoch [1/25], Step [60/590], Loss: 124.8024\n",
      "Epoch [1/25], Step [64/590], Loss: 130.4782\n",
      "Epoch [1/25], Step [68/590], Loss: 129.8755\n",
      "Epoch [1/25], Step [72/590], Loss: 129.0356\n",
      "Epoch [1/25], Step [76/590], Loss: 128.3780\n",
      "Epoch [1/25], Step [80/590], Loss: 126.6509\n",
      "Epoch [1/25], Step [84/590], Loss: 126.5151\n",
      "Epoch [1/25], Step [88/590], Loss: 122.3848\n",
      "Epoch [1/25], Step [92/590], Loss: 118.4316\n",
      "Epoch [1/25], Step [96/590], Loss: 110.3789\n",
      "Epoch [1/25], Step [100/590], Loss: 118.0933\n",
      "Epoch [1/25], Step [104/590], Loss: 119.7795\n",
      "Epoch [1/25], Step [108/590], Loss: 117.9899\n",
      "Epoch [1/25], Step [112/590], Loss: 107.7107\n",
      "Epoch [1/25], Step [116/590], Loss: 112.9780\n",
      "Epoch [1/25], Step [120/590], Loss: 109.8873\n",
      "Epoch [1/25], Step [124/590], Loss: 118.3192\n",
      "Epoch [1/25], Step [128/590], Loss: 98.2291\n",
      "Epoch [1/25], Step [132/590], Loss: 108.2891\n",
      "Epoch [1/25], Step [136/590], Loss: 99.9470\n",
      "Epoch [1/25], Step [140/590], Loss: 112.1461\n",
      "Epoch [1/25], Step [144/590], Loss: 104.4967\n",
      "Epoch [1/25], Step [148/590], Loss: 109.1926\n",
      "Epoch [1/25], Step [152/590], Loss: 113.0774\n",
      "Epoch [1/25], Step [156/590], Loss: 105.8070\n",
      "Epoch [1/25], Step [160/590], Loss: 103.6085\n",
      "Epoch [1/25], Step [164/590], Loss: 95.1572\n",
      "Epoch [1/25], Step [168/590], Loss: 91.4428\n",
      "Epoch [1/25], Step [172/590], Loss: 113.0714\n",
      "Epoch [1/25], Step [176/590], Loss: 96.6190\n",
      "Epoch [1/25], Step [180/590], Loss: 99.7316\n",
      "Epoch [1/25], Step [184/590], Loss: 96.2305\n",
      "Epoch [1/25], Step [188/590], Loss: 104.3161\n",
      "Epoch [1/25], Step [192/590], Loss: 95.4562\n",
      "Epoch [1/25], Step [196/590], Loss: 105.3354\n",
      "Epoch [1/25], Step [200/590], Loss: 106.1502\n",
      "Epoch [1/25], Step [204/590], Loss: 102.0818\n",
      "Epoch [1/25], Step [208/590], Loss: 104.8767\n",
      "Epoch [1/25], Step [212/590], Loss: 89.2442\n",
      "Epoch [1/25], Step [216/590], Loss: 94.4688\n",
      "Epoch [1/25], Step [220/590], Loss: 87.9306\n",
      "Epoch [1/25], Step [224/590], Loss: 90.4049\n",
      "Epoch [1/25], Step [228/590], Loss: 101.1163\n",
      "Epoch [1/25], Step [232/590], Loss: 91.9417\n",
      "Epoch [1/25], Step [236/590], Loss: 99.0630\n",
      "Epoch [1/25], Step [240/590], Loss: 90.8211\n",
      "Epoch [1/25], Step [244/590], Loss: 97.5053\n",
      "Epoch [1/25], Step [248/590], Loss: 82.3213\n",
      "Epoch [1/25], Step [252/590], Loss: 84.3840\n",
      "Epoch [1/25], Step [256/590], Loss: 96.0516\n",
      "Epoch [1/25], Step [260/590], Loss: 85.0347\n",
      "Epoch [1/25], Step [264/590], Loss: 102.9193\n",
      "Epoch [1/25], Step [268/590], Loss: 98.1137\n",
      "Epoch [1/25], Step [272/590], Loss: 83.6115\n",
      "Epoch [1/25], Step [276/590], Loss: 74.4177\n",
      "Epoch [1/25], Step [280/590], Loss: 93.5254\n",
      "Epoch [1/25], Step [284/590], Loss: 96.8242\n",
      "Epoch [1/25], Step [288/590], Loss: 87.4948\n",
      "Epoch [1/25], Step [292/590], Loss: 89.6990\n",
      "Epoch [1/25], Step [296/590], Loss: 71.5240\n",
      "Epoch [1/25], Step [300/590], Loss: 83.4348\n",
      "Epoch [1/25], Step [304/590], Loss: 74.5888\n",
      "Epoch [1/25], Step [308/590], Loss: 82.7977\n",
      "Epoch [1/25], Step [312/590], Loss: 75.0338\n",
      "Epoch [1/25], Step [316/590], Loss: 79.8698\n",
      "Epoch [1/25], Step [320/590], Loss: 93.5031\n",
      "Epoch [1/25], Step [324/590], Loss: 84.6834\n",
      "Epoch [1/25], Step [328/590], Loss: 78.9361\n",
      "Epoch [1/25], Step [332/590], Loss: 82.2505\n",
      "Epoch [1/25], Step [336/590], Loss: 80.1749\n",
      "Epoch [1/25], Step [340/590], Loss: 76.7337\n",
      "Epoch [1/25], Step [344/590], Loss: 84.0763\n",
      "Epoch [1/25], Step [348/590], Loss: 84.7456\n",
      "Epoch [1/25], Step [352/590], Loss: 73.8891\n",
      "Epoch [1/25], Step [356/590], Loss: 79.3271\n",
      "Epoch [1/25], Step [360/590], Loss: 76.1001\n",
      "Epoch [1/25], Step [364/590], Loss: 77.5962\n",
      "Epoch [1/25], Step [368/590], Loss: 73.5885\n",
      "Epoch [1/25], Step [372/590], Loss: 68.1862\n",
      "Epoch [1/25], Step [376/590], Loss: 67.3353\n",
      "Epoch [1/25], Step [380/590], Loss: 59.4876\n",
      "Epoch [1/25], Step [384/590], Loss: 66.2900\n",
      "Epoch [1/25], Step [388/590], Loss: 64.8491\n",
      "Epoch [1/25], Step [392/590], Loss: 67.7036\n",
      "Epoch [1/25], Step [396/590], Loss: 76.3808\n",
      "Epoch [1/25], Step [400/590], Loss: 67.2865\n",
      "Epoch [1/25], Step [404/590], Loss: 69.9858\n",
      "Epoch [1/25], Step [408/590], Loss: 75.9984\n",
      "Epoch [1/25], Step [412/590], Loss: 74.1434\n",
      "Epoch [1/25], Step [416/590], Loss: 60.5615\n",
      "Epoch [1/25], Step [420/590], Loss: 80.9743\n",
      "Epoch [1/25], Step [424/590], Loss: 72.6249\n",
      "Epoch [1/25], Step [428/590], Loss: 58.2794\n",
      "Epoch [1/25], Step [432/590], Loss: 65.8044\n",
      "Epoch [1/25], Step [436/590], Loss: 67.7339\n",
      "Epoch [1/25], Step [440/590], Loss: 55.4436\n",
      "Epoch [1/25], Step [444/590], Loss: 56.7499\n",
      "Epoch [1/25], Step [448/590], Loss: 64.4325\n",
      "Epoch [1/25], Step [452/590], Loss: 71.1326\n",
      "Epoch [1/25], Step [456/590], Loss: 55.1616\n",
      "Epoch [1/25], Step [460/590], Loss: 56.1760\n",
      "Epoch [1/25], Step [464/590], Loss: 58.4903\n",
      "Epoch [1/25], Step [468/590], Loss: 58.4793\n",
      "Epoch [1/25], Step [472/590], Loss: 57.8745\n",
      "Epoch [1/25], Step [476/590], Loss: 65.9274\n",
      "Epoch [1/25], Step [480/590], Loss: 51.5464\n",
      "Epoch [1/25], Step [484/590], Loss: 46.1564\n",
      "Epoch [1/25], Step [488/590], Loss: 49.3946\n",
      "Epoch [1/25], Step [492/590], Loss: 49.9604\n",
      "Epoch [1/25], Step [496/590], Loss: 40.4041\n",
      "Epoch [1/25], Step [500/590], Loss: 52.3586\n",
      "Epoch [1/25], Step [504/590], Loss: 62.6933\n",
      "Epoch [1/25], Step [508/590], Loss: 53.4565\n",
      "Epoch [1/25], Step [512/590], Loss: 53.4539\n",
      "Epoch [1/25], Step [516/590], Loss: 50.2565\n",
      "Epoch [1/25], Step [520/590], Loss: 49.6972\n",
      "Epoch [1/25], Step [524/590], Loss: 51.0521\n",
      "Epoch [1/25], Step [528/590], Loss: 67.3694\n",
      "Epoch [1/25], Step [532/590], Loss: 50.5779\n",
      "Epoch [1/25], Step [536/590], Loss: 50.4653\n",
      "Epoch [1/25], Step [540/590], Loss: 58.1484\n",
      "Epoch [1/25], Step [544/590], Loss: 48.8309\n",
      "Epoch [1/25], Step [548/590], Loss: 50.5508\n",
      "Epoch [1/25], Step [552/590], Loss: 51.4939\n",
      "Epoch [1/25], Step [556/590], Loss: 51.9883\n",
      "Epoch [1/25], Step [560/590], Loss: 41.9696\n",
      "Epoch [1/25], Step [564/590], Loss: 53.7144\n",
      "Epoch [1/25], Step [568/590], Loss: 62.4583\n",
      "Epoch [1/25], Step [572/590], Loss: 47.4216\n",
      "Epoch [1/25], Step [576/590], Loss: 57.3211\n",
      "Epoch [1/25], Step [580/590], Loss: 54.6098\n",
      "Epoch [1/25], Step [584/590], Loss: 43.4320\n",
      "Epoch [1/25], Step [588/590], Loss: 51.3915\n",
      "epoch 1\n",
      "Epoch [2/25], Step [4/590], Loss: 53.2952\n",
      "Epoch [2/25], Step [8/590], Loss: 39.2088\n",
      "Epoch [2/25], Step [12/590], Loss: 42.5104\n",
      "Epoch [2/25], Step [16/590], Loss: 42.0298\n",
      "Epoch [2/25], Step [20/590], Loss: 53.3004\n",
      "Epoch [2/25], Step [24/590], Loss: 41.0331\n",
      "Epoch [2/25], Step [28/590], Loss: 37.6549\n",
      "Epoch [2/25], Step [32/590], Loss: 49.8029\n",
      "Epoch [2/25], Step [36/590], Loss: 43.9921\n",
      "Epoch [2/25], Step [40/590], Loss: 38.5092\n",
      "Epoch [2/25], Step [44/590], Loss: 37.1836\n",
      "Epoch [2/25], Step [48/590], Loss: 43.8069\n",
      "Epoch [2/25], Step [52/590], Loss: 36.7322\n",
      "Epoch [2/25], Step [56/590], Loss: 35.2690\n",
      "Epoch [2/25], Step [60/590], Loss: 47.6805\n",
      "Epoch [2/25], Step [64/590], Loss: 46.5535\n",
      "Epoch [2/25], Step [68/590], Loss: 34.1162\n",
      "Epoch [2/25], Step [72/590], Loss: 46.4195\n",
      "Epoch [2/25], Step [76/590], Loss: 44.4619\n",
      "Epoch [2/25], Step [80/590], Loss: 41.3328\n",
      "Epoch [2/25], Step [84/590], Loss: 55.6488\n",
      "Epoch [2/25], Step [88/590], Loss: 31.7351\n",
      "Epoch [2/25], Step [92/590], Loss: 46.2963\n",
      "Epoch [2/25], Step [96/590], Loss: 36.3628\n",
      "Epoch [2/25], Step [100/590], Loss: 34.6745\n",
      "Epoch [2/25], Step [104/590], Loss: 43.1162\n",
      "Epoch [2/25], Step [108/590], Loss: 53.8313\n",
      "Epoch [2/25], Step [112/590], Loss: 41.1270\n",
      "Epoch [2/25], Step [116/590], Loss: 41.7685\n",
      "Epoch [2/25], Step [120/590], Loss: 52.0808\n",
      "Epoch [2/25], Step [124/590], Loss: 37.5160\n",
      "Epoch [2/25], Step [128/590], Loss: 50.2584\n",
      "Epoch [2/25], Step [132/590], Loss: 41.5034\n",
      "Epoch [2/25], Step [136/590], Loss: 39.4208\n",
      "Epoch [2/25], Step [140/590], Loss: 38.5046\n",
      "Epoch [2/25], Step [144/590], Loss: 41.0286\n",
      "Epoch [2/25], Step [148/590], Loss: 37.6672\n",
      "Epoch [2/25], Step [152/590], Loss: 28.5344\n",
      "Epoch [2/25], Step [156/590], Loss: 36.3630\n",
      "Epoch [2/25], Step [160/590], Loss: 49.1483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Step [164/590], Loss: 35.7189\n",
      "Epoch [2/25], Step [168/590], Loss: 31.9827\n",
      "Epoch [2/25], Step [172/590], Loss: 41.2650\n",
      "Epoch [2/25], Step [176/590], Loss: 36.1515\n",
      "Epoch [2/25], Step [180/590], Loss: 24.9524\n",
      "Epoch [2/25], Step [184/590], Loss: 35.5971\n",
      "Epoch [2/25], Step [188/590], Loss: 40.6738\n",
      "Epoch [2/25], Step [192/590], Loss: 31.4475\n",
      "Epoch [2/25], Step [196/590], Loss: 27.9428\n",
      "Epoch [2/25], Step [200/590], Loss: 30.9912\n",
      "Epoch [2/25], Step [204/590], Loss: 33.6154\n",
      "Epoch [2/25], Step [208/590], Loss: 38.5541\n",
      "Epoch [2/25], Step [212/590], Loss: 41.9552\n",
      "Epoch [2/25], Step [216/590], Loss: 40.0477\n",
      "Epoch [2/25], Step [220/590], Loss: 30.4736\n",
      "Epoch [2/25], Step [224/590], Loss: 29.1196\n",
      "Epoch [2/25], Step [228/590], Loss: 26.9132\n",
      "Epoch [2/25], Step [232/590], Loss: 24.7780\n",
      "Epoch [2/25], Step [236/590], Loss: 32.8177\n",
      "Epoch [2/25], Step [240/590], Loss: 33.9107\n",
      "Epoch [2/25], Step [244/590], Loss: 34.6374\n",
      "Epoch [2/25], Step [248/590], Loss: 28.0133\n",
      "Epoch [2/25], Step [252/590], Loss: 28.4177\n",
      "Epoch [2/25], Step [256/590], Loss: 25.2406\n",
      "Epoch [2/25], Step [260/590], Loss: 22.8365\n",
      "Epoch [2/25], Step [264/590], Loss: 28.7827\n",
      "Epoch [2/25], Step [268/590], Loss: 29.8598\n",
      "Epoch [2/25], Step [272/590], Loss: 37.3546\n",
      "Epoch [2/25], Step [276/590], Loss: 35.9651\n",
      "Epoch [2/25], Step [280/590], Loss: 27.0720\n",
      "Epoch [2/25], Step [284/590], Loss: 22.9342\n",
      "Epoch [2/25], Step [288/590], Loss: 25.5521\n",
      "Epoch [2/25], Step [292/590], Loss: 21.9251\n",
      "Epoch [2/25], Step [296/590], Loss: 40.6068\n",
      "Epoch [2/25], Step [300/590], Loss: 31.5925\n",
      "Epoch [2/25], Step [304/590], Loss: 19.3007\n",
      "Epoch [2/25], Step [308/590], Loss: 23.5506\n",
      "Epoch [2/25], Step [312/590], Loss: 32.8084\n",
      "Epoch [2/25], Step [316/590], Loss: 40.3217\n",
      "Epoch [2/25], Step [320/590], Loss: 22.2025\n",
      "Epoch [2/25], Step [324/590], Loss: 28.5759\n",
      "Epoch [2/25], Step [328/590], Loss: 26.4243\n",
      "Epoch [2/25], Step [332/590], Loss: 26.3242\n",
      "Epoch [2/25], Step [336/590], Loss: 28.4074\n",
      "Epoch [2/25], Step [340/590], Loss: 22.3915\n",
      "Epoch [2/25], Step [344/590], Loss: 19.3305\n",
      "Epoch [2/25], Step [348/590], Loss: 25.0478\n",
      "Epoch [2/25], Step [352/590], Loss: 17.2562\n",
      "Epoch [2/25], Step [356/590], Loss: 27.9082\n",
      "Epoch [2/25], Step [360/590], Loss: 31.6941\n",
      "Epoch [2/25], Step [364/590], Loss: 38.9424\n",
      "Epoch [2/25], Step [368/590], Loss: 36.1077\n",
      "Epoch [2/25], Step [372/590], Loss: 17.2628\n",
      "Epoch [2/25], Step [376/590], Loss: 35.1699\n",
      "Epoch [2/25], Step [380/590], Loss: 34.4434\n",
      "Epoch [2/25], Step [384/590], Loss: 25.4825\n",
      "Epoch [2/25], Step [388/590], Loss: 22.3625\n",
      "Epoch [2/25], Step [392/590], Loss: 33.5533\n",
      "Epoch [2/25], Step [396/590], Loss: 19.3311\n",
      "Epoch [2/25], Step [400/590], Loss: 15.6806\n",
      "Epoch [2/25], Step [404/590], Loss: 35.3021\n",
      "Epoch [2/25], Step [408/590], Loss: 25.9841\n",
      "Epoch [2/25], Step [412/590], Loss: 39.3821\n",
      "Epoch [2/25], Step [416/590], Loss: 25.1859\n",
      "Epoch [2/25], Step [420/590], Loss: 20.8943\n",
      "Epoch [2/25], Step [424/590], Loss: 36.6112\n",
      "Epoch [2/25], Step [428/590], Loss: 26.6856\n",
      "Epoch [2/25], Step [432/590], Loss: 19.6827\n",
      "Epoch [2/25], Step [436/590], Loss: 27.3535\n",
      "Epoch [2/25], Step [440/590], Loss: 22.9173\n",
      "Epoch [2/25], Step [444/590], Loss: 18.8933\n",
      "Epoch [2/25], Step [448/590], Loss: 29.8202\n",
      "Epoch [2/25], Step [452/590], Loss: 26.1130\n",
      "Epoch [2/25], Step [456/590], Loss: 28.8364\n",
      "Epoch [2/25], Step [460/590], Loss: 19.1641\n",
      "Epoch [2/25], Step [464/590], Loss: 31.4242\n",
      "Epoch [2/25], Step [468/590], Loss: 13.2149\n",
      "Epoch [2/25], Step [472/590], Loss: 30.5010\n",
      "Epoch [2/25], Step [476/590], Loss: 25.9888\n",
      "Epoch [2/25], Step [480/590], Loss: 23.6328\n",
      "Epoch [2/25], Step [484/590], Loss: 17.2543\n",
      "Epoch [2/25], Step [488/590], Loss: 26.9941\n",
      "Epoch [2/25], Step [492/590], Loss: 29.7535\n",
      "Epoch [2/25], Step [496/590], Loss: 25.3933\n",
      "Epoch [2/25], Step [500/590], Loss: 17.3144\n",
      "Epoch [2/25], Step [504/590], Loss: 27.1852\n",
      "Epoch [2/25], Step [508/590], Loss: 21.6384\n",
      "Epoch [2/25], Step [512/590], Loss: 19.2273\n",
      "Epoch [2/25], Step [516/590], Loss: 32.3000\n",
      "Epoch [2/25], Step [520/590], Loss: 16.4499\n",
      "Epoch [2/25], Step [524/590], Loss: 14.6331\n",
      "Epoch [2/25], Step [528/590], Loss: 19.8894\n",
      "Epoch [2/25], Step [532/590], Loss: 34.6508\n",
      "Epoch [2/25], Step [536/590], Loss: 22.0573\n",
      "Epoch [2/25], Step [540/590], Loss: 33.9653\n",
      "Epoch [2/25], Step [544/590], Loss: 26.1143\n",
      "Epoch [2/25], Step [548/590], Loss: 28.3229\n",
      "Epoch [2/25], Step [552/590], Loss: 24.0666\n",
      "Epoch [2/25], Step [556/590], Loss: 20.7076\n",
      "Epoch [2/25], Step [560/590], Loss: 21.9634\n",
      "Epoch [2/25], Step [564/590], Loss: 21.8029\n",
      "Epoch [2/25], Step [568/590], Loss: 29.2157\n",
      "Epoch [2/25], Step [572/590], Loss: 17.2797\n",
      "Epoch [2/25], Step [576/590], Loss: 25.8752\n",
      "Epoch [2/25], Step [580/590], Loss: 12.5574\n",
      "Epoch [2/25], Step [584/590], Loss: 12.3070\n",
      "Epoch [2/25], Step [588/590], Loss: 23.6298\n",
      "epoch 2\n",
      "Epoch [3/25], Step [4/590], Loss: 20.7704\n",
      "Epoch [3/25], Step [8/590], Loss: 13.0779\n",
      "Epoch [3/25], Step [12/590], Loss: 20.9823\n",
      "Epoch [3/25], Step [16/590], Loss: 25.3695\n",
      "Epoch [3/25], Step [20/590], Loss: 14.4147\n",
      "Epoch [3/25], Step [24/590], Loss: 19.3976\n",
      "Epoch [3/25], Step [28/590], Loss: 14.9309\n",
      "Epoch [3/25], Step [32/590], Loss: 23.6476\n",
      "Epoch [3/25], Step [36/590], Loss: 19.4946\n",
      "Epoch [3/25], Step [40/590], Loss: 22.6058\n",
      "Epoch [3/25], Step [44/590], Loss: 17.4670\n",
      "Epoch [3/25], Step [48/590], Loss: 16.9407\n",
      "Epoch [3/25], Step [52/590], Loss: 19.8178\n",
      "Epoch [3/25], Step [56/590], Loss: 13.9327\n",
      "Epoch [3/25], Step [60/590], Loss: 12.5265\n",
      "Epoch [3/25], Step [64/590], Loss: 13.7441\n",
      "Epoch [3/25], Step [68/590], Loss: 24.3686\n",
      "Epoch [3/25], Step [72/590], Loss: 10.3582\n",
      "Epoch [3/25], Step [76/590], Loss: 27.8125\n",
      "Epoch [3/25], Step [80/590], Loss: 22.7566\n",
      "Epoch [3/25], Step [84/590], Loss: 14.7942\n",
      "Epoch [3/25], Step [88/590], Loss: 29.1109\n",
      "Epoch [3/25], Step [92/590], Loss: 19.4923\n",
      "Epoch [3/25], Step [96/590], Loss: 16.8783\n",
      "Epoch [3/25], Step [100/590], Loss: 24.5306\n",
      "Epoch [3/25], Step [104/590], Loss: 16.4657\n",
      "Epoch [3/25], Step [108/590], Loss: 16.9788\n",
      "Epoch [3/25], Step [112/590], Loss: 20.5713\n",
      "Epoch [3/25], Step [116/590], Loss: 10.5958\n",
      "Epoch [3/25], Step [120/590], Loss: 18.2836\n",
      "Epoch [3/25], Step [124/590], Loss: 32.9285\n",
      "Epoch [3/25], Step [128/590], Loss: 24.5626\n",
      "Epoch [3/25], Step [132/590], Loss: 15.5096\n",
      "Epoch [3/25], Step [136/590], Loss: 18.9838\n",
      "Epoch [3/25], Step [140/590], Loss: 22.8023\n",
      "Epoch [3/25], Step [144/590], Loss: 19.2935\n",
      "Epoch [3/25], Step [148/590], Loss: 17.9358\n",
      "Epoch [3/25], Step [152/590], Loss: 10.7995\n",
      "Epoch [3/25], Step [156/590], Loss: 20.5961\n",
      "Epoch [3/25], Step [160/590], Loss: 16.6614\n",
      "Epoch [3/25], Step [164/590], Loss: 9.9289\n",
      "Epoch [3/25], Step [168/590], Loss: 19.2700\n",
      "Epoch [3/25], Step [172/590], Loss: 15.8034\n",
      "Epoch [3/25], Step [176/590], Loss: 13.2353\n",
      "Epoch [3/25], Step [180/590], Loss: 11.1368\n",
      "Epoch [3/25], Step [184/590], Loss: 11.7290\n",
      "Epoch [3/25], Step [188/590], Loss: 16.3338\n",
      "Epoch [3/25], Step [192/590], Loss: 7.9450\n",
      "Epoch [3/25], Step [196/590], Loss: 6.4195\n",
      "Epoch [3/25], Step [200/590], Loss: 11.1440\n",
      "Epoch [3/25], Step [204/590], Loss: 16.7836\n",
      "Epoch [3/25], Step [208/590], Loss: 13.5133\n",
      "Epoch [3/25], Step [212/590], Loss: 19.1380\n",
      "Epoch [3/25], Step [216/590], Loss: 11.7976\n",
      "Epoch [3/25], Step [220/590], Loss: 16.7461\n",
      "Epoch [3/25], Step [224/590], Loss: 13.1772\n",
      "Epoch [3/25], Step [228/590], Loss: 11.6726\n",
      "Epoch [3/25], Step [232/590], Loss: 11.9611\n",
      "Epoch [3/25], Step [236/590], Loss: 16.5913\n",
      "Epoch [3/25], Step [240/590], Loss: 17.9574\n",
      "Epoch [3/25], Step [244/590], Loss: 9.3195\n",
      "Epoch [3/25], Step [248/590], Loss: 25.0220\n",
      "Epoch [3/25], Step [252/590], Loss: 27.4519\n",
      "Epoch [3/25], Step [256/590], Loss: 8.3102\n",
      "Epoch [3/25], Step [260/590], Loss: 15.6549\n",
      "Epoch [3/25], Step [264/590], Loss: 21.6759\n",
      "Epoch [3/25], Step [268/590], Loss: 15.0988\n",
      "Epoch [3/25], Step [272/590], Loss: 15.6924\n",
      "Epoch [3/25], Step [276/590], Loss: 6.7681\n",
      "Epoch [3/25], Step [280/590], Loss: 22.0615\n",
      "Epoch [3/25], Step [284/590], Loss: 13.8676\n",
      "Epoch [3/25], Step [288/590], Loss: 14.6023\n",
      "Epoch [3/25], Step [292/590], Loss: 24.0929\n",
      "Epoch [3/25], Step [296/590], Loss: 25.1836\n",
      "Epoch [3/25], Step [300/590], Loss: 18.4113\n",
      "Epoch [3/25], Step [304/590], Loss: 14.9826\n",
      "Epoch [3/25], Step [308/590], Loss: 9.6244\n",
      "Epoch [3/25], Step [312/590], Loss: 18.8029\n",
      "Epoch [3/25], Step [316/590], Loss: 16.3294\n",
      "Epoch [3/25], Step [320/590], Loss: 15.8294\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Step [324/590], Loss: 13.7369\n",
      "Epoch [3/25], Step [328/590], Loss: 19.6116\n",
      "Epoch [3/25], Step [332/590], Loss: 12.8418\n",
      "Epoch [3/25], Step [336/590], Loss: 14.9764\n",
      "Epoch [3/25], Step [340/590], Loss: 10.6412\n",
      "Epoch [3/25], Step [344/590], Loss: 12.7865\n",
      "Epoch [3/25], Step [348/590], Loss: 14.5134\n",
      "Epoch [3/25], Step [352/590], Loss: 10.1731\n",
      "Epoch [3/25], Step [356/590], Loss: 14.3945\n",
      "Epoch [3/25], Step [360/590], Loss: 12.4252\n",
      "Epoch [3/25], Step [364/590], Loss: 16.6590\n",
      "Epoch [3/25], Step [368/590], Loss: 11.3807\n",
      "Epoch [3/25], Step [372/590], Loss: 16.2051\n",
      "Epoch [3/25], Step [376/590], Loss: 22.0892\n",
      "Epoch [3/25], Step [380/590], Loss: 16.1594\n",
      "Epoch [3/25], Step [384/590], Loss: 17.6668\n",
      "Epoch [3/25], Step [388/590], Loss: 14.3764\n",
      "Epoch [3/25], Step [392/590], Loss: 6.1202\n",
      "Epoch [3/25], Step [396/590], Loss: 12.7490\n",
      "Epoch [3/25], Step [400/590], Loss: 12.4245\n",
      "Epoch [3/25], Step [404/590], Loss: 20.3768\n",
      "Epoch [3/25], Step [408/590], Loss: 15.1903\n",
      "Epoch [3/25], Step [412/590], Loss: 13.8010\n",
      "Epoch [3/25], Step [416/590], Loss: 17.4193\n",
      "Epoch [3/25], Step [420/590], Loss: 14.2905\n",
      "Epoch [3/25], Step [424/590], Loss: 14.1739\n",
      "Epoch [3/25], Step [428/590], Loss: 18.7217\n",
      "Epoch [3/25], Step [432/590], Loss: 9.5257\n",
      "Epoch [3/25], Step [436/590], Loss: 15.6695\n",
      "Epoch [3/25], Step [440/590], Loss: 11.2531\n",
      "Epoch [3/25], Step [444/590], Loss: 18.0039\n",
      "Epoch [3/25], Step [448/590], Loss: 14.7104\n",
      "Epoch [3/25], Step [452/590], Loss: 7.4115\n",
      "Epoch [3/25], Step [456/590], Loss: 7.5920\n",
      "Epoch [3/25], Step [460/590], Loss: 19.4782\n",
      "Epoch [3/25], Step [464/590], Loss: 11.4932\n",
      "Epoch [3/25], Step [468/590], Loss: 15.7003\n",
      "Epoch [3/25], Step [472/590], Loss: 12.0574\n",
      "Epoch [3/25], Step [476/590], Loss: 22.1539\n",
      "Epoch [3/25], Step [480/590], Loss: 11.0022\n",
      "Epoch [3/25], Step [484/590], Loss: 10.0172\n",
      "Epoch [3/25], Step [488/590], Loss: 4.1473\n",
      "Epoch [3/25], Step [492/590], Loss: 7.1264\n",
      "Epoch [3/25], Step [496/590], Loss: 10.0780\n",
      "Epoch [3/25], Step [500/590], Loss: 8.5176\n",
      "Epoch [3/25], Step [504/590], Loss: 12.5287\n",
      "Epoch [3/25], Step [508/590], Loss: 9.9146\n",
      "Epoch [3/25], Step [512/590], Loss: 7.0271\n",
      "Epoch [3/25], Step [516/590], Loss: 12.8629\n",
      "Epoch [3/25], Step [520/590], Loss: 16.8277\n",
      "Epoch [3/25], Step [524/590], Loss: 12.3304\n",
      "Epoch [3/25], Step [528/590], Loss: 12.0563\n",
      "Epoch [3/25], Step [532/590], Loss: 11.9214\n",
      "Epoch [3/25], Step [536/590], Loss: 22.8265\n",
      "Epoch [3/25], Step [540/590], Loss: 18.9559\n",
      "Epoch [3/25], Step [544/590], Loss: 13.9659\n",
      "Epoch [3/25], Step [548/590], Loss: 17.9822\n",
      "Epoch [3/25], Step [552/590], Loss: 24.5409\n",
      "Epoch [3/25], Step [556/590], Loss: 12.0748\n",
      "Epoch [3/25], Step [560/590], Loss: 14.2797\n",
      "Epoch [3/25], Step [564/590], Loss: 11.3395\n",
      "Epoch [3/25], Step [568/590], Loss: 19.1437\n",
      "Epoch [3/25], Step [572/590], Loss: 9.0540\n",
      "Epoch [3/25], Step [576/590], Loss: 10.6808\n",
      "Epoch [3/25], Step [580/590], Loss: 11.6770\n",
      "Epoch [3/25], Step [584/590], Loss: 7.1017\n",
      "Epoch [3/25], Step [588/590], Loss: 9.1788\n",
      "epoch 3\n",
      "Epoch [4/25], Step [4/590], Loss: 14.8431\n",
      "Epoch [4/25], Step [8/590], Loss: 7.2152\n",
      "Epoch [4/25], Step [12/590], Loss: 11.4645\n",
      "Epoch [4/25], Step [16/590], Loss: 14.7307\n",
      "Epoch [4/25], Step [20/590], Loss: 5.5439\n",
      "Epoch [4/25], Step [24/590], Loss: 9.9266\n",
      "Epoch [4/25], Step [28/590], Loss: 4.6783\n",
      "Epoch [4/25], Step [32/590], Loss: 9.6754\n",
      "Epoch [4/25], Step [36/590], Loss: 6.4144\n",
      "Epoch [4/25], Step [40/590], Loss: 13.1201\n",
      "Epoch [4/25], Step [44/590], Loss: 12.9513\n",
      "Epoch [4/25], Step [48/590], Loss: 13.0576\n",
      "Epoch [4/25], Step [52/590], Loss: 6.2053\n",
      "Epoch [4/25], Step [56/590], Loss: 9.7923\n",
      "Epoch [4/25], Step [60/590], Loss: 11.3710\n",
      "Epoch [4/25], Step [64/590], Loss: 6.2442\n",
      "Epoch [4/25], Step [68/590], Loss: 8.1834\n",
      "Epoch [4/25], Step [72/590], Loss: 11.3738\n",
      "Epoch [4/25], Step [76/590], Loss: 15.6599\n",
      "Epoch [4/25], Step [80/590], Loss: 8.8029\n",
      "Epoch [4/25], Step [84/590], Loss: 7.5073\n",
      "Epoch [4/25], Step [88/590], Loss: 8.3721\n",
      "Epoch [4/25], Step [92/590], Loss: 11.4706\n",
      "Epoch [4/25], Step [96/590], Loss: 5.2930\n",
      "Epoch [4/25], Step [100/590], Loss: 17.1524\n",
      "Epoch [4/25], Step [104/590], Loss: 3.8443\n",
      "Epoch [4/25], Step [108/590], Loss: 7.7058\n",
      "Epoch [4/25], Step [112/590], Loss: 17.0419\n",
      "Epoch [4/25], Step [116/590], Loss: 14.7931\n",
      "Epoch [4/25], Step [120/590], Loss: 10.7174\n",
      "Epoch [4/25], Step [124/590], Loss: 8.3085\n",
      "Epoch [4/25], Step [128/590], Loss: 6.8653\n",
      "Epoch [4/25], Step [132/590], Loss: 12.9600\n",
      "Epoch [4/25], Step [136/590], Loss: 4.0125\n",
      "Epoch [4/25], Step [140/590], Loss: 12.1192\n",
      "Epoch [4/25], Step [144/590], Loss: 9.7337\n",
      "Epoch [4/25], Step [148/590], Loss: 9.8184\n",
      "Epoch [4/25], Step [152/590], Loss: 4.4275\n",
      "Epoch [4/25], Step [156/590], Loss: 6.7542\n",
      "Epoch [4/25], Step [160/590], Loss: 8.8884\n",
      "Epoch [4/25], Step [164/590], Loss: 11.8294\n",
      "Epoch [4/25], Step [168/590], Loss: 14.4560\n",
      "Epoch [4/25], Step [172/590], Loss: 10.0863\n",
      "Epoch [4/25], Step [176/590], Loss: 6.6017\n",
      "Epoch [4/25], Step [180/590], Loss: 4.5173\n",
      "Epoch [4/25], Step [184/590], Loss: 10.7208\n",
      "Epoch [4/25], Step [188/590], Loss: 14.7719\n",
      "Epoch [4/25], Step [192/590], Loss: 3.9351\n",
      "Epoch [4/25], Step [196/590], Loss: 6.8543\n",
      "Epoch [4/25], Step [200/590], Loss: 3.2552\n",
      "Epoch [4/25], Step [204/590], Loss: 12.4195\n",
      "Epoch [4/25], Step [208/590], Loss: 5.5169\n",
      "Epoch [4/25], Step [212/590], Loss: 5.4141\n",
      "Epoch [4/25], Step [216/590], Loss: 9.0924\n",
      "Epoch [4/25], Step [220/590], Loss: 8.6580\n",
      "Epoch [4/25], Step [224/590], Loss: 16.9645\n",
      "Epoch [4/25], Step [228/590], Loss: 5.4278\n",
      "Epoch [4/25], Step [232/590], Loss: 5.7798\n",
      "Epoch [4/25], Step [236/590], Loss: 12.2876\n",
      "Epoch [4/25], Step [240/590], Loss: 10.3932\n",
      "Epoch [4/25], Step [244/590], Loss: 9.1198\n",
      "Epoch [4/25], Step [248/590], Loss: 10.3183\n",
      "Epoch [4/25], Step [252/590], Loss: 8.8269\n",
      "Epoch [4/25], Step [256/590], Loss: 6.9341\n",
      "Epoch [4/25], Step [260/590], Loss: 10.1900\n",
      "Epoch [4/25], Step [264/590], Loss: 5.6728\n",
      "Epoch [4/25], Step [268/590], Loss: 6.1346\n",
      "Epoch [4/25], Step [272/590], Loss: 8.0510\n",
      "Epoch [4/25], Step [276/590], Loss: 12.2738\n",
      "Epoch [4/25], Step [280/590], Loss: 2.7191\n",
      "Epoch [4/25], Step [284/590], Loss: 7.0348\n",
      "Epoch [4/25], Step [288/590], Loss: 8.1122\n",
      "Epoch [4/25], Step [292/590], Loss: 10.6408\n",
      "Epoch [4/25], Step [296/590], Loss: 4.9969\n",
      "Epoch [4/25], Step [300/590], Loss: 3.7531\n",
      "Epoch [4/25], Step [304/590], Loss: 8.0871\n",
      "Epoch [4/25], Step [308/590], Loss: 10.2857\n",
      "Epoch [4/25], Step [312/590], Loss: 15.3792\n",
      "Epoch [4/25], Step [316/590], Loss: 9.0426\n",
      "Epoch [4/25], Step [320/590], Loss: 3.8510\n",
      "Epoch [4/25], Step [324/590], Loss: 15.6963\n",
      "Epoch [4/25], Step [328/590], Loss: 10.5991\n",
      "Epoch [4/25], Step [332/590], Loss: 3.0542\n",
      "Epoch [4/25], Step [336/590], Loss: 3.7942\n",
      "Epoch [4/25], Step [340/590], Loss: 4.9933\n",
      "Epoch [4/25], Step [344/590], Loss: 5.6761\n",
      "Epoch [4/25], Step [348/590], Loss: 4.7636\n",
      "Epoch [4/25], Step [352/590], Loss: 4.6289\n",
      "Epoch [4/25], Step [356/590], Loss: 10.9535\n",
      "Epoch [4/25], Step [360/590], Loss: 13.4668\n",
      "Epoch [4/25], Step [364/590], Loss: 18.1550\n",
      "Epoch [4/25], Step [368/590], Loss: 2.1071\n",
      "Epoch [4/25], Step [372/590], Loss: 10.5014\n",
      "Epoch [4/25], Step [376/590], Loss: 7.6997\n",
      "Epoch [4/25], Step [380/590], Loss: 12.9961\n",
      "Epoch [4/25], Step [384/590], Loss: 10.9274\n",
      "Epoch [4/25], Step [388/590], Loss: 11.3003\n",
      "Epoch [4/25], Step [392/590], Loss: 6.8152\n",
      "Epoch [4/25], Step [396/590], Loss: 6.8268\n",
      "Epoch [4/25], Step [400/590], Loss: 9.6697\n",
      "Epoch [4/25], Step [404/590], Loss: 10.2538\n",
      "Epoch [4/25], Step [408/590], Loss: 5.5109\n",
      "Epoch [4/25], Step [412/590], Loss: 3.2262\n",
      "Epoch [4/25], Step [416/590], Loss: 8.0036\n",
      "Epoch [4/25], Step [420/590], Loss: 6.2745\n",
      "Epoch [4/25], Step [424/590], Loss: 9.5448\n",
      "Epoch [4/25], Step [428/590], Loss: 6.5428\n",
      "Epoch [4/25], Step [432/590], Loss: 3.3753\n",
      "Epoch [4/25], Step [436/590], Loss: 6.4341\n",
      "Epoch [4/25], Step [440/590], Loss: 12.5539\n",
      "Epoch [4/25], Step [444/590], Loss: 9.3229\n",
      "Epoch [4/25], Step [448/590], Loss: 23.5838\n",
      "Epoch [4/25], Step [452/590], Loss: 9.9583\n",
      "Epoch [4/25], Step [456/590], Loss: 7.7707\n",
      "Epoch [4/25], Step [460/590], Loss: 3.1968\n",
      "Epoch [4/25], Step [464/590], Loss: 10.4909\n",
      "Epoch [4/25], Step [468/590], Loss: 8.9966\n",
      "Epoch [4/25], Step [472/590], Loss: 5.0460\n",
      "Epoch [4/25], Step [476/590], Loss: 3.9933\n",
      "Epoch [4/25], Step [480/590], Loss: 5.8696\n",
      "Epoch [4/25], Step [484/590], Loss: 11.1641\n",
      "Epoch [4/25], Step [488/590], Loss: 9.8339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Step [492/590], Loss: 13.5626\n",
      "Epoch [4/25], Step [496/590], Loss: 10.8120\n",
      "Epoch [4/25], Step [500/590], Loss: 17.8870\n",
      "Epoch [4/25], Step [504/590], Loss: 5.4569\n",
      "Epoch [4/25], Step [508/590], Loss: 6.4002\n",
      "Epoch [4/25], Step [512/590], Loss: 5.0246\n",
      "Epoch [4/25], Step [516/590], Loss: 11.0510\n",
      "Epoch [4/25], Step [520/590], Loss: 8.2906\n",
      "Epoch [4/25], Step [524/590], Loss: 5.5962\n",
      "Epoch [4/25], Step [528/590], Loss: 12.1214\n",
      "Epoch [4/25], Step [532/590], Loss: 5.4143\n",
      "Epoch [4/25], Step [536/590], Loss: 6.6691\n",
      "Epoch [4/25], Step [540/590], Loss: 11.7599\n",
      "Epoch [4/25], Step [544/590], Loss: 4.2995\n",
      "Epoch [4/25], Step [548/590], Loss: 5.1377\n",
      "Epoch [4/25], Step [552/590], Loss: 6.7846\n",
      "Epoch [4/25], Step [556/590], Loss: 11.3952\n",
      "Epoch [4/25], Step [560/590], Loss: 10.6068\n",
      "Epoch [4/25], Step [564/590], Loss: 15.8064\n",
      "Epoch [4/25], Step [568/590], Loss: 8.7861\n",
      "Epoch [4/25], Step [572/590], Loss: 12.9969\n",
      "Epoch [4/25], Step [576/590], Loss: 5.9465\n",
      "Epoch [4/25], Step [580/590], Loss: 11.2010\n",
      "Epoch [4/25], Step [584/590], Loss: 8.5079\n",
      "Epoch [4/25], Step [588/590], Loss: 4.8396\n",
      "epoch 4\n",
      "Epoch [5/25], Step [4/590], Loss: 4.6035\n",
      "Epoch [5/25], Step [8/590], Loss: 2.7402\n",
      "Epoch [5/25], Step [12/590], Loss: 2.8890\n",
      "Epoch [5/25], Step [16/590], Loss: 5.2308\n",
      "Epoch [5/25], Step [20/590], Loss: 4.1663\n",
      "Epoch [5/25], Step [24/590], Loss: 3.9085\n",
      "Epoch [5/25], Step [28/590], Loss: 7.9081\n",
      "Epoch [5/25], Step [32/590], Loss: 8.8821\n",
      "Epoch [5/25], Step [36/590], Loss: 7.0098\n",
      "Epoch [5/25], Step [40/590], Loss: 6.4950\n",
      "Epoch [5/25], Step [44/590], Loss: 8.2862\n",
      "Epoch [5/25], Step [48/590], Loss: 7.0121\n",
      "Epoch [5/25], Step [52/590], Loss: 6.5339\n",
      "Epoch [5/25], Step [56/590], Loss: 17.9870\n",
      "Epoch [5/25], Step [60/590], Loss: 3.3818\n",
      "Epoch [5/25], Step [64/590], Loss: 7.7973\n",
      "Epoch [5/25], Step [68/590], Loss: 5.7819\n",
      "Epoch [5/25], Step [72/590], Loss: 6.2372\n",
      "Epoch [5/25], Step [76/590], Loss: 2.0541\n",
      "Epoch [5/25], Step [80/590], Loss: 5.4452\n",
      "Epoch [5/25], Step [84/590], Loss: 4.2407\n",
      "Epoch [5/25], Step [88/590], Loss: 2.2773\n",
      "Epoch [5/25], Step [92/590], Loss: 2.0830\n",
      "Epoch [5/25], Step [96/590], Loss: 5.0422\n",
      "Epoch [5/25], Step [100/590], Loss: 4.5507\n",
      "Epoch [5/25], Step [104/590], Loss: 7.0022\n",
      "Epoch [5/25], Step [108/590], Loss: 11.1177\n",
      "Epoch [5/25], Step [112/590], Loss: 3.7046\n",
      "Epoch [5/25], Step [116/590], Loss: 6.1293\n",
      "Epoch [5/25], Step [120/590], Loss: 4.5397\n",
      "Epoch [5/25], Step [124/590], Loss: 3.9235\n",
      "Epoch [5/25], Step [128/590], Loss: 2.3216\n",
      "Epoch [5/25], Step [132/590], Loss: 7.3354\n",
      "Epoch [5/25], Step [136/590], Loss: 1.2099\n",
      "Epoch [5/25], Step [140/590], Loss: 4.2325\n",
      "Epoch [5/25], Step [144/590], Loss: 1.5181\n",
      "Epoch [5/25], Step [148/590], Loss: 5.0582\n",
      "Epoch [5/25], Step [152/590], Loss: 3.0077\n",
      "Epoch [5/25], Step [156/590], Loss: 3.1050\n",
      "Epoch [5/25], Step [160/590], Loss: 3.4734\n",
      "Epoch [5/25], Step [164/590], Loss: 5.2858\n",
      "Epoch [5/25], Step [168/590], Loss: 10.5966\n",
      "Epoch [5/25], Step [172/590], Loss: 3.9716\n",
      "Epoch [5/25], Step [176/590], Loss: 4.1916\n",
      "Epoch [5/25], Step [180/590], Loss: 9.6692\n",
      "Epoch [5/25], Step [184/590], Loss: 4.6191\n",
      "Epoch [5/25], Step [188/590], Loss: 3.4079\n",
      "Epoch [5/25], Step [192/590], Loss: 2.7491\n",
      "Epoch [5/25], Step [196/590], Loss: 5.7226\n",
      "Epoch [5/25], Step [200/590], Loss: 8.2019\n",
      "Epoch [5/25], Step [204/590], Loss: 5.0292\n",
      "Epoch [5/25], Step [208/590], Loss: 5.7577\n",
      "Epoch [5/25], Step [212/590], Loss: 5.6974\n",
      "Epoch [5/25], Step [216/590], Loss: 2.6457\n",
      "Epoch [5/25], Step [220/590], Loss: 6.0836\n",
      "Epoch [5/25], Step [224/590], Loss: 4.5887\n",
      "Epoch [5/25], Step [228/590], Loss: 3.0445\n",
      "Epoch [5/25], Step [232/590], Loss: 7.0441\n",
      "Epoch [5/25], Step [236/590], Loss: 3.3693\n",
      "Epoch [5/25], Step [240/590], Loss: 1.9889\n",
      "Epoch [5/25], Step [244/590], Loss: 3.0740\n",
      "Epoch [5/25], Step [248/590], Loss: 8.3842\n",
      "Epoch [5/25], Step [252/590], Loss: 0.5422\n",
      "Epoch [5/25], Step [256/590], Loss: 6.7590\n",
      "Epoch [5/25], Step [260/590], Loss: 6.1927\n",
      "Epoch [5/25], Step [264/590], Loss: 1.8942\n",
      "Epoch [5/25], Step [268/590], Loss: 6.5403\n",
      "Epoch [5/25], Step [272/590], Loss: 5.8640\n",
      "Epoch [5/25], Step [276/590], Loss: 6.9679\n",
      "Epoch [5/25], Step [280/590], Loss: 12.5895\n",
      "Epoch [5/25], Step [284/590], Loss: 2.2974\n",
      "Epoch [5/25], Step [288/590], Loss: 4.0010\n",
      "Epoch [5/25], Step [292/590], Loss: 3.0518\n",
      "Epoch [5/25], Step [296/590], Loss: 2.1075\n",
      "Epoch [5/25], Step [300/590], Loss: 1.6596\n",
      "Epoch [5/25], Step [304/590], Loss: 7.8298\n",
      "Epoch [5/25], Step [308/590], Loss: 9.2496\n",
      "Epoch [5/25], Step [312/590], Loss: 4.0224\n",
      "Epoch [5/25], Step [316/590], Loss: 7.3038\n",
      "Epoch [5/25], Step [320/590], Loss: 3.4450\n",
      "Epoch [5/25], Step [324/590], Loss: 1.7547\n",
      "Epoch [5/25], Step [328/590], Loss: 5.3720\n",
      "Epoch [5/25], Step [332/590], Loss: 9.9978\n",
      "Epoch [5/25], Step [336/590], Loss: 3.5913\n",
      "Epoch [5/25], Step [340/590], Loss: 10.2270\n",
      "Epoch [5/25], Step [344/590], Loss: 7.7326\n",
      "Epoch [5/25], Step [348/590], Loss: 2.6442\n",
      "Epoch [5/25], Step [352/590], Loss: 4.9769\n",
      "Epoch [5/25], Step [356/590], Loss: 3.8434\n",
      "Epoch [5/25], Step [360/590], Loss: 9.7362\n",
      "Epoch [5/25], Step [364/590], Loss: 7.9094\n",
      "Epoch [5/25], Step [368/590], Loss: 4.4838\n",
      "Epoch [5/25], Step [372/590], Loss: 2.1318\n",
      "Epoch [5/25], Step [376/590], Loss: 4.9580\n",
      "Epoch [5/25], Step [380/590], Loss: 11.0442\n",
      "Epoch [5/25], Step [384/590], Loss: 4.2743\n",
      "Epoch [5/25], Step [388/590], Loss: 8.6157\n",
      "Epoch [5/25], Step [392/590], Loss: 5.8783\n",
      "Epoch [5/25], Step [396/590], Loss: 5.8215\n",
      "Epoch [5/25], Step [400/590], Loss: 3.9382\n",
      "Epoch [5/25], Step [404/590], Loss: 4.9079\n",
      "Epoch [5/25], Step [408/590], Loss: 4.8143\n",
      "Epoch [5/25], Step [412/590], Loss: 5.2159\n",
      "Epoch [5/25], Step [416/590], Loss: 2.3168\n",
      "Epoch [5/25], Step [420/590], Loss: 2.9895\n",
      "Epoch [5/25], Step [424/590], Loss: 15.7856\n",
      "Epoch [5/25], Step [428/590], Loss: 3.4867\n",
      "Epoch [5/25], Step [432/590], Loss: 14.1319\n",
      "Epoch [5/25], Step [436/590], Loss: 4.6468\n",
      "Epoch [5/25], Step [440/590], Loss: 6.5394\n",
      "Epoch [5/25], Step [444/590], Loss: 1.9963\n",
      "Epoch [5/25], Step [448/590], Loss: 5.2758\n",
      "Epoch [5/25], Step [452/590], Loss: 8.3135\n",
      "Epoch [5/25], Step [456/590], Loss: 6.2665\n",
      "Epoch [5/25], Step [460/590], Loss: 1.3137\n",
      "Epoch [5/25], Step [464/590], Loss: 2.0149\n",
      "Epoch [5/25], Step [468/590], Loss: 5.0471\n",
      "Epoch [5/25], Step [472/590], Loss: 3.4075\n",
      "Epoch [5/25], Step [476/590], Loss: 5.9529\n",
      "Epoch [5/25], Step [480/590], Loss: 3.1374\n",
      "Epoch [5/25], Step [484/590], Loss: 10.1328\n",
      "Epoch [5/25], Step [488/590], Loss: 6.3386\n",
      "Epoch [5/25], Step [492/590], Loss: 1.9607\n",
      "Epoch [5/25], Step [496/590], Loss: 3.6726\n",
      "Epoch [5/25], Step [500/590], Loss: 4.3076\n",
      "Epoch [5/25], Step [504/590], Loss: 7.3493\n",
      "Epoch [5/25], Step [508/590], Loss: 2.1093\n",
      "Epoch [5/25], Step [512/590], Loss: 5.0015\n",
      "Epoch [5/25], Step [516/590], Loss: 1.9386\n",
      "Epoch [5/25], Step [520/590], Loss: 5.5409\n",
      "Epoch [5/25], Step [524/590], Loss: 2.3527\n",
      "Epoch [5/25], Step [528/590], Loss: 7.1204\n",
      "Epoch [5/25], Step [532/590], Loss: 5.4676\n",
      "Epoch [5/25], Step [536/590], Loss: 4.2868\n",
      "Epoch [5/25], Step [540/590], Loss: 10.7105\n",
      "Epoch [5/25], Step [544/590], Loss: 2.5301\n",
      "Epoch [5/25], Step [548/590], Loss: 3.4816\n",
      "Epoch [5/25], Step [552/590], Loss: 4.1408\n",
      "Epoch [5/25], Step [556/590], Loss: 4.8840\n",
      "Epoch [5/25], Step [560/590], Loss: 6.4003\n",
      "Epoch [5/25], Step [564/590], Loss: 2.9422\n",
      "Epoch [5/25], Step [568/590], Loss: 2.9634\n",
      "Epoch [5/25], Step [572/590], Loss: 3.5613\n",
      "Epoch [5/25], Step [576/590], Loss: 1.9267\n",
      "Epoch [5/25], Step [580/590], Loss: 5.5529\n",
      "Epoch [5/25], Step [584/590], Loss: 1.6779\n",
      "Epoch [5/25], Step [588/590], Loss: 3.1333\n",
      "epoch 5\n",
      "Epoch [6/25], Step [4/590], Loss: 0.2650\n",
      "Epoch [6/25], Step [8/590], Loss: 1.1430\n",
      "Epoch [6/25], Step [12/590], Loss: 2.0623\n",
      "Epoch [6/25], Step [16/590], Loss: 1.3787\n",
      "Epoch [6/25], Step [20/590], Loss: 1.0671\n",
      "Epoch [6/25], Step [24/590], Loss: 1.2514\n",
      "Epoch [6/25], Step [28/590], Loss: 2.7420\n",
      "Epoch [6/25], Step [32/590], Loss: 1.6928\n",
      "Epoch [6/25], Step [36/590], Loss: 1.1531\n",
      "Epoch [6/25], Step [40/590], Loss: 0.4652\n",
      "Epoch [6/25], Step [44/590], Loss: 1.9101\n",
      "Epoch [6/25], Step [48/590], Loss: 6.7814\n",
      "Epoch [6/25], Step [52/590], Loss: 1.2407\n",
      "Epoch [6/25], Step [56/590], Loss: 1.9479\n",
      "Epoch [6/25], Step [60/590], Loss: 3.2198\n",
      "Epoch [6/25], Step [64/590], Loss: 1.9145\n",
      "Epoch [6/25], Step [68/590], Loss: 3.7253\n",
      "Epoch [6/25], Step [72/590], Loss: 2.1624\n",
      "Epoch [6/25], Step [76/590], Loss: 2.2986\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Step [80/590], Loss: 4.1793\n",
      "Epoch [6/25], Step [84/590], Loss: 5.1372\n",
      "Epoch [6/25], Step [88/590], Loss: 1.9301\n",
      "Epoch [6/25], Step [92/590], Loss: 2.0594\n",
      "Epoch [6/25], Step [96/590], Loss: 7.8133\n",
      "Epoch [6/25], Step [100/590], Loss: 2.9460\n",
      "Epoch [6/25], Step [104/590], Loss: 0.3954\n",
      "Epoch [6/25], Step [108/590], Loss: 7.4262\n",
      "Epoch [6/25], Step [112/590], Loss: 3.1251\n",
      "Epoch [6/25], Step [116/590], Loss: 0.9147\n",
      "Epoch [6/25], Step [120/590], Loss: 7.8174\n",
      "Epoch [6/25], Step [124/590], Loss: 3.9351\n",
      "Epoch [6/25], Step [128/590], Loss: 2.3984\n",
      "Epoch [6/25], Step [132/590], Loss: 4.1068\n",
      "Epoch [6/25], Step [136/590], Loss: 2.4294\n",
      "Epoch [6/25], Step [140/590], Loss: 1.2221\n",
      "Epoch [6/25], Step [144/590], Loss: 1.8034\n",
      "Epoch [6/25], Step [148/590], Loss: 1.0052\n",
      "Epoch [6/25], Step [152/590], Loss: 5.3887\n",
      "Epoch [6/25], Step [156/590], Loss: 3.2008\n",
      "Epoch [6/25], Step [160/590], Loss: 2.0818\n",
      "Epoch [6/25], Step [164/590], Loss: 6.9533\n",
      "Epoch [6/25], Step [168/590], Loss: 7.8026\n",
      "Epoch [6/25], Step [172/590], Loss: 5.8517\n",
      "Epoch [6/25], Step [176/590], Loss: 7.3978\n",
      "Epoch [6/25], Step [180/590], Loss: 0.7409\n",
      "Epoch [6/25], Step [184/590], Loss: 0.9148\n",
      "Epoch [6/25], Step [188/590], Loss: 3.1054\n",
      "Epoch [6/25], Step [192/590], Loss: 6.2159\n",
      "Epoch [6/25], Step [196/590], Loss: 0.6155\n",
      "Epoch [6/25], Step [200/590], Loss: 6.3432\n",
      "Epoch [6/25], Step [204/590], Loss: 5.0231\n",
      "Epoch [6/25], Step [208/590], Loss: 2.5133\n",
      "Epoch [6/25], Step [212/590], Loss: 2.6815\n",
      "Epoch [6/25], Step [216/590], Loss: 0.6169\n",
      "Epoch [6/25], Step [220/590], Loss: 3.8874\n",
      "Epoch [6/25], Step [224/590], Loss: 5.2695\n",
      "Epoch [6/25], Step [228/590], Loss: 1.8878\n",
      "Epoch [6/25], Step [232/590], Loss: 10.0800\n",
      "Epoch [6/25], Step [236/590], Loss: 2.2095\n",
      "Epoch [6/25], Step [240/590], Loss: 9.0182\n",
      "Epoch [6/25], Step [244/590], Loss: 8.3483\n",
      "Epoch [6/25], Step [248/590], Loss: 5.4338\n",
      "Epoch [6/25], Step [252/590], Loss: 6.8657\n",
      "Epoch [6/25], Step [256/590], Loss: 4.9454\n",
      "Epoch [6/25], Step [260/590], Loss: 1.4100\n",
      "Epoch [6/25], Step [264/590], Loss: 3.5856\n",
      "Epoch [6/25], Step [268/590], Loss: 1.0174\n",
      "Epoch [6/25], Step [272/590], Loss: 2.0304\n",
      "Epoch [6/25], Step [276/590], Loss: 2.0248\n",
      "Epoch [6/25], Step [280/590], Loss: 1.9875\n",
      "Epoch [6/25], Step [284/590], Loss: 8.1937\n",
      "Epoch [6/25], Step [288/590], Loss: 2.5189\n",
      "Epoch [6/25], Step [292/590], Loss: 1.5705\n",
      "Epoch [6/25], Step [296/590], Loss: 1.5450\n",
      "Epoch [6/25], Step [300/590], Loss: 2.4726\n",
      "Epoch [6/25], Step [304/590], Loss: 0.9891\n",
      "Epoch [6/25], Step [308/590], Loss: 0.6517\n",
      "Epoch [6/25], Step [312/590], Loss: 1.3456\n",
      "Epoch [6/25], Step [316/590], Loss: 8.4017\n",
      "Epoch [6/25], Step [320/590], Loss: 6.9558\n",
      "Epoch [6/25], Step [324/590], Loss: 4.8832\n",
      "Epoch [6/25], Step [328/590], Loss: 4.5302\n",
      "Epoch [6/25], Step [332/590], Loss: 2.1379\n",
      "Epoch [6/25], Step [336/590], Loss: 6.2287\n",
      "Epoch [6/25], Step [340/590], Loss: 4.7050\n",
      "Epoch [6/25], Step [344/590], Loss: 7.6581\n",
      "Epoch [6/25], Step [348/590], Loss: 7.7794\n",
      "Epoch [6/25], Step [352/590], Loss: 4.2889\n",
      "Epoch [6/25], Step [356/590], Loss: 1.2506\n",
      "Epoch [6/25], Step [360/590], Loss: 1.2841\n",
      "Epoch [6/25], Step [364/590], Loss: 3.0993\n",
      "Epoch [6/25], Step [368/590], Loss: 2.7915\n",
      "Epoch [6/25], Step [372/590], Loss: 1.0596\n",
      "Epoch [6/25], Step [376/590], Loss: 0.6159\n",
      "Epoch [6/25], Step [380/590], Loss: 1.3022\n",
      "Epoch [6/25], Step [384/590], Loss: 13.7172\n",
      "Epoch [6/25], Step [388/590], Loss: 2.3963\n",
      "Epoch [6/25], Step [392/590], Loss: 3.4397\n",
      "Epoch [6/25], Step [396/590], Loss: 1.8868\n",
      "Epoch [6/25], Step [400/590], Loss: 3.9245\n",
      "Epoch [6/25], Step [404/590], Loss: 5.9399\n",
      "Epoch [6/25], Step [408/590], Loss: 1.4069\n",
      "Epoch [6/25], Step [412/590], Loss: 1.7722\n",
      "Epoch [6/25], Step [416/590], Loss: 3.4606\n",
      "Epoch [6/25], Step [420/590], Loss: 3.9563\n",
      "Epoch [6/25], Step [424/590], Loss: 1.3571\n",
      "Epoch [6/25], Step [428/590], Loss: 0.1424\n",
      "Epoch [6/25], Step [432/590], Loss: 1.7267\n",
      "Epoch [6/25], Step [436/590], Loss: 0.8617\n",
      "Epoch [6/25], Step [440/590], Loss: 7.2860\n",
      "Epoch [6/25], Step [444/590], Loss: 0.2446\n",
      "Epoch [6/25], Step [448/590], Loss: 2.0302\n",
      "Epoch [6/25], Step [452/590], Loss: 1.3288\n",
      "Epoch [6/25], Step [456/590], Loss: 3.6121\n",
      "Epoch [6/25], Step [460/590], Loss: 2.8795\n",
      "Epoch [6/25], Step [464/590], Loss: 3.9266\n",
      "Epoch [6/25], Step [468/590], Loss: 2.4784\n",
      "Epoch [6/25], Step [472/590], Loss: 3.6325\n",
      "Epoch [6/25], Step [476/590], Loss: 1.4005\n",
      "Epoch [6/25], Step [480/590], Loss: 0.9821\n",
      "Epoch [6/25], Step [484/590], Loss: 5.1925\n",
      "Epoch [6/25], Step [488/590], Loss: 1.6527\n",
      "Epoch [6/25], Step [492/590], Loss: 0.9507\n",
      "Epoch [6/25], Step [496/590], Loss: 5.8329\n",
      "Epoch [6/25], Step [500/590], Loss: 4.8727\n",
      "Epoch [6/25], Step [504/590], Loss: 3.5056\n",
      "Epoch [6/25], Step [508/590], Loss: 1.1240\n",
      "Epoch [6/25], Step [512/590], Loss: 0.8854\n",
      "Epoch [6/25], Step [516/590], Loss: 5.4868\n",
      "Epoch [6/25], Step [520/590], Loss: 3.6704\n",
      "Epoch [6/25], Step [524/590], Loss: 0.6225\n",
      "Epoch [6/25], Step [528/590], Loss: 2.2941\n",
      "Epoch [6/25], Step [532/590], Loss: 4.9239\n",
      "Epoch [6/25], Step [536/590], Loss: 4.8442\n",
      "Epoch [6/25], Step [540/590], Loss: 4.5933\n",
      "Epoch [6/25], Step [544/590], Loss: 1.1064\n",
      "Epoch [6/25], Step [548/590], Loss: 1.1246\n",
      "Epoch [6/25], Step [552/590], Loss: 2.5853\n",
      "Epoch [6/25], Step [556/590], Loss: 1.4650\n",
      "Epoch [6/25], Step [560/590], Loss: 5.3035\n",
      "Epoch [6/25], Step [564/590], Loss: 1.8551\n",
      "Epoch [6/25], Step [568/590], Loss: 3.1821\n",
      "Epoch [6/25], Step [572/590], Loss: 2.6232\n",
      "Epoch [6/25], Step [576/590], Loss: 6.6533\n",
      "Epoch [6/25], Step [580/590], Loss: 5.1615\n",
      "Epoch [6/25], Step [584/590], Loss: 2.0709\n",
      "Epoch [6/25], Step [588/590], Loss: 3.0681\n",
      "epoch 6\n",
      "Epoch [7/25], Step [4/590], Loss: 0.8475\n",
      "Epoch [7/25], Step [8/590], Loss: 2.8715\n",
      "Epoch [7/25], Step [12/590], Loss: 3.3469\n",
      "Epoch [7/25], Step [16/590], Loss: 2.3154\n",
      "Epoch [7/25], Step [20/590], Loss: 0.4908\n",
      "Epoch [7/25], Step [24/590], Loss: 2.1051\n",
      "Epoch [7/25], Step [28/590], Loss: 4.6054\n",
      "Epoch [7/25], Step [32/590], Loss: 1.1854\n",
      "Epoch [7/25], Step [36/590], Loss: 5.9718\n",
      "Epoch [7/25], Step [40/590], Loss: 1.1983\n",
      "Epoch [7/25], Step [44/590], Loss: 2.0870\n",
      "Epoch [7/25], Step [48/590], Loss: 0.1582\n",
      "Epoch [7/25], Step [52/590], Loss: 8.4928\n",
      "Epoch [7/25], Step [56/590], Loss: 1.2296\n",
      "Epoch [7/25], Step [60/590], Loss: 1.0583\n",
      "Epoch [7/25], Step [64/590], Loss: 1.9808\n",
      "Epoch [7/25], Step [68/590], Loss: 0.4233\n",
      "Epoch [7/25], Step [72/590], Loss: 8.1674\n",
      "Epoch [7/25], Step [76/590], Loss: 2.7585\n",
      "Epoch [7/25], Step [80/590], Loss: 2.2943\n",
      "Epoch [7/25], Step [84/590], Loss: 1.6027\n",
      "Epoch [7/25], Step [88/590], Loss: 0.4144\n",
      "Epoch [7/25], Step [92/590], Loss: 1.4666\n",
      "Epoch [7/25], Step [96/590], Loss: 13.0657\n",
      "Epoch [7/25], Step [100/590], Loss: 1.5291\n",
      "Epoch [7/25], Step [104/590], Loss: 1.2629\n",
      "Epoch [7/25], Step [108/590], Loss: 1.3330\n",
      "Epoch [7/25], Step [112/590], Loss: 0.8004\n",
      "Epoch [7/25], Step [116/590], Loss: 2.9082\n",
      "Epoch [7/25], Step [120/590], Loss: 2.8460\n",
      "Epoch [7/25], Step [124/590], Loss: 6.5144\n",
      "Epoch [7/25], Step [128/590], Loss: 0.6346\n",
      "Epoch [7/25], Step [132/590], Loss: 1.2353\n",
      "Epoch [7/25], Step [136/590], Loss: 0.2815\n",
      "Epoch [7/25], Step [140/590], Loss: 2.9472\n",
      "Epoch [7/25], Step [144/590], Loss: 2.8035\n",
      "Epoch [7/25], Step [148/590], Loss: 1.3690\n",
      "Epoch [7/25], Step [152/590], Loss: 0.6036\n",
      "Epoch [7/25], Step [156/590], Loss: 2.5760\n",
      "Epoch [7/25], Step [160/590], Loss: 0.9851\n",
      "Epoch [7/25], Step [164/590], Loss: 2.0328\n",
      "Epoch [7/25], Step [168/590], Loss: 7.0211\n",
      "Epoch [7/25], Step [172/590], Loss: 1.5183\n",
      "Epoch [7/25], Step [176/590], Loss: 1.5839\n",
      "Epoch [7/25], Step [180/590], Loss: 4.8093\n",
      "Epoch [7/25], Step [184/590], Loss: 1.2681\n",
      "Epoch [7/25], Step [188/590], Loss: 1.3088\n",
      "Epoch [7/25], Step [192/590], Loss: 4.3336\n",
      "Epoch [7/25], Step [196/590], Loss: 2.9219\n",
      "Epoch [7/25], Step [200/590], Loss: 6.9286\n",
      "Epoch [7/25], Step [204/590], Loss: 5.7832\n",
      "Epoch [7/25], Step [208/590], Loss: 1.4317\n",
      "Epoch [7/25], Step [212/590], Loss: 0.7680\n",
      "Epoch [7/25], Step [216/590], Loss: 1.2337\n",
      "Epoch [7/25], Step [220/590], Loss: 2.7438\n",
      "Epoch [7/25], Step [224/590], Loss: 1.6894\n",
      "Epoch [7/25], Step [228/590], Loss: 1.4463\n",
      "Epoch [7/25], Step [232/590], Loss: 6.0967\n",
      "Epoch [7/25], Step [236/590], Loss: 4.2687\n",
      "Epoch [7/25], Step [240/590], Loss: 0.2510\n",
      "Epoch [7/25], Step [244/590], Loss: 1.9634\n",
      "Epoch [7/25], Step [248/590], Loss: 3.1706\n",
      "Epoch [7/25], Step [252/590], Loss: 7.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Step [256/590], Loss: 5.8094\n",
      "Epoch [7/25], Step [260/590], Loss: 0.9281\n",
      "Epoch [7/25], Step [264/590], Loss: 1.2714\n",
      "Epoch [7/25], Step [268/590], Loss: 1.5316\n",
      "Epoch [7/25], Step [272/590], Loss: 5.5560\n",
      "Epoch [7/25], Step [276/590], Loss: 1.3080\n",
      "Epoch [7/25], Step [280/590], Loss: 5.0390\n",
      "Epoch [7/25], Step [284/590], Loss: 2.1937\n",
      "Epoch [7/25], Step [288/590], Loss: 1.1670\n",
      "Epoch [7/25], Step [292/590], Loss: 0.6418\n",
      "Epoch [7/25], Step [296/590], Loss: 1.4169\n",
      "Epoch [7/25], Step [300/590], Loss: 1.8254\n",
      "Epoch [7/25], Step [304/590], Loss: 3.6098\n",
      "Epoch [7/25], Step [308/590], Loss: 0.5035\n",
      "Epoch [7/25], Step [312/590], Loss: 6.0277\n",
      "Epoch [7/25], Step [316/590], Loss: 0.5947\n",
      "Epoch [7/25], Step [320/590], Loss: 1.1794\n",
      "Epoch [7/25], Step [324/590], Loss: 2.6229\n",
      "Epoch [7/25], Step [328/590], Loss: 2.3344\n",
      "Epoch [7/25], Step [332/590], Loss: 1.9175\n",
      "Epoch [7/25], Step [336/590], Loss: 3.9507\n",
      "Epoch [7/25], Step [340/590], Loss: 0.2594\n",
      "Epoch [7/25], Step [344/590], Loss: 1.9381\n",
      "Epoch [7/25], Step [348/590], Loss: 2.2020\n",
      "Epoch [7/25], Step [352/590], Loss: 1.1355\n",
      "Epoch [7/25], Step [356/590], Loss: 2.2845\n",
      "Epoch [7/25], Step [360/590], Loss: 1.0434\n",
      "Epoch [7/25], Step [364/590], Loss: 1.3705\n",
      "Epoch [7/25], Step [368/590], Loss: 1.8259\n",
      "Epoch [7/25], Step [372/590], Loss: 0.7838\n",
      "Epoch [7/25], Step [376/590], Loss: 1.8364\n",
      "Epoch [7/25], Step [380/590], Loss: 2.7965\n",
      "Epoch [7/25], Step [384/590], Loss: 2.6344\n",
      "Epoch [7/25], Step [388/590], Loss: 4.4333\n",
      "Epoch [7/25], Step [392/590], Loss: 3.8731\n",
      "Epoch [7/25], Step [396/590], Loss: 0.8375\n",
      "Epoch [7/25], Step [400/590], Loss: 1.7798\n",
      "Epoch [7/25], Step [404/590], Loss: 2.5314\n",
      "Epoch [7/25], Step [408/590], Loss: 7.2065\n",
      "Epoch [7/25], Step [412/590], Loss: 2.4161\n",
      "Epoch [7/25], Step [416/590], Loss: 2.7095\n",
      "Epoch [7/25], Step [420/590], Loss: 3.5543\n",
      "Epoch [7/25], Step [424/590], Loss: 0.3322\n",
      "Epoch [7/25], Step [428/590], Loss: 2.6020\n",
      "Epoch [7/25], Step [432/590], Loss: 5.2785\n",
      "Epoch [7/25], Step [436/590], Loss: 1.6738\n",
      "Epoch [7/25], Step [440/590], Loss: 4.2377\n",
      "Epoch [7/25], Step [444/590], Loss: 4.5877\n",
      "Epoch [7/25], Step [448/590], Loss: 1.2265\n",
      "Epoch [7/25], Step [452/590], Loss: 2.1283\n",
      "Epoch [7/25], Step [456/590], Loss: 2.1570\n",
      "Epoch [7/25], Step [460/590], Loss: 2.0426\n",
      "Epoch [7/25], Step [464/590], Loss: 1.3298\n",
      "Epoch [7/25], Step [468/590], Loss: 6.4831\n",
      "Epoch [7/25], Step [472/590], Loss: 1.6297\n",
      "Epoch [7/25], Step [476/590], Loss: 0.9726\n",
      "Epoch [7/25], Step [480/590], Loss: 6.7482\n",
      "Epoch [7/25], Step [484/590], Loss: 6.3565\n",
      "Epoch [7/25], Step [488/590], Loss: 1.1480\n",
      "Epoch [7/25], Step [492/590], Loss: 0.9525\n",
      "Epoch [7/25], Step [496/590], Loss: 9.8880\n",
      "Epoch [7/25], Step [500/590], Loss: 3.5992\n",
      "Epoch [7/25], Step [504/590], Loss: 5.7656\n",
      "Epoch [7/25], Step [508/590], Loss: 6.6037\n",
      "Epoch [7/25], Step [512/590], Loss: 12.0899\n",
      "Epoch [7/25], Step [516/590], Loss: 3.2045\n",
      "Epoch [7/25], Step [520/590], Loss: 2.9339\n",
      "Epoch [7/25], Step [524/590], Loss: 4.2882\n",
      "Epoch [7/25], Step [528/590], Loss: 9.8740\n",
      "Epoch [7/25], Step [532/590], Loss: 4.3958\n",
      "Epoch [7/25], Step [536/590], Loss: 9.5917\n",
      "Epoch [7/25], Step [540/590], Loss: 6.2582\n",
      "Epoch [7/25], Step [544/590], Loss: 7.0389\n",
      "Epoch [7/25], Step [548/590], Loss: 2.0451\n",
      "Epoch [7/25], Step [552/590], Loss: 2.8265\n",
      "Epoch [7/25], Step [556/590], Loss: 5.4764\n",
      "Epoch [7/25], Step [560/590], Loss: 4.7632\n",
      "Epoch [7/25], Step [564/590], Loss: 8.3773\n",
      "Epoch [7/25], Step [568/590], Loss: 1.3595\n",
      "Epoch [7/25], Step [572/590], Loss: 2.2963\n",
      "Epoch [7/25], Step [576/590], Loss: 0.6260\n",
      "Epoch [7/25], Step [580/590], Loss: 4.8460\n",
      "Epoch [7/25], Step [584/590], Loss: 4.9527\n",
      "Epoch [7/25], Step [588/590], Loss: 6.8612\n",
      "epoch 7\n",
      "Epoch [8/25], Step [4/590], Loss: 5.5268\n",
      "Epoch [8/25], Step [8/590], Loss: 1.8148\n",
      "Epoch [8/25], Step [12/590], Loss: 3.6917\n",
      "Epoch [8/25], Step [16/590], Loss: 1.9765\n",
      "Epoch [8/25], Step [20/590], Loss: 7.8790\n",
      "Epoch [8/25], Step [24/590], Loss: 0.7216\n",
      "Epoch [8/25], Step [28/590], Loss: 2.1148\n",
      "Epoch [8/25], Step [32/590], Loss: 3.5892\n",
      "Epoch [8/25], Step [36/590], Loss: 2.0815\n",
      "Epoch [8/25], Step [40/590], Loss: 0.9180\n",
      "Epoch [8/25], Step [44/590], Loss: 0.6003\n",
      "Epoch [8/25], Step [48/590], Loss: 0.8869\n",
      "Epoch [8/25], Step [52/590], Loss: 0.4223\n",
      "Epoch [8/25], Step [56/590], Loss: 1.7853\n",
      "Epoch [8/25], Step [60/590], Loss: 0.7031\n",
      "Epoch [8/25], Step [64/590], Loss: 2.9585\n",
      "Epoch [8/25], Step [68/590], Loss: 2.2570\n",
      "Epoch [8/25], Step [72/590], Loss: 4.0267\n",
      "Epoch [8/25], Step [76/590], Loss: 0.6259\n",
      "Epoch [8/25], Step [80/590], Loss: 0.5061\n",
      "Epoch [8/25], Step [84/590], Loss: 1.1267\n",
      "Epoch [8/25], Step [88/590], Loss: 0.6416\n",
      "Epoch [8/25], Step [92/590], Loss: 0.2257\n",
      "Epoch [8/25], Step [96/590], Loss: 4.6377\n",
      "Epoch [8/25], Step [100/590], Loss: 1.3583\n",
      "Epoch [8/25], Step [104/590], Loss: 2.6477\n",
      "Epoch [8/25], Step [108/590], Loss: 3.6220\n",
      "Epoch [8/25], Step [112/590], Loss: 1.4803\n",
      "Epoch [8/25], Step [116/590], Loss: 0.3766\n",
      "Epoch [8/25], Step [120/590], Loss: 16.3886\n",
      "Epoch [8/25], Step [124/590], Loss: 3.3841\n",
      "Epoch [8/25], Step [128/590], Loss: 2.7176\n",
      "Epoch [8/25], Step [132/590], Loss: 0.7641\n",
      "Epoch [8/25], Step [136/590], Loss: 0.4807\n",
      "Epoch [8/25], Step [140/590], Loss: 2.6871\n",
      "Epoch [8/25], Step [144/590], Loss: 4.0652\n",
      "Epoch [8/25], Step [148/590], Loss: 0.2371\n",
      "Epoch [8/25], Step [152/590], Loss: 3.2276\n",
      "Epoch [8/25], Step [156/590], Loss: 1.7978\n",
      "Epoch [8/25], Step [160/590], Loss: 3.5878\n",
      "Epoch [8/25], Step [164/590], Loss: 8.7887\n",
      "Epoch [8/25], Step [168/590], Loss: 3.3248\n",
      "Epoch [8/25], Step [172/590], Loss: 0.2474\n",
      "Epoch [8/25], Step [176/590], Loss: 0.9175\n",
      "Epoch [8/25], Step [180/590], Loss: 2.3353\n",
      "Epoch [8/25], Step [184/590], Loss: 1.2428\n",
      "Epoch [8/25], Step [188/590], Loss: 0.9228\n",
      "Epoch [8/25], Step [192/590], Loss: 2.1034\n",
      "Epoch [8/25], Step [196/590], Loss: 9.2077\n",
      "Epoch [8/25], Step [200/590], Loss: 0.9557\n",
      "Epoch [8/25], Step [204/590], Loss: 8.0425\n",
      "Epoch [8/25], Step [208/590], Loss: 3.7742\n",
      "Epoch [8/25], Step [212/590], Loss: 4.2097\n",
      "Epoch [8/25], Step [216/590], Loss: 0.8130\n",
      "Epoch [8/25], Step [220/590], Loss: 6.6667\n",
      "Epoch [8/25], Step [224/590], Loss: 0.6141\n",
      "Epoch [8/25], Step [228/590], Loss: 2.7014\n",
      "Epoch [8/25], Step [232/590], Loss: 15.3754\n",
      "Epoch [8/25], Step [236/590], Loss: 2.2516\n",
      "Epoch [8/25], Step [240/590], Loss: 0.6435\n",
      "Epoch [8/25], Step [244/590], Loss: 5.2167\n",
      "Epoch [8/25], Step [248/590], Loss: 3.7915\n",
      "Epoch [8/25], Step [252/590], Loss: 7.2865\n",
      "Epoch [8/25], Step [256/590], Loss: 1.4044\n",
      "Epoch [8/25], Step [260/590], Loss: 5.5532\n",
      "Epoch [8/25], Step [264/590], Loss: 1.2533\n",
      "Epoch [8/25], Step [268/590], Loss: 0.7119\n",
      "Epoch [8/25], Step [272/590], Loss: 2.4321\n",
      "Epoch [8/25], Step [276/590], Loss: 0.7852\n",
      "Epoch [8/25], Step [280/590], Loss: 8.7457\n",
      "Epoch [8/25], Step [284/590], Loss: 1.1780\n",
      "Epoch [8/25], Step [288/590], Loss: 2.2329\n",
      "Epoch [8/25], Step [292/590], Loss: 12.8891\n",
      "Epoch [8/25], Step [296/590], Loss: 4.3691\n",
      "Epoch [8/25], Step [300/590], Loss: 1.7151\n",
      "Epoch [8/25], Step [304/590], Loss: 0.7429\n",
      "Epoch [8/25], Step [308/590], Loss: 0.6767\n",
      "Epoch [8/25], Step [312/590], Loss: 2.6284\n",
      "Epoch [8/25], Step [316/590], Loss: 0.4552\n",
      "Epoch [8/25], Step [320/590], Loss: 2.3412\n",
      "Epoch [8/25], Step [324/590], Loss: 0.7394\n",
      "Epoch [8/25], Step [328/590], Loss: 1.4040\n",
      "Epoch [8/25], Step [332/590], Loss: 1.2519\n",
      "Epoch [8/25], Step [336/590], Loss: 3.4541\n",
      "Epoch [8/25], Step [340/590], Loss: 1.0735\n",
      "Epoch [8/25], Step [344/590], Loss: 0.4046\n",
      "Epoch [8/25], Step [348/590], Loss: 0.6993\n",
      "Epoch [8/25], Step [352/590], Loss: 1.1090\n",
      "Epoch [8/25], Step [356/590], Loss: 0.2767\n",
      "Epoch [8/25], Step [360/590], Loss: 0.8502\n",
      "Epoch [8/25], Step [364/590], Loss: 0.6453\n",
      "Epoch [8/25], Step [368/590], Loss: 2.4037\n",
      "Epoch [8/25], Step [372/590], Loss: 0.5058\n",
      "Epoch [8/25], Step [376/590], Loss: 1.8884\n",
      "Epoch [8/25], Step [380/590], Loss: 1.0894\n",
      "Epoch [8/25], Step [384/590], Loss: 1.2611\n",
      "Epoch [8/25], Step [388/590], Loss: 1.6976\n",
      "Epoch [8/25], Step [392/590], Loss: 0.4829\n",
      "Epoch [8/25], Step [396/590], Loss: 0.4802\n",
      "Epoch [8/25], Step [400/590], Loss: 0.1961\n",
      "Epoch [8/25], Step [404/590], Loss: 0.3058\n",
      "Epoch [8/25], Step [408/590], Loss: 0.5378\n",
      "Epoch [8/25], Step [412/590], Loss: 0.2771\n",
      "Epoch [8/25], Step [416/590], Loss: 3.7001\n",
      "Epoch [8/25], Step [420/590], Loss: 4.7478\n",
      "Epoch [8/25], Step [424/590], Loss: 2.3186\n",
      "Epoch [8/25], Step [428/590], Loss: 2.7734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], Step [432/590], Loss: 9.5260\n",
      "Epoch [8/25], Step [436/590], Loss: 1.1436\n",
      "Epoch [8/25], Step [440/590], Loss: 2.1824\n",
      "Epoch [8/25], Step [444/590], Loss: 4.7788\n",
      "Epoch [8/25], Step [448/590], Loss: 0.6571\n",
      "Epoch [8/25], Step [452/590], Loss: 9.8710\n",
      "Epoch [8/25], Step [456/590], Loss: 0.4511\n",
      "Epoch [8/25], Step [460/590], Loss: 2.3014\n",
      "Epoch [8/25], Step [464/590], Loss: 0.6874\n",
      "Epoch [8/25], Step [468/590], Loss: 0.2863\n",
      "Epoch [8/25], Step [472/590], Loss: 1.6098\n",
      "Epoch [8/25], Step [476/590], Loss: 0.4196\n",
      "Epoch [8/25], Step [480/590], Loss: 0.3266\n",
      "Epoch [8/25], Step [484/590], Loss: 1.3032\n",
      "Epoch [8/25], Step [488/590], Loss: 0.6983\n",
      "Epoch [8/25], Step [492/590], Loss: 1.6298\n",
      "Epoch [8/25], Step [496/590], Loss: 3.4331\n",
      "Epoch [8/25], Step [500/590], Loss: 1.3591\n",
      "Epoch [8/25], Step [504/590], Loss: 6.0170\n",
      "Epoch [8/25], Step [508/590], Loss: 0.4681\n",
      "Epoch [8/25], Step [512/590], Loss: 0.5299\n",
      "Epoch [8/25], Step [516/590], Loss: 0.8514\n",
      "Epoch [8/25], Step [520/590], Loss: 1.2944\n",
      "Epoch [8/25], Step [524/590], Loss: 4.9612\n",
      "Epoch [8/25], Step [528/590], Loss: 1.5089\n",
      "Epoch [8/25], Step [532/590], Loss: 1.3648\n",
      "Epoch [8/25], Step [536/590], Loss: 3.2423\n",
      "Epoch [8/25], Step [540/590], Loss: 7.9319\n",
      "Epoch [8/25], Step [544/590], Loss: 1.1210\n",
      "Epoch [8/25], Step [548/590], Loss: 1.4216\n",
      "Epoch [8/25], Step [552/590], Loss: 9.2556\n",
      "Epoch [8/25], Step [556/590], Loss: 0.4561\n",
      "Epoch [8/25], Step [560/590], Loss: 3.4599\n",
      "Epoch [8/25], Step [564/590], Loss: 3.4639\n",
      "Epoch [8/25], Step [568/590], Loss: 0.3015\n",
      "Epoch [8/25], Step [572/590], Loss: 1.1674\n",
      "Epoch [8/25], Step [576/590], Loss: 0.9273\n",
      "Epoch [8/25], Step [580/590], Loss: 2.2225\n",
      "Epoch [8/25], Step [584/590], Loss: 5.3902\n",
      "Epoch [8/25], Step [588/590], Loss: 5.6145\n",
      "epoch 8\n",
      "Epoch [9/25], Step [4/590], Loss: 1.1936\n",
      "Epoch [9/25], Step [8/590], Loss: 2.1174\n",
      "Epoch [9/25], Step [12/590], Loss: 4.0697\n",
      "Epoch [9/25], Step [16/590], Loss: 1.5047\n",
      "Epoch [9/25], Step [20/590], Loss: 1.5839\n",
      "Epoch [9/25], Step [24/590], Loss: 0.5538\n",
      "Epoch [9/25], Step [28/590], Loss: 2.9882\n",
      "Epoch [9/25], Step [32/590], Loss: 2.3105\n",
      "Epoch [9/25], Step [36/590], Loss: 1.0817\n",
      "Epoch [9/25], Step [40/590], Loss: 1.0252\n",
      "Epoch [9/25], Step [44/590], Loss: 1.1408\n",
      "Epoch [9/25], Step [48/590], Loss: 1.0984\n",
      "Epoch [9/25], Step [52/590], Loss: 2.5712\n",
      "Epoch [9/25], Step [56/590], Loss: 7.1462\n",
      "Epoch [9/25], Step [60/590], Loss: 1.6266\n",
      "Epoch [9/25], Step [64/590], Loss: 1.0736\n",
      "Epoch [9/25], Step [68/590], Loss: 2.9542\n",
      "Epoch [9/25], Step [72/590], Loss: 2.6170\n",
      "Epoch [9/25], Step [76/590], Loss: 0.3758\n",
      "Epoch [9/25], Step [80/590], Loss: 0.5320\n",
      "Epoch [9/25], Step [84/590], Loss: 0.4923\n",
      "Epoch [9/25], Step [88/590], Loss: 0.4550\n",
      "Epoch [9/25], Step [92/590], Loss: 6.8051\n",
      "Epoch [9/25], Step [96/590], Loss: 2.5092\n",
      "Epoch [9/25], Step [100/590], Loss: 1.3833\n",
      "Epoch [9/25], Step [104/590], Loss: 0.4749\n",
      "Epoch [9/25], Step [108/590], Loss: 8.1492\n",
      "Epoch [9/25], Step [112/590], Loss: 0.6847\n",
      "Epoch [9/25], Step [116/590], Loss: 0.9969\n",
      "Epoch [9/25], Step [120/590], Loss: 4.2291\n",
      "Epoch [9/25], Step [124/590], Loss: 0.1414\n",
      "Epoch [9/25], Step [128/590], Loss: 1.1662\n",
      "Epoch [9/25], Step [132/590], Loss: 0.2802\n",
      "Epoch [9/25], Step [136/590], Loss: 2.9228\n",
      "Epoch [9/25], Step [140/590], Loss: 0.7933\n",
      "Epoch [9/25], Step [144/590], Loss: 0.2812\n",
      "Epoch [9/25], Step [148/590], Loss: 4.9216\n",
      "Epoch [9/25], Step [152/590], Loss: 2.5566\n",
      "Epoch [9/25], Step [156/590], Loss: 2.0082\n",
      "Epoch [9/25], Step [160/590], Loss: 1.6683\n",
      "Epoch [9/25], Step [164/590], Loss: 0.5493\n",
      "Epoch [9/25], Step [168/590], Loss: 4.5166\n",
      "Epoch [9/25], Step [172/590], Loss: 0.3891\n",
      "Epoch [9/25], Step [176/590], Loss: 1.3635\n",
      "Epoch [9/25], Step [180/590], Loss: 3.6934\n",
      "Epoch [9/25], Step [184/590], Loss: 3.6529\n",
      "Epoch [9/25], Step [188/590], Loss: 5.0579\n",
      "Epoch [9/25], Step [192/590], Loss: 8.9469\n",
      "Epoch [9/25], Step [196/590], Loss: 0.7416\n",
      "Epoch [9/25], Step [200/590], Loss: 0.6985\n",
      "Epoch [9/25], Step [204/590], Loss: 3.4484\n",
      "Epoch [9/25], Step [208/590], Loss: 1.9792\n",
      "Epoch [9/25], Step [212/590], Loss: 0.9905\n",
      "Epoch [9/25], Step [216/590], Loss: 0.3452\n",
      "Epoch [9/25], Step [220/590], Loss: 4.8754\n",
      "Epoch [9/25], Step [224/590], Loss: 0.8339\n",
      "Epoch [9/25], Step [228/590], Loss: 0.5310\n",
      "Epoch [9/25], Step [232/590], Loss: 4.6413\n",
      "Epoch [9/25], Step [236/590], Loss: 2.2085\n",
      "Epoch [9/25], Step [240/590], Loss: 5.4010\n",
      "Epoch [9/25], Step [244/590], Loss: 0.3738\n",
      "Epoch [9/25], Step [248/590], Loss: 1.5294\n",
      "Epoch [9/25], Step [252/590], Loss: 0.6558\n",
      "Epoch [9/25], Step [256/590], Loss: 0.6485\n",
      "Epoch [9/25], Step [260/590], Loss: 1.2978\n",
      "Epoch [9/25], Step [264/590], Loss: 0.2483\n",
      "Epoch [9/25], Step [268/590], Loss: 2.8203\n",
      "Epoch [9/25], Step [272/590], Loss: 2.7646\n",
      "Epoch [9/25], Step [276/590], Loss: 1.4030\n",
      "Epoch [9/25], Step [280/590], Loss: 2.0998\n",
      "Epoch [9/25], Step [284/590], Loss: 0.6046\n",
      "Epoch [9/25], Step [288/590], Loss: 3.6298\n",
      "Epoch [9/25], Step [292/590], Loss: 0.8133\n",
      "Epoch [9/25], Step [296/590], Loss: 5.7627\n",
      "Epoch [9/25], Step [300/590], Loss: 0.4939\n",
      "Epoch [9/25], Step [304/590], Loss: 2.0085\n",
      "Epoch [9/25], Step [308/590], Loss: 2.5977\n",
      "Epoch [9/25], Step [312/590], Loss: 1.0465\n",
      "Epoch [9/25], Step [316/590], Loss: 2.3366\n",
      "Epoch [9/25], Step [320/590], Loss: 1.1773\n",
      "Epoch [9/25], Step [324/590], Loss: 1.0055\n",
      "Epoch [9/25], Step [328/590], Loss: 6.8279\n",
      "Epoch [9/25], Step [332/590], Loss: 2.2758\n",
      "Epoch [9/25], Step [336/590], Loss: 5.9810\n",
      "Epoch [9/25], Step [340/590], Loss: 0.7394\n",
      "Epoch [9/25], Step [344/590], Loss: 0.8388\n",
      "Epoch [9/25], Step [348/590], Loss: 6.1231\n",
      "Epoch [9/25], Step [352/590], Loss: 1.8300\n",
      "Epoch [9/25], Step [356/590], Loss: 0.4515\n",
      "Epoch [9/25], Step [360/590], Loss: 0.4872\n",
      "Epoch [9/25], Step [364/590], Loss: 1.5133\n",
      "Epoch [9/25], Step [368/590], Loss: 0.9050\n",
      "Epoch [9/25], Step [372/590], Loss: 3.1272\n",
      "Epoch [9/25], Step [376/590], Loss: 2.0292\n",
      "Epoch [9/25], Step [380/590], Loss: 0.9758\n",
      "Epoch [9/25], Step [384/590], Loss: 0.1795\n",
      "Epoch [9/25], Step [388/590], Loss: 4.6410\n",
      "Epoch [9/25], Step [392/590], Loss: 1.3466\n",
      "Epoch [9/25], Step [396/590], Loss: 0.8376\n",
      "Epoch [9/25], Step [400/590], Loss: 0.7655\n",
      "Epoch [9/25], Step [404/590], Loss: 0.2076\n",
      "Epoch [9/25], Step [408/590], Loss: 0.5400\n",
      "Epoch [9/25], Step [412/590], Loss: 0.1451\n",
      "Epoch [9/25], Step [416/590], Loss: 0.3390\n",
      "Epoch [9/25], Step [420/590], Loss: 0.7614\n",
      "Epoch [9/25], Step [424/590], Loss: 0.5434\n",
      "Epoch [9/25], Step [428/590], Loss: 2.3866\n",
      "Epoch [9/25], Step [432/590], Loss: 4.0237\n",
      "Epoch [9/25], Step [436/590], Loss: 0.8162\n",
      "Epoch [9/25], Step [440/590], Loss: 0.6147\n",
      "Epoch [9/25], Step [444/590], Loss: 2.2554\n",
      "Epoch [9/25], Step [448/590], Loss: 0.3462\n",
      "Epoch [9/25], Step [452/590], Loss: 0.1084\n",
      "Epoch [9/25], Step [456/590], Loss: 2.9138\n",
      "Epoch [9/25], Step [460/590], Loss: 0.8554\n",
      "Epoch [9/25], Step [464/590], Loss: 0.3424\n",
      "Epoch [9/25], Step [468/590], Loss: 0.6956\n",
      "Epoch [9/25], Step [472/590], Loss: 0.0495\n",
      "Epoch [9/25], Step [476/590], Loss: 0.7320\n",
      "Epoch [9/25], Step [480/590], Loss: 1.0116\n",
      "Epoch [9/25], Step [484/590], Loss: 0.1629\n",
      "Epoch [9/25], Step [488/590], Loss: 4.3281\n",
      "Epoch [9/25], Step [492/590], Loss: 1.2869\n",
      "Epoch [9/25], Step [496/590], Loss: 0.0748\n",
      "Epoch [9/25], Step [500/590], Loss: 2.0553\n",
      "Epoch [9/25], Step [504/590], Loss: 7.1847\n",
      "Epoch [9/25], Step [508/590], Loss: 0.1675\n",
      "Epoch [9/25], Step [512/590], Loss: 0.3007\n",
      "Epoch [9/25], Step [516/590], Loss: 0.1112\n",
      "Epoch [9/25], Step [520/590], Loss: 0.5294\n",
      "Epoch [9/25], Step [524/590], Loss: 0.5567\n",
      "Epoch [9/25], Step [528/590], Loss: 3.5433\n",
      "Epoch [9/25], Step [532/590], Loss: 3.4739\n",
      "Epoch [9/25], Step [536/590], Loss: 0.2102\n",
      "Epoch [9/25], Step [540/590], Loss: 1.5050\n",
      "Epoch [9/25], Step [544/590], Loss: 6.8406\n",
      "Epoch [9/25], Step [548/590], Loss: 1.6232\n",
      "Epoch [9/25], Step [552/590], Loss: 2.5705\n",
      "Epoch [9/25], Step [556/590], Loss: 0.1606\n",
      "Epoch [9/25], Step [560/590], Loss: 1.9036\n",
      "Epoch [9/25], Step [564/590], Loss: 0.3591\n",
      "Epoch [9/25], Step [568/590], Loss: 0.3877\n",
      "Epoch [9/25], Step [572/590], Loss: 0.7226\n",
      "Epoch [9/25], Step [576/590], Loss: 1.0097\n",
      "Epoch [9/25], Step [580/590], Loss: 2.8610\n",
      "Epoch [9/25], Step [584/590], Loss: 7.6089\n",
      "Epoch [9/25], Step [588/590], Loss: 1.9508\n",
      "epoch 9\n",
      "Epoch [10/25], Step [4/590], Loss: 0.5552\n",
      "Epoch [10/25], Step [8/590], Loss: 3.1393\n",
      "Epoch [10/25], Step [12/590], Loss: 0.4366\n",
      "Epoch [10/25], Step [16/590], Loss: 1.0272\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], Step [20/590], Loss: 0.1881\n",
      "Epoch [10/25], Step [24/590], Loss: 1.1677\n",
      "Epoch [10/25], Step [28/590], Loss: 2.0676\n",
      "Epoch [10/25], Step [32/590], Loss: 1.0161\n",
      "Epoch [10/25], Step [36/590], Loss: 0.8176\n",
      "Epoch [10/25], Step [40/590], Loss: 1.8567\n",
      "Epoch [10/25], Step [44/590], Loss: 0.3887\n",
      "Epoch [10/25], Step [48/590], Loss: 5.9305\n",
      "Epoch [10/25], Step [52/590], Loss: 1.6362\n",
      "Epoch [10/25], Step [56/590], Loss: 0.6132\n",
      "Epoch [10/25], Step [60/590], Loss: 0.1311\n",
      "Epoch [10/25], Step [64/590], Loss: 0.6295\n",
      "Epoch [10/25], Step [68/590], Loss: 0.0458\n",
      "Epoch [10/25], Step [72/590], Loss: 0.0672\n",
      "Epoch [10/25], Step [76/590], Loss: 0.5017\n",
      "Epoch [10/25], Step [80/590], Loss: 0.2616\n",
      "Epoch [10/25], Step [84/590], Loss: 0.2255\n",
      "Epoch [10/25], Step [88/590], Loss: 0.5839\n",
      "Epoch [10/25], Step [92/590], Loss: 4.8913\n",
      "Epoch [10/25], Step [96/590], Loss: 0.2791\n",
      "Epoch [10/25], Step [100/590], Loss: 0.0399\n",
      "Epoch [10/25], Step [104/590], Loss: 0.1787\n",
      "Epoch [10/25], Step [108/590], Loss: 3.7003\n",
      "Epoch [10/25], Step [112/590], Loss: 0.3923\n",
      "Epoch [10/25], Step [116/590], Loss: 5.5902\n",
      "Epoch [10/25], Step [120/590], Loss: 3.4820\n",
      "Epoch [10/25], Step [124/590], Loss: 3.0078\n",
      "Epoch [10/25], Step [128/590], Loss: 0.2317\n",
      "Epoch [10/25], Step [132/590], Loss: 4.3217\n",
      "Epoch [10/25], Step [136/590], Loss: 3.0798\n",
      "Epoch [10/25], Step [140/590], Loss: 7.1494\n",
      "Epoch [10/25], Step [144/590], Loss: 5.1767\n",
      "Epoch [10/25], Step [148/590], Loss: 9.7061\n",
      "Epoch [10/25], Step [152/590], Loss: 1.1210\n",
      "Epoch [10/25], Step [156/590], Loss: 9.4316\n",
      "Epoch [10/25], Step [160/590], Loss: 14.7667\n",
      "Epoch [10/25], Step [164/590], Loss: 1.4285\n",
      "Epoch [10/25], Step [168/590], Loss: 1.6247\n",
      "Epoch [10/25], Step [172/590], Loss: 2.1607\n",
      "Epoch [10/25], Step [176/590], Loss: 1.5912\n",
      "Epoch [10/25], Step [180/590], Loss: 2.7933\n",
      "Epoch [10/25], Step [184/590], Loss: 2.8338\n",
      "Epoch [10/25], Step [188/590], Loss: 1.3863\n",
      "Epoch [10/25], Step [192/590], Loss: 2.1491\n",
      "Epoch [10/25], Step [196/590], Loss: 3.8729\n",
      "Epoch [10/25], Step [200/590], Loss: 0.5242\n",
      "Epoch [10/25], Step [204/590], Loss: 0.6208\n",
      "Epoch [10/25], Step [208/590], Loss: 3.0086\n",
      "Epoch [10/25], Step [212/590], Loss: 1.4826\n",
      "Epoch [10/25], Step [216/590], Loss: 0.2853\n",
      "Epoch [10/25], Step [220/590], Loss: 1.0705\n",
      "Epoch [10/25], Step [224/590], Loss: 2.3530\n",
      "Epoch [10/25], Step [228/590], Loss: 0.3657\n",
      "Epoch [10/25], Step [232/590], Loss: 0.8618\n",
      "Epoch [10/25], Step [236/590], Loss: 5.9014\n",
      "Epoch [10/25], Step [240/590], Loss: 0.7944\n",
      "Epoch [10/25], Step [244/590], Loss: 0.6362\n",
      "Epoch [10/25], Step [248/590], Loss: 3.7560\n",
      "Epoch [10/25], Step [252/590], Loss: 0.9458\n",
      "Epoch [10/25], Step [256/590], Loss: 1.3488\n",
      "Epoch [10/25], Step [260/590], Loss: 5.2583\n",
      "Epoch [10/25], Step [264/590], Loss: 6.7979\n",
      "Epoch [10/25], Step [268/590], Loss: 3.9167\n",
      "Epoch [10/25], Step [272/590], Loss: 0.4687\n",
      "Epoch [10/25], Step [276/590], Loss: 2.0475\n",
      "Epoch [10/25], Step [280/590], Loss: 1.0673\n",
      "Epoch [10/25], Step [284/590], Loss: 1.4848\n",
      "Epoch [10/25], Step [288/590], Loss: 0.5828\n",
      "Epoch [10/25], Step [292/590], Loss: 12.2960\n",
      "Epoch [10/25], Step [296/590], Loss: 6.3537\n",
      "Epoch [10/25], Step [300/590], Loss: 2.4140\n",
      "Epoch [10/25], Step [304/590], Loss: 1.8765\n",
      "Epoch [10/25], Step [308/590], Loss: 0.3875\n",
      "Epoch [10/25], Step [312/590], Loss: 1.8264\n",
      "Epoch [10/25], Step [316/590], Loss: 1.3310\n",
      "Epoch [10/25], Step [320/590], Loss: 1.0018\n",
      "Epoch [10/25], Step [324/590], Loss: 8.8009\n",
      "Epoch [10/25], Step [328/590], Loss: 2.5926\n",
      "Epoch [10/25], Step [332/590], Loss: 0.7565\n",
      "Epoch [10/25], Step [336/590], Loss: 7.0801\n",
      "Epoch [10/25], Step [340/590], Loss: 1.9100\n",
      "Epoch [10/25], Step [344/590], Loss: 0.6281\n",
      "Epoch [10/25], Step [348/590], Loss: 12.9144\n",
      "Epoch [10/25], Step [352/590], Loss: 1.1978\n",
      "Epoch [10/25], Step [356/590], Loss: 4.7872\n",
      "Epoch [10/25], Step [360/590], Loss: 1.4308\n",
      "Epoch [10/25], Step [364/590], Loss: 0.6982\n",
      "Epoch [10/25], Step [368/590], Loss: 5.2210\n",
      "Epoch [10/25], Step [372/590], Loss: 1.3544\n",
      "Epoch [10/25], Step [376/590], Loss: 6.0627\n",
      "Epoch [10/25], Step [380/590], Loss: 0.3723\n",
      "Epoch [10/25], Step [384/590], Loss: 0.3239\n",
      "Epoch [10/25], Step [388/590], Loss: 3.1911\n",
      "Epoch [10/25], Step [392/590], Loss: 1.0471\n",
      "Epoch [10/25], Step [396/590], Loss: 0.1592\n",
      "Epoch [10/25], Step [400/590], Loss: 0.4037\n",
      "Epoch [10/25], Step [404/590], Loss: 1.1583\n",
      "Epoch [10/25], Step [408/590], Loss: 2.1545\n",
      "Epoch [10/25], Step [412/590], Loss: 6.0484\n",
      "Epoch [10/25], Step [416/590], Loss: 1.5735\n",
      "Epoch [10/25], Step [420/590], Loss: 1.8878\n",
      "Epoch [10/25], Step [424/590], Loss: 0.7266\n",
      "Epoch [10/25], Step [428/590], Loss: 0.2297\n",
      "Epoch [10/25], Step [432/590], Loss: 0.4119\n",
      "Epoch [10/25], Step [436/590], Loss: 2.3557\n",
      "Epoch [10/25], Step [440/590], Loss: 2.2328\n",
      "Epoch [10/25], Step [444/590], Loss: 0.3126\n",
      "Epoch [10/25], Step [448/590], Loss: 1.3209\n",
      "Epoch [10/25], Step [452/590], Loss: 2.2498\n",
      "Epoch [10/25], Step [456/590], Loss: 1.6142\n",
      "Epoch [10/25], Step [460/590], Loss: 2.5491\n",
      "Epoch [10/25], Step [464/590], Loss: 0.0527\n",
      "Epoch [10/25], Step [468/590], Loss: 0.6075\n",
      "Epoch [10/25], Step [472/590], Loss: 2.4173\n",
      "Epoch [10/25], Step [476/590], Loss: 0.3043\n",
      "Epoch [10/25], Step [480/590], Loss: 0.1471\n",
      "Epoch [10/25], Step [484/590], Loss: 0.2560\n",
      "Epoch [10/25], Step [488/590], Loss: 1.7947\n",
      "Epoch [10/25], Step [492/590], Loss: 1.3921\n",
      "Epoch [10/25], Step [496/590], Loss: 6.3605\n",
      "Epoch [10/25], Step [500/590], Loss: 0.2755\n",
      "Epoch [10/25], Step [504/590], Loss: 0.1849\n",
      "Epoch [10/25], Step [508/590], Loss: 0.3680\n",
      "Epoch [10/25], Step [512/590], Loss: 1.2672\n",
      "Epoch [10/25], Step [516/590], Loss: 1.7684\n",
      "Epoch [10/25], Step [520/590], Loss: 8.0822\n",
      "Epoch [10/25], Step [524/590], Loss: 1.7779\n",
      "Epoch [10/25], Step [528/590], Loss: 0.6151\n",
      "Epoch [10/25], Step [532/590], Loss: 2.8492\n",
      "Epoch [10/25], Step [536/590], Loss: 0.6610\n",
      "Epoch [10/25], Step [540/590], Loss: 0.7628\n",
      "Epoch [10/25], Step [544/590], Loss: 4.9672\n",
      "Epoch [10/25], Step [548/590], Loss: 11.8088\n",
      "Epoch [10/25], Step [552/590], Loss: 0.0880\n",
      "Epoch [10/25], Step [556/590], Loss: 1.6346\n",
      "Epoch [10/25], Step [560/590], Loss: 0.1270\n",
      "Epoch [10/25], Step [564/590], Loss: 3.4395\n",
      "Epoch [10/25], Step [568/590], Loss: 1.0427\n",
      "Epoch [10/25], Step [572/590], Loss: 8.3308\n",
      "Epoch [10/25], Step [576/590], Loss: 1.0344\n",
      "Epoch [10/25], Step [580/590], Loss: 4.3542\n",
      "Epoch [10/25], Step [584/590], Loss: 0.6271\n",
      "Epoch [10/25], Step [588/590], Loss: 0.2556\n",
      "epoch 10\n",
      "Epoch [11/25], Step [4/590], Loss: 1.2548\n",
      "Epoch [11/25], Step [8/590], Loss: 0.3253\n",
      "Epoch [11/25], Step [12/590], Loss: 1.8612\n",
      "Epoch [11/25], Step [16/590], Loss: 0.2930\n",
      "Epoch [11/25], Step [20/590], Loss: 0.1614\n",
      "Epoch [11/25], Step [24/590], Loss: 6.4368\n",
      "Epoch [11/25], Step [28/590], Loss: 0.1322\n",
      "Epoch [11/25], Step [32/590], Loss: 0.5405\n",
      "Epoch [11/25], Step [36/590], Loss: 4.8815\n",
      "Epoch [11/25], Step [40/590], Loss: 0.6906\n",
      "Epoch [11/25], Step [44/590], Loss: 0.5762\n",
      "Epoch [11/25], Step [48/590], Loss: 0.1720\n",
      "Epoch [11/25], Step [52/590], Loss: 4.9791\n",
      "Epoch [11/25], Step [56/590], Loss: 0.6866\n",
      "Epoch [11/25], Step [60/590], Loss: 5.5094\n",
      "Epoch [11/25], Step [64/590], Loss: 0.3438\n",
      "Epoch [11/25], Step [68/590], Loss: 8.6360\n",
      "Epoch [11/25], Step [72/590], Loss: 3.3495\n",
      "Epoch [11/25], Step [76/590], Loss: 3.0873\n",
      "Epoch [11/25], Step [80/590], Loss: 0.3596\n",
      "Epoch [11/25], Step [84/590], Loss: 2.0316\n",
      "Epoch [11/25], Step [88/590], Loss: 0.9506\n",
      "Epoch [11/25], Step [92/590], Loss: 0.8014\n",
      "Epoch [11/25], Step [96/590], Loss: 0.3810\n",
      "Epoch [11/25], Step [100/590], Loss: 0.2533\n",
      "Epoch [11/25], Step [104/590], Loss: 2.8426\n",
      "Epoch [11/25], Step [108/590], Loss: 0.3558\n",
      "Epoch [11/25], Step [112/590], Loss: 4.3409\n",
      "Epoch [11/25], Step [116/590], Loss: 1.3198\n",
      "Epoch [11/25], Step [120/590], Loss: 0.3066\n",
      "Epoch [11/25], Step [124/590], Loss: 1.5745\n",
      "Epoch [11/25], Step [128/590], Loss: 3.6335\n",
      "Epoch [11/25], Step [132/590], Loss: 2.3059\n",
      "Epoch [11/25], Step [136/590], Loss: 0.2269\n",
      "Epoch [11/25], Step [140/590], Loss: 0.4472\n",
      "Epoch [11/25], Step [144/590], Loss: 0.2977\n",
      "Epoch [11/25], Step [148/590], Loss: 1.5600\n",
      "Epoch [11/25], Step [152/590], Loss: 2.6668\n",
      "Epoch [11/25], Step [156/590], Loss: 0.1722\n",
      "Epoch [11/25], Step [160/590], Loss: 2.0078\n",
      "Epoch [11/25], Step [164/590], Loss: 0.3725\n",
      "Epoch [11/25], Step [168/590], Loss: 1.0086\n",
      "Epoch [11/25], Step [172/590], Loss: 0.5903\n",
      "Epoch [11/25], Step [176/590], Loss: 2.4688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Step [180/590], Loss: 0.8072\n",
      "Epoch [11/25], Step [184/590], Loss: 2.5857\n",
      "Epoch [11/25], Step [188/590], Loss: 1.3169\n",
      "Epoch [11/25], Step [192/590], Loss: 3.1881\n",
      "Epoch [11/25], Step [196/590], Loss: 4.2226\n",
      "Epoch [11/25], Step [200/590], Loss: 0.5133\n",
      "Epoch [11/25], Step [204/590], Loss: 0.7639\n",
      "Epoch [11/25], Step [208/590], Loss: 0.2910\n",
      "Epoch [11/25], Step [212/590], Loss: 0.1720\n",
      "Epoch [11/25], Step [216/590], Loss: 0.3548\n",
      "Epoch [11/25], Step [220/590], Loss: 3.2268\n",
      "Epoch [11/25], Step [224/590], Loss: 1.2122\n",
      "Epoch [11/25], Step [228/590], Loss: 0.2789\n",
      "Epoch [11/25], Step [232/590], Loss: 0.1160\n",
      "Epoch [11/25], Step [236/590], Loss: 0.1015\n",
      "Epoch [11/25], Step [240/590], Loss: 0.4439\n",
      "Epoch [11/25], Step [244/590], Loss: 0.3290\n",
      "Epoch [11/25], Step [248/590], Loss: 0.1383\n",
      "Epoch [11/25], Step [252/590], Loss: 0.8691\n",
      "Epoch [11/25], Step [256/590], Loss: 0.2912\n",
      "Epoch [11/25], Step [260/590], Loss: 0.4074\n",
      "Epoch [11/25], Step [264/590], Loss: 4.1079\n",
      "Epoch [11/25], Step [268/590], Loss: 3.4395\n",
      "Epoch [11/25], Step [272/590], Loss: 2.8124\n",
      "Epoch [11/25], Step [276/590], Loss: 1.2755\n",
      "Epoch [11/25], Step [280/590], Loss: 5.9520\n",
      "Epoch [11/25], Step [284/590], Loss: 6.6951\n",
      "Epoch [11/25], Step [288/590], Loss: 1.6859\n",
      "Epoch [11/25], Step [292/590], Loss: 5.2514\n",
      "Epoch [11/25], Step [296/590], Loss: 1.3786\n",
      "Epoch [11/25], Step [300/590], Loss: 2.2748\n",
      "Epoch [11/25], Step [304/590], Loss: 0.5356\n",
      "Epoch [11/25], Step [308/590], Loss: 2.3830\n",
      "Epoch [11/25], Step [312/590], Loss: 0.5854\n",
      "Epoch [11/25], Step [316/590], Loss: 0.1111\n",
      "Epoch [11/25], Step [320/590], Loss: 3.7466\n",
      "Epoch [11/25], Step [324/590], Loss: 0.3380\n",
      "Epoch [11/25], Step [328/590], Loss: 0.9874\n",
      "Epoch [11/25], Step [332/590], Loss: 0.4318\n",
      "Epoch [11/25], Step [336/590], Loss: 7.1246\n",
      "Epoch [11/25], Step [340/590], Loss: 0.8996\n",
      "Epoch [11/25], Step [344/590], Loss: 4.2663\n",
      "Epoch [11/25], Step [348/590], Loss: 0.7399\n",
      "Epoch [11/25], Step [352/590], Loss: 0.3772\n",
      "Epoch [11/25], Step [356/590], Loss: 0.3572\n",
      "Epoch [11/25], Step [360/590], Loss: 0.2457\n",
      "Epoch [11/25], Step [364/590], Loss: 1.9086\n",
      "Epoch [11/25], Step [368/590], Loss: 3.6073\n",
      "Epoch [11/25], Step [372/590], Loss: 1.3729\n",
      "Epoch [11/25], Step [376/590], Loss: 1.3946\n",
      "Epoch [11/25], Step [380/590], Loss: 0.2954\n",
      "Epoch [11/25], Step [384/590], Loss: 2.6569\n",
      "Epoch [11/25], Step [388/590], Loss: 4.3855\n",
      "Epoch [11/25], Step [392/590], Loss: 0.0918\n",
      "Epoch [11/25], Step [396/590], Loss: 0.8528\n",
      "Epoch [11/25], Step [400/590], Loss: 0.3455\n",
      "Epoch [11/25], Step [404/590], Loss: 0.1729\n",
      "Epoch [11/25], Step [408/590], Loss: 5.5684\n",
      "Epoch [11/25], Step [412/590], Loss: 0.7428\n",
      "Epoch [11/25], Step [416/590], Loss: 5.1061\n",
      "Epoch [11/25], Step [420/590], Loss: 1.0421\n",
      "Epoch [11/25], Step [424/590], Loss: 1.1371\n",
      "Epoch [11/25], Step [428/590], Loss: 0.6262\n",
      "Epoch [11/25], Step [432/590], Loss: 1.7903\n",
      "Epoch [11/25], Step [436/590], Loss: 0.9463\n",
      "Epoch [11/25], Step [440/590], Loss: 1.0068\n",
      "Epoch [11/25], Step [444/590], Loss: 1.4855\n",
      "Epoch [11/25], Step [448/590], Loss: 2.9608\n",
      "Epoch [11/25], Step [452/590], Loss: 0.3976\n",
      "Epoch [11/25], Step [456/590], Loss: 4.1962\n",
      "Epoch [11/25], Step [460/590], Loss: 0.3498\n",
      "Epoch [11/25], Step [464/590], Loss: 3.5474\n",
      "Epoch [11/25], Step [468/590], Loss: 2.7075\n",
      "Epoch [11/25], Step [472/590], Loss: 1.6703\n",
      "Epoch [11/25], Step [476/590], Loss: 0.0574\n",
      "Epoch [11/25], Step [480/590], Loss: 0.4860\n",
      "Epoch [11/25], Step [484/590], Loss: 1.2144\n",
      "Epoch [11/25], Step [488/590], Loss: 1.4549\n",
      "Epoch [11/25], Step [492/590], Loss: 0.0410\n",
      "Epoch [11/25], Step [496/590], Loss: 2.0031\n",
      "Epoch [11/25], Step [500/590], Loss: 0.7604\n",
      "Epoch [11/25], Step [504/590], Loss: 0.2615\n",
      "Epoch [11/25], Step [508/590], Loss: 0.2525\n",
      "Epoch [11/25], Step [512/590], Loss: 0.8679\n",
      "Epoch [11/25], Step [516/590], Loss: 3.5844\n",
      "Epoch [11/25], Step [520/590], Loss: 0.0740\n",
      "Epoch [11/25], Step [524/590], Loss: 1.8286\n",
      "Epoch [11/25], Step [528/590], Loss: 0.8687\n",
      "Epoch [11/25], Step [532/590], Loss: 0.8699\n",
      "Epoch [11/25], Step [536/590], Loss: 0.5061\n",
      "Epoch [11/25], Step [540/590], Loss: 7.1858\n",
      "Epoch [11/25], Step [544/590], Loss: 4.5243\n",
      "Epoch [11/25], Step [548/590], Loss: 4.2812\n",
      "Epoch [11/25], Step [552/590], Loss: 0.4443\n",
      "Epoch [11/25], Step [556/590], Loss: 1.9859\n",
      "Epoch [11/25], Step [560/590], Loss: 0.2052\n",
      "Epoch [11/25], Step [564/590], Loss: 0.2118\n",
      "Epoch [11/25], Step [568/590], Loss: 0.0730\n",
      "Epoch [11/25], Step [572/590], Loss: 0.0082\n",
      "Epoch [11/25], Step [576/590], Loss: 1.9775\n",
      "Epoch [11/25], Step [580/590], Loss: 4.9491\n",
      "Epoch [11/25], Step [584/590], Loss: 1.8304\n",
      "Epoch [11/25], Step [588/590], Loss: 0.5442\n",
      "epoch 11\n",
      "Epoch [12/25], Step [4/590], Loss: 0.7785\n",
      "Epoch [12/25], Step [8/590], Loss: 1.0844\n",
      "Epoch [12/25], Step [12/590], Loss: 1.3899\n",
      "Epoch [12/25], Step [16/590], Loss: 1.8265\n",
      "Epoch [12/25], Step [20/590], Loss: 1.1210\n",
      "Epoch [12/25], Step [24/590], Loss: 0.0980\n",
      "Epoch [12/25], Step [28/590], Loss: 3.0329\n",
      "Epoch [12/25], Step [32/590], Loss: 0.7539\n",
      "Epoch [12/25], Step [36/590], Loss: 0.1644\n",
      "Epoch [12/25], Step [40/590], Loss: 0.6798\n",
      "Epoch [12/25], Step [44/590], Loss: 0.4782\n",
      "Epoch [12/25], Step [48/590], Loss: 2.8979\n",
      "Epoch [12/25], Step [52/590], Loss: 0.2980\n",
      "Epoch [12/25], Step [56/590], Loss: 0.5855\n",
      "Epoch [12/25], Step [60/590], Loss: 3.5789\n",
      "Epoch [12/25], Step [64/590], Loss: 0.5190\n",
      "Epoch [12/25], Step [68/590], Loss: 0.1850\n",
      "Epoch [12/25], Step [72/590], Loss: 0.3729\n",
      "Epoch [12/25], Step [76/590], Loss: 0.3119\n",
      "Epoch [12/25], Step [80/590], Loss: 0.8003\n",
      "Epoch [12/25], Step [84/590], Loss: 1.0497\n",
      "Epoch [12/25], Step [88/590], Loss: 0.1875\n",
      "Epoch [12/25], Step [92/590], Loss: 0.5790\n",
      "Epoch [12/25], Step [96/590], Loss: 6.3368\n",
      "Epoch [12/25], Step [100/590], Loss: 0.7337\n",
      "Epoch [12/25], Step [104/590], Loss: 0.5591\n",
      "Epoch [12/25], Step [108/590], Loss: 0.8874\n",
      "Epoch [12/25], Step [112/590], Loss: 1.4271\n",
      "Epoch [12/25], Step [116/590], Loss: 1.8629\n",
      "Epoch [12/25], Step [120/590], Loss: 0.6152\n",
      "Epoch [12/25], Step [124/590], Loss: 4.0854\n",
      "Epoch [12/25], Step [128/590], Loss: 1.0064\n",
      "Epoch [12/25], Step [132/590], Loss: 0.3337\n",
      "Epoch [12/25], Step [136/590], Loss: 1.0459\n",
      "Epoch [12/25], Step [140/590], Loss: 0.4377\n",
      "Epoch [12/25], Step [144/590], Loss: 0.0756\n",
      "Epoch [12/25], Step [148/590], Loss: 0.0368\n",
      "Epoch [12/25], Step [152/590], Loss: 0.1402\n",
      "Epoch [12/25], Step [156/590], Loss: 2.7078\n",
      "Epoch [12/25], Step [160/590], Loss: 0.9855\n",
      "Epoch [12/25], Step [164/590], Loss: 0.0672\n",
      "Epoch [12/25], Step [168/590], Loss: 0.1234\n",
      "Epoch [12/25], Step [172/590], Loss: 0.2409\n",
      "Epoch [12/25], Step [176/590], Loss: 0.3456\n",
      "Epoch [12/25], Step [180/590], Loss: 4.4444\n",
      "Epoch [12/25], Step [184/590], Loss: 2.2924\n",
      "Epoch [12/25], Step [188/590], Loss: 0.4871\n",
      "Epoch [12/25], Step [192/590], Loss: 2.5773\n",
      "Epoch [12/25], Step [196/590], Loss: 0.8835\n",
      "Epoch [12/25], Step [200/590], Loss: 0.5817\n",
      "Epoch [12/25], Step [204/590], Loss: 0.2302\n",
      "Epoch [12/25], Step [208/590], Loss: 0.1561\n",
      "Epoch [12/25], Step [212/590], Loss: 0.1826\n",
      "Epoch [12/25], Step [216/590], Loss: 0.3378\n",
      "Epoch [12/25], Step [220/590], Loss: 0.5218\n",
      "Epoch [12/25], Step [224/590], Loss: 1.2681\n",
      "Epoch [12/25], Step [228/590], Loss: 0.0867\n",
      "Epoch [12/25], Step [232/590], Loss: 2.7914\n",
      "Epoch [12/25], Step [236/590], Loss: 0.2792\n",
      "Epoch [12/25], Step [240/590], Loss: 0.4698\n",
      "Epoch [12/25], Step [244/590], Loss: 0.3713\n",
      "Epoch [12/25], Step [248/590], Loss: 0.6629\n",
      "Epoch [12/25], Step [252/590], Loss: 0.4048\n",
      "Epoch [12/25], Step [256/590], Loss: 3.2872\n",
      "Epoch [12/25], Step [260/590], Loss: 2.6988\n",
      "Epoch [12/25], Step [264/590], Loss: 1.3364\n",
      "Epoch [12/25], Step [268/590], Loss: 0.2245\n",
      "Epoch [12/25], Step [272/590], Loss: 2.0566\n",
      "Epoch [12/25], Step [276/590], Loss: 1.0887\n",
      "Epoch [12/25], Step [280/590], Loss: 0.1868\n",
      "Epoch [12/25], Step [284/590], Loss: 0.3255\n",
      "Epoch [12/25], Step [288/590], Loss: 1.2052\n",
      "Epoch [12/25], Step [292/590], Loss: 6.2469\n",
      "Epoch [12/25], Step [296/590], Loss: 0.1624\n",
      "Epoch [12/25], Step [300/590], Loss: 0.3721\n",
      "Epoch [12/25], Step [304/590], Loss: 0.3677\n",
      "Epoch [12/25], Step [308/590], Loss: 2.0847\n",
      "Epoch [12/25], Step [312/590], Loss: 8.1035\n",
      "Epoch [12/25], Step [316/590], Loss: 0.0736\n",
      "Epoch [12/25], Step [320/590], Loss: 2.2681\n",
      "Epoch [12/25], Step [324/590], Loss: 6.3106\n",
      "Epoch [12/25], Step [328/590], Loss: 0.3745\n",
      "Epoch [12/25], Step [332/590], Loss: 0.7417\n",
      "Epoch [12/25], Step [336/590], Loss: 0.2021\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Step [340/590], Loss: 3.5237\n",
      "Epoch [12/25], Step [344/590], Loss: 0.4598\n",
      "Epoch [12/25], Step [348/590], Loss: 3.5345\n",
      "Epoch [12/25], Step [352/590], Loss: 0.2553\n",
      "Epoch [12/25], Step [356/590], Loss: 2.6334\n",
      "Epoch [12/25], Step [360/590], Loss: 0.5525\n",
      "Epoch [12/25], Step [364/590], Loss: 0.7417\n",
      "Epoch [12/25], Step [368/590], Loss: 0.0990\n",
      "Epoch [12/25], Step [372/590], Loss: 0.7026\n",
      "Epoch [12/25], Step [376/590], Loss: 1.9076\n",
      "Epoch [12/25], Step [380/590], Loss: 0.2661\n",
      "Epoch [12/25], Step [384/590], Loss: 1.1747\n",
      "Epoch [12/25], Step [388/590], Loss: 0.9790\n",
      "Epoch [12/25], Step [392/590], Loss: 1.5614\n",
      "Epoch [12/25], Step [396/590], Loss: 6.6142\n",
      "Epoch [12/25], Step [400/590], Loss: 0.2587\n",
      "Epoch [12/25], Step [404/590], Loss: 1.3785\n",
      "Epoch [12/25], Step [408/590], Loss: 0.2373\n",
      "Epoch [12/25], Step [412/590], Loss: 0.4805\n",
      "Epoch [12/25], Step [416/590], Loss: 0.4508\n",
      "Epoch [12/25], Step [420/590], Loss: 0.1945\n",
      "Epoch [12/25], Step [424/590], Loss: 0.6499\n",
      "Epoch [12/25], Step [428/590], Loss: 0.2004\n",
      "Epoch [12/25], Step [432/590], Loss: 0.1613\n",
      "Epoch [12/25], Step [436/590], Loss: 0.3240\n",
      "Epoch [12/25], Step [440/590], Loss: 2.8999\n",
      "Epoch [12/25], Step [444/590], Loss: 0.1486\n",
      "Epoch [12/25], Step [448/590], Loss: 1.2234\n",
      "Epoch [12/25], Step [452/590], Loss: 0.9254\n",
      "Epoch [12/25], Step [456/590], Loss: 1.6165\n",
      "Epoch [12/25], Step [460/590], Loss: 0.1261\n",
      "Epoch [12/25], Step [464/590], Loss: 1.4068\n",
      "Epoch [12/25], Step [468/590], Loss: 0.6353\n",
      "Epoch [12/25], Step [472/590], Loss: 0.5027\n",
      "Epoch [12/25], Step [476/590], Loss: 0.6244\n",
      "Epoch [12/25], Step [480/590], Loss: 1.7271\n",
      "Epoch [12/25], Step [484/590], Loss: 0.1293\n",
      "Epoch [12/25], Step [488/590], Loss: 0.6784\n",
      "Epoch [12/25], Step [492/590], Loss: 1.4464\n",
      "Epoch [12/25], Step [496/590], Loss: 0.2551\n",
      "Epoch [12/25], Step [500/590], Loss: 0.6944\n",
      "Epoch [12/25], Step [504/590], Loss: 0.1036\n",
      "Epoch [12/25], Step [508/590], Loss: 1.1535\n",
      "Epoch [12/25], Step [512/590], Loss: 2.4584\n",
      "Epoch [12/25], Step [516/590], Loss: 2.6585\n",
      "Epoch [12/25], Step [520/590], Loss: 1.2086\n",
      "Epoch [12/25], Step [524/590], Loss: 1.1626\n",
      "Epoch [12/25], Step [528/590], Loss: 0.1736\n",
      "Epoch [12/25], Step [532/590], Loss: 0.9131\n",
      "Epoch [12/25], Step [536/590], Loss: 1.3325\n",
      "Epoch [12/25], Step [540/590], Loss: 0.1555\n",
      "Epoch [12/25], Step [544/590], Loss: 0.2559\n",
      "Epoch [12/25], Step [548/590], Loss: 0.1419\n",
      "Epoch [12/25], Step [552/590], Loss: 0.4412\n",
      "Epoch [12/25], Step [556/590], Loss: 6.7086\n",
      "Epoch [12/25], Step [560/590], Loss: 3.0042\n",
      "Epoch [12/25], Step [564/590], Loss: 0.5363\n",
      "Epoch [12/25], Step [568/590], Loss: 0.6727\n",
      "Epoch [12/25], Step [572/590], Loss: 0.8548\n",
      "Epoch [12/25], Step [576/590], Loss: 0.4939\n",
      "Epoch [12/25], Step [580/590], Loss: 0.7062\n",
      "Epoch [12/25], Step [584/590], Loss: 0.8205\n",
      "Epoch [12/25], Step [588/590], Loss: 1.6043\n",
      "epoch 12\n",
      "Epoch [13/25], Step [4/590], Loss: 3.6303\n",
      "Epoch [13/25], Step [8/590], Loss: 0.5276\n",
      "Epoch [13/25], Step [12/590], Loss: 0.3114\n",
      "Epoch [13/25], Step [16/590], Loss: 6.2813\n",
      "Epoch [13/25], Step [20/590], Loss: 2.0751\n",
      "Epoch [13/25], Step [24/590], Loss: 0.0752\n",
      "Epoch [13/25], Step [28/590], Loss: 4.8801\n",
      "Epoch [13/25], Step [32/590], Loss: 1.2398\n",
      "Epoch [13/25], Step [36/590], Loss: 6.1658\n",
      "Epoch [13/25], Step [40/590], Loss: 1.0819\n",
      "Epoch [13/25], Step [44/590], Loss: 0.9899\n",
      "Epoch [13/25], Step [48/590], Loss: 3.9689\n",
      "Epoch [13/25], Step [52/590], Loss: 1.6885\n",
      "Epoch [13/25], Step [56/590], Loss: 0.2289\n",
      "Epoch [13/25], Step [60/590], Loss: 1.2857\n",
      "Epoch [13/25], Step [64/590], Loss: 1.7659\n",
      "Epoch [13/25], Step [68/590], Loss: 0.7144\n",
      "Epoch [13/25], Step [72/590], Loss: 1.1878\n",
      "Epoch [13/25], Step [76/590], Loss: 4.9024\n",
      "Epoch [13/25], Step [80/590], Loss: 0.1249\n",
      "Epoch [13/25], Step [84/590], Loss: 8.0603\n",
      "Epoch [13/25], Step [88/590], Loss: 1.9379\n",
      "Epoch [13/25], Step [92/590], Loss: 0.2149\n",
      "Epoch [13/25], Step [96/590], Loss: 1.4474\n",
      "Epoch [13/25], Step [100/590], Loss: 0.7479\n",
      "Epoch [13/25], Step [104/590], Loss: 0.2827\n",
      "Epoch [13/25], Step [108/590], Loss: 0.5022\n",
      "Epoch [13/25], Step [112/590], Loss: 2.8675\n",
      "Epoch [13/25], Step [116/590], Loss: 0.0732\n",
      "Epoch [13/25], Step [120/590], Loss: 0.5930\n",
      "Epoch [13/25], Step [124/590], Loss: 0.9392\n",
      "Epoch [13/25], Step [128/590], Loss: 0.4983\n",
      "Epoch [13/25], Step [132/590], Loss: 6.0743\n",
      "Epoch [13/25], Step [136/590], Loss: 1.4094\n",
      "Epoch [13/25], Step [140/590], Loss: 0.1211\n",
      "Epoch [13/25], Step [144/590], Loss: 0.2190\n",
      "Epoch [13/25], Step [148/590], Loss: 1.5012\n",
      "Epoch [13/25], Step [152/590], Loss: 1.1817\n",
      "Epoch [13/25], Step [156/590], Loss: 0.1529\n",
      "Epoch [13/25], Step [160/590], Loss: 0.1554\n",
      "Epoch [13/25], Step [164/590], Loss: 0.7747\n",
      "Epoch [13/25], Step [168/590], Loss: 0.9281\n",
      "Epoch [13/25], Step [172/590], Loss: 0.1539\n",
      "Epoch [13/25], Step [176/590], Loss: 0.2026\n",
      "Epoch [13/25], Step [180/590], Loss: 0.2815\n",
      "Epoch [13/25], Step [184/590], Loss: 0.1440\n",
      "Epoch [13/25], Step [188/590], Loss: 16.0860\n",
      "Epoch [13/25], Step [192/590], Loss: 0.1578\n",
      "Epoch [13/25], Step [196/590], Loss: 0.0586\n",
      "Epoch [13/25], Step [200/590], Loss: 0.4169\n",
      "Epoch [13/25], Step [204/590], Loss: 0.2745\n",
      "Epoch [13/25], Step [208/590], Loss: 0.0926\n",
      "Epoch [13/25], Step [212/590], Loss: 0.1547\n",
      "Epoch [13/25], Step [216/590], Loss: 0.1259\n",
      "Epoch [13/25], Step [220/590], Loss: 0.5087\n",
      "Epoch [13/25], Step [224/590], Loss: 0.0455\n",
      "Epoch [13/25], Step [228/590], Loss: 0.6238\n",
      "Epoch [13/25], Step [232/590], Loss: 0.1642\n",
      "Epoch [13/25], Step [236/590], Loss: 0.0806\n",
      "Epoch [13/25], Step [240/590], Loss: 0.1113\n",
      "Epoch [13/25], Step [244/590], Loss: 0.3758\n",
      "Epoch [13/25], Step [248/590], Loss: 0.2195\n",
      "Epoch [13/25], Step [252/590], Loss: 0.0460\n",
      "Epoch [13/25], Step [256/590], Loss: 0.0577\n",
      "Epoch [13/25], Step [260/590], Loss: 0.6846\n",
      "Epoch [13/25], Step [264/590], Loss: 0.0998\n",
      "Epoch [13/25], Step [268/590], Loss: 0.1343\n",
      "Epoch [13/25], Step [272/590], Loss: 0.2123\n",
      "Epoch [13/25], Step [276/590], Loss: 0.8338\n",
      "Epoch [13/25], Step [280/590], Loss: 0.4536\n",
      "Epoch [13/25], Step [284/590], Loss: 1.1872\n",
      "Epoch [13/25], Step [288/590], Loss: 0.6737\n",
      "Epoch [13/25], Step [292/590], Loss: 0.4896\n",
      "Epoch [13/25], Step [296/590], Loss: 0.3119\n",
      "Epoch [13/25], Step [300/590], Loss: 0.0188\n",
      "Epoch [13/25], Step [304/590], Loss: 0.1437\n",
      "Epoch [13/25], Step [308/590], Loss: 0.0516\n",
      "Epoch [13/25], Step [312/590], Loss: 0.2768\n",
      "Epoch [13/25], Step [316/590], Loss: 1.0875\n",
      "Epoch [13/25], Step [320/590], Loss: 0.9536\n",
      "Epoch [13/25], Step [324/590], Loss: 0.0622\n",
      "Epoch [13/25], Step [328/590], Loss: 0.0452\n",
      "Epoch [13/25], Step [332/590], Loss: 0.3962\n",
      "Epoch [13/25], Step [336/590], Loss: 0.0264\n",
      "Epoch [13/25], Step [340/590], Loss: 0.1963\n",
      "Epoch [13/25], Step [344/590], Loss: 1.4961\n",
      "Epoch [13/25], Step [348/590], Loss: 5.1115\n",
      "Epoch [13/25], Step [352/590], Loss: 1.0549\n",
      "Epoch [13/25], Step [356/590], Loss: 0.0815\n",
      "Epoch [13/25], Step [360/590], Loss: 3.6633\n",
      "Epoch [13/25], Step [364/590], Loss: 0.3332\n",
      "Epoch [13/25], Step [368/590], Loss: 3.3440\n",
      "Epoch [13/25], Step [372/590], Loss: 0.2383\n",
      "Epoch [13/25], Step [376/590], Loss: 2.7878\n",
      "Epoch [13/25], Step [380/590], Loss: 1.9356\n",
      "Epoch [13/25], Step [384/590], Loss: 0.2707\n",
      "Epoch [13/25], Step [388/590], Loss: 0.3827\n",
      "Epoch [13/25], Step [392/590], Loss: 2.7072\n",
      "Epoch [13/25], Step [396/590], Loss: 0.4782\n",
      "Epoch [13/25], Step [400/590], Loss: 1.5630\n",
      "Epoch [13/25], Step [404/590], Loss: 0.0772\n",
      "Epoch [13/25], Step [408/590], Loss: 2.5308\n",
      "Epoch [13/25], Step [412/590], Loss: 1.7891\n",
      "Epoch [13/25], Step [416/590], Loss: 1.0588\n",
      "Epoch [13/25], Step [420/590], Loss: 0.0397\n",
      "Epoch [13/25], Step [424/590], Loss: 0.8295\n",
      "Epoch [13/25], Step [428/590], Loss: 6.8394\n",
      "Epoch [13/25], Step [432/590], Loss: 0.4204\n",
      "Epoch [13/25], Step [436/590], Loss: 1.3088\n",
      "Epoch [13/25], Step [440/590], Loss: 1.5203\n",
      "Epoch [13/25], Step [444/590], Loss: 3.0419\n",
      "Epoch [13/25], Step [448/590], Loss: 0.3484\n",
      "Epoch [13/25], Step [452/590], Loss: 1.2017\n",
      "Epoch [13/25], Step [456/590], Loss: 5.1688\n",
      "Epoch [13/25], Step [460/590], Loss: 1.0638\n",
      "Epoch [13/25], Step [464/590], Loss: 1.4338\n",
      "Epoch [13/25], Step [468/590], Loss: 1.7888\n",
      "Epoch [13/25], Step [472/590], Loss: 0.2674\n",
      "Epoch [13/25], Step [476/590], Loss: 2.3209\n",
      "Epoch [13/25], Step [480/590], Loss: 0.7199\n",
      "Epoch [13/25], Step [484/590], Loss: 1.0545\n",
      "Epoch [13/25], Step [488/590], Loss: 1.2386\n",
      "Epoch [13/25], Step [492/590], Loss: 2.4139\n",
      "Epoch [13/25], Step [496/590], Loss: 0.7982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Step [500/590], Loss: 2.0329\n",
      "Epoch [13/25], Step [504/590], Loss: 0.3468\n",
      "Epoch [13/25], Step [508/590], Loss: 0.0499\n",
      "Epoch [13/25], Step [512/590], Loss: 0.2712\n",
      "Epoch [13/25], Step [516/590], Loss: 0.3096\n",
      "Epoch [13/25], Step [520/590], Loss: 0.6004\n",
      "Epoch [13/25], Step [524/590], Loss: 0.9961\n",
      "Epoch [13/25], Step [528/590], Loss: 0.1460\n",
      "Epoch [13/25], Step [532/590], Loss: 0.0714\n",
      "Epoch [13/25], Step [536/590], Loss: 0.3787\n",
      "Epoch [13/25], Step [540/590], Loss: 1.6951\n",
      "Epoch [13/25], Step [544/590], Loss: 1.3761\n",
      "Epoch [13/25], Step [548/590], Loss: 1.0746\n",
      "Epoch [13/25], Step [552/590], Loss: 0.5512\n",
      "Epoch [13/25], Step [556/590], Loss: 3.9748\n",
      "Epoch [13/25], Step [560/590], Loss: 1.0831\n",
      "Epoch [13/25], Step [564/590], Loss: 1.0797\n",
      "Epoch [13/25], Step [568/590], Loss: 0.3392\n",
      "Epoch [13/25], Step [572/590], Loss: 3.4923\n",
      "Epoch [13/25], Step [576/590], Loss: 0.2681\n",
      "Epoch [13/25], Step [580/590], Loss: 1.0210\n",
      "Epoch [13/25], Step [584/590], Loss: 1.1901\n",
      "Epoch [13/25], Step [588/590], Loss: 2.0974\n",
      "epoch 13\n",
      "Epoch [14/25], Step [4/590], Loss: 0.1409\n",
      "Epoch [14/25], Step [8/590], Loss: 2.8142\n",
      "Epoch [14/25], Step [12/590], Loss: 0.0967\n",
      "Epoch [14/25], Step [16/590], Loss: 0.7215\n",
      "Epoch [14/25], Step [20/590], Loss: 0.2235\n",
      "Epoch [14/25], Step [24/590], Loss: 4.7139\n",
      "Epoch [14/25], Step [28/590], Loss: 4.1080\n",
      "Epoch [14/25], Step [32/590], Loss: 0.5395\n",
      "Epoch [14/25], Step [36/590], Loss: 0.0591\n",
      "Epoch [14/25], Step [40/590], Loss: 0.0719\n",
      "Epoch [14/25], Step [44/590], Loss: 0.1907\n",
      "Epoch [14/25], Step [48/590], Loss: 0.0508\n",
      "Epoch [14/25], Step [52/590], Loss: 6.1537\n",
      "Epoch [14/25], Step [56/590], Loss: 0.6457\n",
      "Epoch [14/25], Step [60/590], Loss: 0.1006\n",
      "Epoch [14/25], Step [64/590], Loss: 0.4367\n",
      "Epoch [14/25], Step [68/590], Loss: 0.1778\n",
      "Epoch [14/25], Step [72/590], Loss: 0.1774\n",
      "Epoch [14/25], Step [76/590], Loss: 6.8823\n",
      "Epoch [14/25], Step [80/590], Loss: 0.4355\n",
      "Epoch [14/25], Step [84/590], Loss: 0.0406\n",
      "Epoch [14/25], Step [88/590], Loss: 1.0041\n",
      "Epoch [14/25], Step [92/590], Loss: 1.0985\n",
      "Epoch [14/25], Step [96/590], Loss: 2.6894\n",
      "Epoch [14/25], Step [100/590], Loss: 0.1445\n",
      "Epoch [14/25], Step [104/590], Loss: 1.3473\n",
      "Epoch [14/25], Step [108/590], Loss: 0.2198\n",
      "Epoch [14/25], Step [112/590], Loss: 1.5151\n",
      "Epoch [14/25], Step [116/590], Loss: 2.5745\n",
      "Epoch [14/25], Step [120/590], Loss: 2.0987\n",
      "Epoch [14/25], Step [124/590], Loss: 0.9220\n",
      "Epoch [14/25], Step [128/590], Loss: 2.5969\n",
      "Epoch [14/25], Step [132/590], Loss: 2.2530\n",
      "Epoch [14/25], Step [136/590], Loss: 0.4532\n",
      "Epoch [14/25], Step [140/590], Loss: 5.2539\n",
      "Epoch [14/25], Step [144/590], Loss: 5.1729\n",
      "Epoch [14/25], Step [148/590], Loss: 8.2442\n",
      "Epoch [14/25], Step [152/590], Loss: 0.8689\n",
      "Epoch [14/25], Step [156/590], Loss: 1.4646\n",
      "Epoch [14/25], Step [160/590], Loss: 0.5819\n",
      "Epoch [14/25], Step [164/590], Loss: 0.1119\n",
      "Epoch [14/25], Step [168/590], Loss: 0.1179\n",
      "Epoch [14/25], Step [172/590], Loss: 0.7892\n",
      "Epoch [14/25], Step [176/590], Loss: 0.1551\n",
      "Epoch [14/25], Step [180/590], Loss: 0.1177\n",
      "Epoch [14/25], Step [184/590], Loss: 0.6029\n",
      "Epoch [14/25], Step [188/590], Loss: 0.0852\n",
      "Epoch [14/25], Step [192/590], Loss: 1.1594\n",
      "Epoch [14/25], Step [196/590], Loss: 0.3207\n",
      "Epoch [14/25], Step [200/590], Loss: 0.1374\n",
      "Epoch [14/25], Step [204/590], Loss: 0.0869\n",
      "Epoch [14/25], Step [208/590], Loss: 0.8174\n",
      "Epoch [14/25], Step [212/590], Loss: 0.5678\n",
      "Epoch [14/25], Step [216/590], Loss: 0.1167\n",
      "Epoch [14/25], Step [220/590], Loss: 0.1372\n",
      "Epoch [14/25], Step [224/590], Loss: 0.1285\n",
      "Epoch [14/25], Step [228/590], Loss: 0.2332\n",
      "Epoch [14/25], Step [232/590], Loss: 1.1354\n",
      "Epoch [14/25], Step [236/590], Loss: 0.2162\n",
      "Epoch [14/25], Step [240/590], Loss: 0.3079\n",
      "Epoch [14/25], Step [244/590], Loss: 0.1702\n",
      "Epoch [14/25], Step [248/590], Loss: 0.0962\n",
      "Epoch [14/25], Step [252/590], Loss: 0.1497\n",
      "Epoch [14/25], Step [256/590], Loss: 3.0933\n",
      "Epoch [14/25], Step [260/590], Loss: 0.5384\n",
      "Epoch [14/25], Step [264/590], Loss: 0.3015\n",
      "Epoch [14/25], Step [268/590], Loss: 0.4233\n",
      "Epoch [14/25], Step [272/590], Loss: 0.5725\n",
      "Epoch [14/25], Step [276/590], Loss: 0.0395\n",
      "Epoch [14/25], Step [280/590], Loss: 0.4805\n",
      "Epoch [14/25], Step [284/590], Loss: 1.2524\n",
      "Epoch [14/25], Step [288/590], Loss: 0.6499\n",
      "Epoch [14/25], Step [292/590], Loss: 4.2768\n",
      "Epoch [14/25], Step [296/590], Loss: 0.2024\n",
      "Epoch [14/25], Step [300/590], Loss: 0.0038\n",
      "Epoch [14/25], Step [304/590], Loss: 0.0220\n",
      "Epoch [14/25], Step [308/590], Loss: 0.0547\n",
      "Epoch [14/25], Step [312/590], Loss: 0.0596\n",
      "Epoch [14/25], Step [316/590], Loss: 0.5981\n",
      "Epoch [14/25], Step [320/590], Loss: 0.0697\n",
      "Epoch [14/25], Step [324/590], Loss: 0.3186\n",
      "Epoch [14/25], Step [328/590], Loss: 0.1713\n",
      "Epoch [14/25], Step [332/590], Loss: 0.2356\n",
      "Epoch [14/25], Step [336/590], Loss: 0.0168\n",
      "Epoch [14/25], Step [340/590], Loss: 0.0628\n",
      "Epoch [14/25], Step [344/590], Loss: 0.0113\n",
      "Epoch [14/25], Step [348/590], Loss: 1.8359\n",
      "Epoch [14/25], Step [352/590], Loss: 0.0752\n",
      "Epoch [14/25], Step [356/590], Loss: 8.1217\n",
      "Epoch [14/25], Step [360/590], Loss: 1.2440\n",
      "Epoch [14/25], Step [364/590], Loss: 0.5883\n",
      "Epoch [14/25], Step [368/590], Loss: 4.2434\n",
      "Epoch [14/25], Step [372/590], Loss: 0.2480\n",
      "Epoch [14/25], Step [376/590], Loss: 3.4356\n",
      "Epoch [14/25], Step [380/590], Loss: 0.5829\n",
      "Epoch [14/25], Step [384/590], Loss: 0.3530\n",
      "Epoch [14/25], Step [388/590], Loss: 1.9622\n",
      "Epoch [14/25], Step [392/590], Loss: 0.7381\n",
      "Epoch [14/25], Step [396/590], Loss: 5.3468\n",
      "Epoch [14/25], Step [400/590], Loss: 0.5462\n",
      "Epoch [14/25], Step [404/590], Loss: 0.3081\n",
      "Epoch [14/25], Step [408/590], Loss: 2.4508\n",
      "Epoch [14/25], Step [412/590], Loss: 2.3362\n",
      "Epoch [14/25], Step [416/590], Loss: 3.3588\n",
      "Epoch [14/25], Step [420/590], Loss: 0.0501\n",
      "Epoch [14/25], Step [424/590], Loss: 0.8492\n",
      "Epoch [14/25], Step [428/590], Loss: 0.6489\n",
      "Epoch [14/25], Step [432/590], Loss: 0.3635\n",
      "Epoch [14/25], Step [436/590], Loss: 0.2710\n",
      "Epoch [14/25], Step [440/590], Loss: 4.4175\n",
      "Epoch [14/25], Step [444/590], Loss: 0.4397\n",
      "Epoch [14/25], Step [448/590], Loss: 1.1029\n",
      "Epoch [14/25], Step [452/590], Loss: 2.0624\n",
      "Epoch [14/25], Step [456/590], Loss: 0.0770\n",
      "Epoch [14/25], Step [460/590], Loss: 1.0157\n",
      "Epoch [14/25], Step [464/590], Loss: 1.9791\n",
      "Epoch [14/25], Step [468/590], Loss: 2.8286\n",
      "Epoch [14/25], Step [472/590], Loss: 4.6420\n",
      "Epoch [14/25], Step [476/590], Loss: 0.3810\n",
      "Epoch [14/25], Step [480/590], Loss: 3.1143\n",
      "Epoch [14/25], Step [484/590], Loss: 6.9631\n",
      "Epoch [14/25], Step [488/590], Loss: 4.4768\n",
      "Epoch [14/25], Step [492/590], Loss: 1.4822\n",
      "Epoch [14/25], Step [496/590], Loss: 0.2480\n",
      "Epoch [14/25], Step [500/590], Loss: 3.4974\n",
      "Epoch [14/25], Step [504/590], Loss: 4.0026\n",
      "Epoch [14/25], Step [508/590], Loss: 0.2669\n",
      "Epoch [14/25], Step [512/590], Loss: 0.9757\n",
      "Epoch [14/25], Step [516/590], Loss: 0.9606\n",
      "Epoch [14/25], Step [520/590], Loss: 0.1822\n",
      "Epoch [14/25], Step [524/590], Loss: 2.0652\n",
      "Epoch [14/25], Step [528/590], Loss: 0.6379\n",
      "Epoch [14/25], Step [532/590], Loss: 0.3046\n",
      "Epoch [14/25], Step [536/590], Loss: 0.3431\n",
      "Epoch [14/25], Step [540/590], Loss: 1.2404\n",
      "Epoch [14/25], Step [544/590], Loss: 0.0973\n",
      "Epoch [14/25], Step [548/590], Loss: 0.7248\n",
      "Epoch [14/25], Step [552/590], Loss: 5.9274\n",
      "Epoch [14/25], Step [556/590], Loss: 0.5766\n",
      "Epoch [14/25], Step [560/590], Loss: 1.2977\n",
      "Epoch [14/25], Step [564/590], Loss: 6.0022\n",
      "Epoch [14/25], Step [568/590], Loss: 0.3924\n",
      "Epoch [14/25], Step [572/590], Loss: 5.4032\n",
      "Epoch [14/25], Step [576/590], Loss: 0.0933\n",
      "Epoch [14/25], Step [580/590], Loss: 1.7085\n",
      "Epoch [14/25], Step [584/590], Loss: 6.3115\n",
      "Epoch [14/25], Step [588/590], Loss: 2.8543\n",
      "epoch 14\n",
      "Epoch [15/25], Step [4/590], Loss: 0.1365\n",
      "Epoch [15/25], Step [8/590], Loss: 1.4514\n",
      "Epoch [15/25], Step [12/590], Loss: 0.9482\n",
      "Epoch [15/25], Step [16/590], Loss: 4.3372\n",
      "Epoch [15/25], Step [20/590], Loss: 0.7670\n",
      "Epoch [15/25], Step [24/590], Loss: 0.1022\n",
      "Epoch [15/25], Step [28/590], Loss: 3.8364\n",
      "Epoch [15/25], Step [32/590], Loss: 0.2897\n",
      "Epoch [15/25], Step [36/590], Loss: 2.7116\n",
      "Epoch [15/25], Step [40/590], Loss: 2.4668\n",
      "Epoch [15/25], Step [44/590], Loss: 4.3878\n",
      "Epoch [15/25], Step [48/590], Loss: 2.1090\n",
      "Epoch [15/25], Step [52/590], Loss: 3.0840\n",
      "Epoch [15/25], Step [56/590], Loss: 3.1985\n",
      "Epoch [15/25], Step [60/590], Loss: 0.2573\n",
      "Epoch [15/25], Step [64/590], Loss: 0.3462\n",
      "Epoch [15/25], Step [68/590], Loss: 2.6122\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Step [72/590], Loss: 1.8240\n",
      "Epoch [15/25], Step [76/590], Loss: 1.0467\n",
      "Epoch [15/25], Step [80/590], Loss: 3.9232\n",
      "Epoch [15/25], Step [84/590], Loss: 0.3681\n",
      "Epoch [15/25], Step [88/590], Loss: 2.6950\n",
      "Epoch [15/25], Step [92/590], Loss: 0.5255\n",
      "Epoch [15/25], Step [96/590], Loss: 1.9928\n",
      "Epoch [15/25], Step [100/590], Loss: 0.9675\n",
      "Epoch [15/25], Step [104/590], Loss: 1.0502\n",
      "Epoch [15/25], Step [108/590], Loss: 0.2962\n",
      "Epoch [15/25], Step [112/590], Loss: 0.3677\n",
      "Epoch [15/25], Step [116/590], Loss: 0.2923\n",
      "Epoch [15/25], Step [120/590], Loss: 2.9975\n",
      "Epoch [15/25], Step [124/590], Loss: 1.8771\n",
      "Epoch [15/25], Step [128/590], Loss: 0.7857\n",
      "Epoch [15/25], Step [132/590], Loss: 4.4750\n",
      "Epoch [15/25], Step [136/590], Loss: 0.7053\n",
      "Epoch [15/25], Step [140/590], Loss: 0.0696\n",
      "Epoch [15/25], Step [144/590], Loss: 0.2419\n",
      "Epoch [15/25], Step [148/590], Loss: 1.3479\n",
      "Epoch [15/25], Step [152/590], Loss: 1.3161\n",
      "Epoch [15/25], Step [156/590], Loss: 0.6799\n",
      "Epoch [15/25], Step [160/590], Loss: 0.6933\n",
      "Epoch [15/25], Step [164/590], Loss: 0.0676\n",
      "Epoch [15/25], Step [168/590], Loss: 4.4527\n",
      "Epoch [15/25], Step [172/590], Loss: 1.0202\n",
      "Epoch [15/25], Step [176/590], Loss: 0.1708\n",
      "Epoch [15/25], Step [180/590], Loss: 0.1302\n",
      "Epoch [15/25], Step [184/590], Loss: 0.0629\n",
      "Epoch [15/25], Step [188/590], Loss: 0.1872\n",
      "Epoch [15/25], Step [192/590], Loss: 0.4115\n",
      "Epoch [15/25], Step [196/590], Loss: 0.4479\n",
      "Epoch [15/25], Step [200/590], Loss: 14.7720\n",
      "Epoch [15/25], Step [204/590], Loss: 0.8816\n",
      "Epoch [15/25], Step [208/590], Loss: 0.1981\n",
      "Epoch [15/25], Step [212/590], Loss: 0.3570\n",
      "Epoch [15/25], Step [216/590], Loss: 0.1897\n",
      "Epoch [15/25], Step [220/590], Loss: 0.1978\n",
      "Epoch [15/25], Step [224/590], Loss: 2.4153\n",
      "Epoch [15/25], Step [228/590], Loss: 0.1016\n",
      "Epoch [15/25], Step [232/590], Loss: 0.2343\n",
      "Epoch [15/25], Step [236/590], Loss: 2.1917\n",
      "Epoch [15/25], Step [240/590], Loss: 0.3841\n",
      "Epoch [15/25], Step [244/590], Loss: 0.0268\n",
      "Epoch [15/25], Step [248/590], Loss: 0.3641\n",
      "Epoch [15/25], Step [252/590], Loss: 0.1178\n",
      "Epoch [15/25], Step [256/590], Loss: 0.3790\n",
      "Epoch [15/25], Step [260/590], Loss: 0.0505\n",
      "Epoch [15/25], Step [264/590], Loss: 0.0991\n",
      "Epoch [15/25], Step [268/590], Loss: 0.1034\n",
      "Epoch [15/25], Step [272/590], Loss: 0.4130\n",
      "Epoch [15/25], Step [276/590], Loss: 5.1427\n",
      "Epoch [15/25], Step [280/590], Loss: 0.0911\n",
      "Epoch [15/25], Step [284/590], Loss: 0.0587\n",
      "Epoch [15/25], Step [288/590], Loss: 0.1078\n",
      "Epoch [15/25], Step [292/590], Loss: 0.2559\n",
      "Epoch [15/25], Step [296/590], Loss: 0.0604\n",
      "Epoch [15/25], Step [300/590], Loss: 1.3344\n",
      "Epoch [15/25], Step [304/590], Loss: 0.0466\n",
      "Epoch [15/25], Step [308/590], Loss: 3.6518\n",
      "Epoch [15/25], Step [312/590], Loss: 0.0968\n",
      "Epoch [15/25], Step [316/590], Loss: 0.0856\n",
      "Epoch [15/25], Step [320/590], Loss: 0.1399\n",
      "Epoch [15/25], Step [324/590], Loss: 0.6451\n",
      "Epoch [15/25], Step [328/590], Loss: 0.1888\n",
      "Epoch [15/25], Step [332/590], Loss: 0.2879\n",
      "Epoch [15/25], Step [336/590], Loss: 0.1596\n",
      "Epoch [15/25], Step [340/590], Loss: 0.4085\n",
      "Epoch [15/25], Step [344/590], Loss: 0.1613\n",
      "Epoch [15/25], Step [348/590], Loss: 0.0416\n",
      "Epoch [15/25], Step [352/590], Loss: 0.4417\n",
      "Epoch [15/25], Step [356/590], Loss: 0.3101\n",
      "Epoch [15/25], Step [360/590], Loss: 0.0650\n",
      "Epoch [15/25], Step [364/590], Loss: 0.1562\n",
      "Epoch [15/25], Step [368/590], Loss: 0.3537\n",
      "Epoch [15/25], Step [372/590], Loss: 0.0824\n",
      "Epoch [15/25], Step [376/590], Loss: 0.5224\n",
      "Epoch [15/25], Step [380/590], Loss: 0.2195\n",
      "Epoch [15/25], Step [384/590], Loss: 0.1751\n",
      "Epoch [15/25], Step [388/590], Loss: 0.3983\n",
      "Epoch [15/25], Step [392/590], Loss: 0.0209\n",
      "Epoch [15/25], Step [396/590], Loss: 0.7064\n",
      "Epoch [15/25], Step [400/590], Loss: 0.3255\n",
      "Epoch [15/25], Step [404/590], Loss: 0.1741\n",
      "Epoch [15/25], Step [408/590], Loss: 0.0426\n",
      "Epoch [15/25], Step [412/590], Loss: 0.0723\n",
      "Epoch [15/25], Step [416/590], Loss: 0.0318\n",
      "Epoch [15/25], Step [420/590], Loss: 0.5895\n",
      "Epoch [15/25], Step [424/590], Loss: 2.9925\n",
      "Epoch [15/25], Step [428/590], Loss: 0.0653\n",
      "Epoch [15/25], Step [432/590], Loss: 0.0536\n",
      "Epoch [15/25], Step [436/590], Loss: 0.8235\n",
      "Epoch [15/25], Step [440/590], Loss: 0.0054\n",
      "Epoch [15/25], Step [444/590], Loss: 0.2543\n",
      "Epoch [15/25], Step [448/590], Loss: 0.5404\n",
      "Epoch [15/25], Step [452/590], Loss: 0.1974\n",
      "Epoch [15/25], Step [456/590], Loss: 0.1793\n",
      "Epoch [15/25], Step [460/590], Loss: 0.4825\n",
      "Epoch [15/25], Step [464/590], Loss: 0.5170\n",
      "Epoch [15/25], Step [468/590], Loss: 0.1945\n",
      "Epoch [15/25], Step [472/590], Loss: 0.1400\n",
      "Epoch [15/25], Step [476/590], Loss: 0.6030\n",
      "Epoch [15/25], Step [480/590], Loss: 0.7740\n",
      "Epoch [15/25], Step [484/590], Loss: 0.0291\n",
      "Epoch [15/25], Step [488/590], Loss: 0.2791\n",
      "Epoch [15/25], Step [492/590], Loss: 0.1188\n",
      "Epoch [15/25], Step [496/590], Loss: 2.2714\n",
      "Epoch [15/25], Step [500/590], Loss: 4.5837\n",
      "Epoch [15/25], Step [504/590], Loss: 0.1196\n",
      "Epoch [15/25], Step [508/590], Loss: 2.0294\n",
      "Epoch [15/25], Step [512/590], Loss: 1.1793\n",
      "Epoch [15/25], Step [516/590], Loss: 1.1362\n",
      "Epoch [15/25], Step [520/590], Loss: 2.1445\n",
      "Epoch [15/25], Step [524/590], Loss: 0.6096\n",
      "Epoch [15/25], Step [528/590], Loss: 0.6893\n",
      "Epoch [15/25], Step [532/590], Loss: 0.2675\n",
      "Epoch [15/25], Step [536/590], Loss: 3.9913\n",
      "Epoch [15/25], Step [540/590], Loss: 0.4316\n",
      "Epoch [15/25], Step [544/590], Loss: 0.6435\n",
      "Epoch [15/25], Step [548/590], Loss: 0.1951\n",
      "Epoch [15/25], Step [552/590], Loss: 0.4969\n",
      "Epoch [15/25], Step [556/590], Loss: 0.6314\n",
      "Epoch [15/25], Step [560/590], Loss: 0.0687\n",
      "Epoch [15/25], Step [564/590], Loss: 0.5425\n",
      "Epoch [15/25], Step [568/590], Loss: 2.9053\n",
      "Epoch [15/25], Step [572/590], Loss: 0.4114\n",
      "Epoch [15/25], Step [576/590], Loss: 2.3257\n",
      "Epoch [15/25], Step [580/590], Loss: 0.3431\n",
      "Epoch [15/25], Step [584/590], Loss: 3.1470\n",
      "Epoch [15/25], Step [588/590], Loss: 2.7404\n",
      "epoch 15\n",
      "Epoch [16/25], Step [4/590], Loss: 0.1290\n",
      "Epoch [16/25], Step [8/590], Loss: 0.5594\n",
      "Epoch [16/25], Step [12/590], Loss: 8.7091\n",
      "Epoch [16/25], Step [16/590], Loss: 0.2616\n",
      "Epoch [16/25], Step [20/590], Loss: 0.5396\n",
      "Epoch [16/25], Step [24/590], Loss: 0.3466\n",
      "Epoch [16/25], Step [28/590], Loss: 1.2845\n",
      "Epoch [16/25], Step [32/590], Loss: 0.5207\n",
      "Epoch [16/25], Step [36/590], Loss: 0.1259\n",
      "Epoch [16/25], Step [40/590], Loss: 0.0223\n",
      "Epoch [16/25], Step [44/590], Loss: 0.2310\n",
      "Epoch [16/25], Step [48/590], Loss: 4.7954\n",
      "Epoch [16/25], Step [52/590], Loss: 0.1529\n",
      "Epoch [16/25], Step [56/590], Loss: 0.3639\n",
      "Epoch [16/25], Step [60/590], Loss: 0.6289\n",
      "Epoch [16/25], Step [64/590], Loss: 0.1937\n",
      "Epoch [16/25], Step [68/590], Loss: 0.7366\n",
      "Epoch [16/25], Step [72/590], Loss: 0.0217\n",
      "Epoch [16/25], Step [76/590], Loss: 1.2269\n",
      "Epoch [16/25], Step [80/590], Loss: 0.0967\n",
      "Epoch [16/25], Step [84/590], Loss: 0.2077\n",
      "Epoch [16/25], Step [88/590], Loss: 0.0645\n",
      "Epoch [16/25], Step [92/590], Loss: 0.0212\n",
      "Epoch [16/25], Step [96/590], Loss: 0.0409\n",
      "Epoch [16/25], Step [100/590], Loss: 0.2309\n",
      "Epoch [16/25], Step [104/590], Loss: 0.3917\n",
      "Epoch [16/25], Step [108/590], Loss: 0.2087\n",
      "Epoch [16/25], Step [112/590], Loss: 4.3320\n",
      "Epoch [16/25], Step [116/590], Loss: 0.1575\n",
      "Epoch [16/25], Step [120/590], Loss: 0.1589\n",
      "Epoch [16/25], Step [124/590], Loss: 0.2259\n",
      "Epoch [16/25], Step [128/590], Loss: 0.0760\n",
      "Epoch [16/25], Step [132/590], Loss: 0.2184\n",
      "Epoch [16/25], Step [136/590], Loss: 0.1999\n",
      "Epoch [16/25], Step [140/590], Loss: 0.8636\n",
      "Epoch [16/25], Step [144/590], Loss: 0.1307\n",
      "Epoch [16/25], Step [148/590], Loss: 0.1709\n",
      "Epoch [16/25], Step [152/590], Loss: 0.2217\n",
      "Epoch [16/25], Step [156/590], Loss: 0.1562\n",
      "Epoch [16/25], Step [160/590], Loss: 1.4559\n",
      "Epoch [16/25], Step [164/590], Loss: 0.2786\n",
      "Epoch [16/25], Step [168/590], Loss: 0.0685\n",
      "Epoch [16/25], Step [172/590], Loss: 1.0115\n",
      "Epoch [16/25], Step [176/590], Loss: 0.8626\n",
      "Epoch [16/25], Step [180/590], Loss: 3.7956\n",
      "Epoch [16/25], Step [184/590], Loss: 2.6666\n",
      "Epoch [16/25], Step [188/590], Loss: 1.4065\n",
      "Epoch [16/25], Step [192/590], Loss: 0.0600\n",
      "Epoch [16/25], Step [196/590], Loss: 0.0603\n",
      "Epoch [16/25], Step [200/590], Loss: 0.0430\n",
      "Epoch [16/25], Step [204/590], Loss: 1.3279\n",
      "Epoch [16/25], Step [208/590], Loss: 0.0540\n",
      "Epoch [16/25], Step [212/590], Loss: 0.2501\n",
      "Epoch [16/25], Step [216/590], Loss: 0.3387\n",
      "Epoch [16/25], Step [220/590], Loss: 1.0659\n",
      "Epoch [16/25], Step [224/590], Loss: 3.5079\n",
      "Epoch [16/25], Step [228/590], Loss: 0.1504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Step [232/590], Loss: 0.0827\n",
      "Epoch [16/25], Step [236/590], Loss: 0.3360\n",
      "Epoch [16/25], Step [240/590], Loss: 0.2410\n",
      "Epoch [16/25], Step [244/590], Loss: 0.2185\n",
      "Epoch [16/25], Step [248/590], Loss: 1.0330\n",
      "Epoch [16/25], Step [252/590], Loss: 0.1288\n",
      "Epoch [16/25], Step [256/590], Loss: 0.9099\n",
      "Epoch [16/25], Step [260/590], Loss: 1.9354\n",
      "Epoch [16/25], Step [264/590], Loss: 0.3325\n",
      "Epoch [16/25], Step [268/590], Loss: 0.6583\n",
      "Epoch [16/25], Step [272/590], Loss: 3.0841\n",
      "Epoch [16/25], Step [276/590], Loss: 0.5646\n",
      "Epoch [16/25], Step [280/590], Loss: 0.2929\n",
      "Epoch [16/25], Step [284/590], Loss: 0.2499\n",
      "Epoch [16/25], Step [288/590], Loss: 0.4909\n",
      "Epoch [16/25], Step [292/590], Loss: 0.0235\n",
      "Epoch [16/25], Step [296/590], Loss: 0.3127\n",
      "Epoch [16/25], Step [300/590], Loss: 0.9730\n",
      "Epoch [16/25], Step [304/590], Loss: 0.2682\n",
      "Epoch [16/25], Step [308/590], Loss: 5.6054\n",
      "Epoch [16/25], Step [312/590], Loss: 0.1381\n",
      "Epoch [16/25], Step [316/590], Loss: 0.0185\n",
      "Epoch [16/25], Step [320/590], Loss: 0.0525\n",
      "Epoch [16/25], Step [324/590], Loss: 0.1612\n",
      "Epoch [16/25], Step [328/590], Loss: 0.1274\n",
      "Epoch [16/25], Step [332/590], Loss: 0.9550\n",
      "Epoch [16/25], Step [336/590], Loss: 6.0758\n",
      "Epoch [16/25], Step [340/590], Loss: 0.3491\n",
      "Epoch [16/25], Step [344/590], Loss: 0.8066\n",
      "Epoch [16/25], Step [348/590], Loss: 0.0770\n",
      "Epoch [16/25], Step [352/590], Loss: 1.4183\n",
      "Epoch [16/25], Step [356/590], Loss: 0.1982\n",
      "Epoch [16/25], Step [360/590], Loss: 0.2198\n",
      "Epoch [16/25], Step [364/590], Loss: 0.9568\n",
      "Epoch [16/25], Step [368/590], Loss: 0.1903\n",
      "Epoch [16/25], Step [372/590], Loss: 0.0657\n",
      "Epoch [16/25], Step [376/590], Loss: 0.3350\n",
      "Epoch [16/25], Step [380/590], Loss: 0.6616\n",
      "Epoch [16/25], Step [384/590], Loss: 0.0694\n",
      "Epoch [16/25], Step [388/590], Loss: 0.0470\n",
      "Epoch [16/25], Step [392/590], Loss: 0.0499\n",
      "Epoch [16/25], Step [396/590], Loss: 0.0798\n",
      "Epoch [16/25], Step [400/590], Loss: 0.1067\n",
      "Epoch [16/25], Step [404/590], Loss: 0.0504\n",
      "Epoch [16/25], Step [408/590], Loss: 0.2877\n",
      "Epoch [16/25], Step [412/590], Loss: 0.0084\n",
      "Epoch [16/25], Step [416/590], Loss: 0.1015\n",
      "Epoch [16/25], Step [420/590], Loss: 0.1424\n",
      "Epoch [16/25], Step [424/590], Loss: 3.0645\n",
      "Epoch [16/25], Step [428/590], Loss: 5.9875\n",
      "Epoch [16/25], Step [432/590], Loss: 1.9100\n",
      "Epoch [16/25], Step [436/590], Loss: 0.0297\n",
      "Epoch [16/25], Step [440/590], Loss: 11.0795\n",
      "Epoch [16/25], Step [444/590], Loss: 0.1544\n",
      "Epoch [16/25], Step [448/590], Loss: 0.8032\n",
      "Epoch [16/25], Step [452/590], Loss: 0.6855\n",
      "Epoch [16/25], Step [456/590], Loss: 0.3638\n",
      "Epoch [16/25], Step [460/590], Loss: 3.8027\n",
      "Epoch [16/25], Step [464/590], Loss: 0.8532\n",
      "Epoch [16/25], Step [468/590], Loss: 0.1363\n",
      "Epoch [16/25], Step [472/590], Loss: 1.9465\n",
      "Epoch [16/25], Step [476/590], Loss: 3.0832\n",
      "Epoch [16/25], Step [480/590], Loss: 0.0166\n",
      "Epoch [16/25], Step [484/590], Loss: 0.0277\n",
      "Epoch [16/25], Step [488/590], Loss: 0.3938\n",
      "Epoch [16/25], Step [492/590], Loss: 1.2588\n",
      "Epoch [16/25], Step [496/590], Loss: 0.8090\n",
      "Epoch [16/25], Step [500/590], Loss: 0.2383\n",
      "Epoch [16/25], Step [504/590], Loss: 0.1032\n",
      "Epoch [16/25], Step [508/590], Loss: 0.0793\n",
      "Epoch [16/25], Step [512/590], Loss: 0.0377\n",
      "Epoch [16/25], Step [516/590], Loss: 0.1416\n",
      "Epoch [16/25], Step [520/590], Loss: 2.2976\n",
      "Epoch [16/25], Step [524/590], Loss: 1.6961\n",
      "Epoch [16/25], Step [528/590], Loss: 0.0781\n",
      "Epoch [16/25], Step [532/590], Loss: 0.0490\n",
      "Epoch [16/25], Step [536/590], Loss: 0.1799\n",
      "Epoch [16/25], Step [540/590], Loss: 4.8278\n",
      "Epoch [16/25], Step [544/590], Loss: 1.6870\n",
      "Epoch [16/25], Step [548/590], Loss: 1.7531\n",
      "Epoch [16/25], Step [552/590], Loss: 0.5540\n",
      "Epoch [16/25], Step [556/590], Loss: 1.1713\n",
      "Epoch [16/25], Step [560/590], Loss: 0.4633\n",
      "Epoch [16/25], Step [564/590], Loss: 0.0271\n",
      "Epoch [16/25], Step [568/590], Loss: 0.4744\n",
      "Epoch [16/25], Step [572/590], Loss: 0.4005\n",
      "Epoch [16/25], Step [576/590], Loss: 0.0629\n",
      "Epoch [16/25], Step [580/590], Loss: 0.1103\n",
      "Epoch [16/25], Step [584/590], Loss: 0.0284\n",
      "Epoch [16/25], Step [588/590], Loss: 4.9364\n",
      "epoch 16\n",
      "Epoch [17/25], Step [4/590], Loss: 0.6363\n",
      "Epoch [17/25], Step [8/590], Loss: 0.0067\n",
      "Epoch [17/25], Step [12/590], Loss: 0.4481\n",
      "Epoch [17/25], Step [16/590], Loss: 0.0163\n",
      "Epoch [17/25], Step [20/590], Loss: 0.3782\n",
      "Epoch [17/25], Step [24/590], Loss: 0.6758\n",
      "Epoch [17/25], Step [28/590], Loss: 0.3182\n",
      "Epoch [17/25], Step [32/590], Loss: 0.0305\n",
      "Epoch [17/25], Step [36/590], Loss: 0.1414\n",
      "Epoch [17/25], Step [40/590], Loss: 0.0772\n",
      "Epoch [17/25], Step [44/590], Loss: 0.3717\n",
      "Epoch [17/25], Step [48/590], Loss: 0.2445\n",
      "Epoch [17/25], Step [52/590], Loss: 0.0061\n",
      "Epoch [17/25], Step [56/590], Loss: 0.3188\n",
      "Epoch [17/25], Step [60/590], Loss: 0.0845\n",
      "Epoch [17/25], Step [64/590], Loss: 1.9973\n",
      "Epoch [17/25], Step [68/590], Loss: 0.6702\n",
      "Epoch [17/25], Step [72/590], Loss: 0.0198\n",
      "Epoch [17/25], Step [76/590], Loss: 0.0644\n",
      "Epoch [17/25], Step [80/590], Loss: 0.0629\n",
      "Epoch [17/25], Step [84/590], Loss: 0.0534\n",
      "Epoch [17/25], Step [88/590], Loss: 0.4391\n",
      "Epoch [17/25], Step [92/590], Loss: 0.4761\n",
      "Epoch [17/25], Step [96/590], Loss: 0.1243\n",
      "Epoch [17/25], Step [100/590], Loss: 1.3980\n",
      "Epoch [17/25], Step [104/590], Loss: 0.0715\n",
      "Epoch [17/25], Step [108/590], Loss: 0.2111\n",
      "Epoch [17/25], Step [112/590], Loss: 0.0495\n",
      "Epoch [17/25], Step [116/590], Loss: 0.0066\n",
      "Epoch [17/25], Step [120/590], Loss: 0.3926\n",
      "Epoch [17/25], Step [124/590], Loss: 2.1081\n",
      "Epoch [17/25], Step [128/590], Loss: 0.0646\n",
      "Epoch [17/25], Step [132/590], Loss: 3.4250\n",
      "Epoch [17/25], Step [136/590], Loss: 0.5340\n",
      "Epoch [17/25], Step [140/590], Loss: 0.1740\n",
      "Epoch [17/25], Step [144/590], Loss: 0.1927\n",
      "Epoch [17/25], Step [148/590], Loss: 0.0446\n",
      "Epoch [17/25], Step [152/590], Loss: 0.8234\n",
      "Epoch [17/25], Step [156/590], Loss: 0.1060\n",
      "Epoch [17/25], Step [160/590], Loss: 0.9615\n",
      "Epoch [17/25], Step [164/590], Loss: 0.3597\n",
      "Epoch [17/25], Step [168/590], Loss: 0.0132\n",
      "Epoch [17/25], Step [172/590], Loss: 0.2631\n",
      "Epoch [17/25], Step [176/590], Loss: 0.2598\n",
      "Epoch [17/25], Step [180/590], Loss: 0.3294\n",
      "Epoch [17/25], Step [184/590], Loss: 0.3895\n",
      "Epoch [17/25], Step [188/590], Loss: 0.0648\n",
      "Epoch [17/25], Step [192/590], Loss: 0.2280\n",
      "Epoch [17/25], Step [196/590], Loss: 0.2481\n",
      "Epoch [17/25], Step [200/590], Loss: 0.1159\n",
      "Epoch [17/25], Step [204/590], Loss: 0.1869\n",
      "Epoch [17/25], Step [208/590], Loss: 1.7267\n",
      "Epoch [17/25], Step [212/590], Loss: 1.7704\n",
      "Epoch [17/25], Step [216/590], Loss: 0.2900\n",
      "Epoch [17/25], Step [220/590], Loss: 7.9735\n",
      "Epoch [17/25], Step [224/590], Loss: 0.8582\n",
      "Epoch [17/25], Step [228/590], Loss: 0.1986\n",
      "Epoch [17/25], Step [232/590], Loss: 0.3839\n",
      "Epoch [17/25], Step [236/590], Loss: 0.1390\n",
      "Epoch [17/25], Step [240/590], Loss: 4.6083\n",
      "Epoch [17/25], Step [244/590], Loss: 2.9843\n",
      "Epoch [17/25], Step [248/590], Loss: 0.8011\n",
      "Epoch [17/25], Step [252/590], Loss: 0.2622\n",
      "Epoch [17/25], Step [256/590], Loss: 0.2488\n",
      "Epoch [17/25], Step [260/590], Loss: 0.2906\n",
      "Epoch [17/25], Step [264/590], Loss: 0.8003\n",
      "Epoch [17/25], Step [268/590], Loss: 0.1909\n",
      "Epoch [17/25], Step [272/590], Loss: 3.2935\n",
      "Epoch [17/25], Step [276/590], Loss: 0.1183\n",
      "Epoch [17/25], Step [280/590], Loss: 0.1335\n",
      "Epoch [17/25], Step [284/590], Loss: 0.1313\n",
      "Epoch [17/25], Step [288/590], Loss: 0.0846\n",
      "Epoch [17/25], Step [292/590], Loss: 1.2865\n",
      "Epoch [17/25], Step [296/590], Loss: 5.5033\n",
      "Epoch [17/25], Step [300/590], Loss: 0.5940\n",
      "Epoch [17/25], Step [304/590], Loss: 7.4087\n",
      "Epoch [17/25], Step [308/590], Loss: 1.3917\n",
      "Epoch [17/25], Step [312/590], Loss: 0.0776\n",
      "Epoch [17/25], Step [316/590], Loss: 12.9425\n",
      "Epoch [17/25], Step [320/590], Loss: 0.2294\n",
      "Epoch [17/25], Step [324/590], Loss: 0.2062\n",
      "Epoch [17/25], Step [328/590], Loss: 0.3333\n",
      "Epoch [17/25], Step [332/590], Loss: 0.1755\n",
      "Epoch [17/25], Step [336/590], Loss: 0.3949\n",
      "Epoch [17/25], Step [340/590], Loss: 3.6575\n",
      "Epoch [17/25], Step [344/590], Loss: 0.2257\n",
      "Epoch [17/25], Step [348/590], Loss: 0.0452\n",
      "Epoch [17/25], Step [352/590], Loss: 4.0739\n",
      "Epoch [17/25], Step [356/590], Loss: 0.5050\n",
      "Epoch [17/25], Step [360/590], Loss: 0.9748\n",
      "Epoch [17/25], Step [364/590], Loss: 0.0577\n",
      "Epoch [17/25], Step [368/590], Loss: 1.1588\n",
      "Epoch [17/25], Step [372/590], Loss: 0.3934\n",
      "Epoch [17/25], Step [376/590], Loss: 1.9047\n",
      "Epoch [17/25], Step [380/590], Loss: 0.3251\n",
      "Epoch [17/25], Step [384/590], Loss: 0.4795\n",
      "Epoch [17/25], Step [388/590], Loss: 0.6514\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Step [392/590], Loss: 0.0745\n",
      "Epoch [17/25], Step [396/590], Loss: 1.1365\n",
      "Epoch [17/25], Step [400/590], Loss: 0.1454\n",
      "Epoch [17/25], Step [404/590], Loss: 0.6636\n",
      "Epoch [17/25], Step [408/590], Loss: 0.2371\n",
      "Epoch [17/25], Step [412/590], Loss: 0.6047\n",
      "Epoch [17/25], Step [416/590], Loss: 0.0481\n",
      "Epoch [17/25], Step [420/590], Loss: 0.4571\n",
      "Epoch [17/25], Step [424/590], Loss: 2.3697\n",
      "Epoch [17/25], Step [428/590], Loss: 0.5909\n",
      "Epoch [17/25], Step [432/590], Loss: 0.8248\n",
      "Epoch [17/25], Step [436/590], Loss: 1.1454\n",
      "Epoch [17/25], Step [440/590], Loss: 0.0702\n",
      "Epoch [17/25], Step [444/590], Loss: 0.1827\n",
      "Epoch [17/25], Step [448/590], Loss: 0.0921\n",
      "Epoch [17/25], Step [452/590], Loss: 1.4077\n",
      "Epoch [17/25], Step [456/590], Loss: 0.0723\n",
      "Epoch [17/25], Step [460/590], Loss: 0.1096\n",
      "Epoch [17/25], Step [464/590], Loss: 0.1381\n",
      "Epoch [17/25], Step [468/590], Loss: 1.8255\n",
      "Epoch [17/25], Step [472/590], Loss: 0.4410\n",
      "Epoch [17/25], Step [476/590], Loss: 0.0787\n",
      "Epoch [17/25], Step [480/590], Loss: 0.0421\n",
      "Epoch [17/25], Step [484/590], Loss: 1.5581\n",
      "Epoch [17/25], Step [488/590], Loss: 0.9189\n",
      "Epoch [17/25], Step [492/590], Loss: 2.3262\n",
      "Epoch [17/25], Step [496/590], Loss: 1.3419\n",
      "Epoch [17/25], Step [500/590], Loss: 1.1872\n",
      "Epoch [17/25], Step [504/590], Loss: 4.3350\n",
      "Epoch [17/25], Step [508/590], Loss: 4.8922\n",
      "Epoch [17/25], Step [512/590], Loss: 0.0532\n",
      "Epoch [17/25], Step [516/590], Loss: 6.1898\n",
      "Epoch [17/25], Step [520/590], Loss: 0.0995\n",
      "Epoch [17/25], Step [524/590], Loss: 2.3397\n",
      "Epoch [17/25], Step [528/590], Loss: 0.0459\n",
      "Epoch [17/25], Step [532/590], Loss: 0.0899\n",
      "Epoch [17/25], Step [536/590], Loss: 0.0927\n",
      "Epoch [17/25], Step [540/590], Loss: 8.1534\n",
      "Epoch [17/25], Step [544/590], Loss: 0.4638\n",
      "Epoch [17/25], Step [548/590], Loss: 0.1727\n",
      "Epoch [17/25], Step [552/590], Loss: 0.0419\n",
      "Epoch [17/25], Step [556/590], Loss: 0.5041\n",
      "Epoch [17/25], Step [560/590], Loss: 0.1459\n",
      "Epoch [17/25], Step [564/590], Loss: 0.3049\n",
      "Epoch [17/25], Step [568/590], Loss: 0.0653\n",
      "Epoch [17/25], Step [572/590], Loss: 0.0970\n",
      "Epoch [17/25], Step [576/590], Loss: 0.6877\n",
      "Epoch [17/25], Step [580/590], Loss: 0.1190\n",
      "Epoch [17/25], Step [584/590], Loss: 0.3381\n",
      "Epoch [17/25], Step [588/590], Loss: 1.8262\n",
      "epoch 17\n",
      "Epoch [18/25], Step [4/590], Loss: 0.0223\n",
      "Epoch [18/25], Step [8/590], Loss: 0.4191\n",
      "Epoch [18/25], Step [12/590], Loss: 0.2436\n",
      "Epoch [18/25], Step [16/590], Loss: 2.9486\n",
      "Epoch [18/25], Step [20/590], Loss: 0.0144\n",
      "Epoch [18/25], Step [24/590], Loss: 0.0526\n",
      "Epoch [18/25], Step [28/590], Loss: 2.7005\n",
      "Epoch [18/25], Step [32/590], Loss: 0.0503\n",
      "Epoch [18/25], Step [36/590], Loss: 0.2150\n",
      "Epoch [18/25], Step [40/590], Loss: 0.0773\n",
      "Epoch [18/25], Step [44/590], Loss: 0.0893\n",
      "Epoch [18/25], Step [48/590], Loss: 0.0480\n",
      "Epoch [18/25], Step [52/590], Loss: 0.0294\n",
      "Epoch [18/25], Step [56/590], Loss: 0.0744\n",
      "Epoch [18/25], Step [60/590], Loss: 0.7466\n",
      "Epoch [18/25], Step [64/590], Loss: 0.5897\n",
      "Epoch [18/25], Step [68/590], Loss: 1.4072\n",
      "Epoch [18/25], Step [72/590], Loss: 0.1192\n",
      "Epoch [18/25], Step [76/590], Loss: 0.4124\n",
      "Epoch [18/25], Step [80/590], Loss: 0.0641\n",
      "Epoch [18/25], Step [84/590], Loss: 0.0254\n",
      "Epoch [18/25], Step [88/590], Loss: 6.7634\n",
      "Epoch [18/25], Step [92/590], Loss: 0.2848\n",
      "Epoch [18/25], Step [96/590], Loss: 0.1833\n",
      "Epoch [18/25], Step [100/590], Loss: 0.4338\n",
      "Epoch [18/25], Step [104/590], Loss: 0.2474\n",
      "Epoch [18/25], Step [108/590], Loss: 0.0719\n",
      "Epoch [18/25], Step [112/590], Loss: 0.3607\n",
      "Epoch [18/25], Step [116/590], Loss: 0.3639\n",
      "Epoch [18/25], Step [120/590], Loss: 0.0404\n",
      "Epoch [18/25], Step [124/590], Loss: 0.0656\n",
      "Epoch [18/25], Step [128/590], Loss: 0.1915\n",
      "Epoch [18/25], Step [132/590], Loss: 0.3371\n",
      "Epoch [18/25], Step [136/590], Loss: 5.4721\n",
      "Epoch [18/25], Step [140/590], Loss: 1.2679\n",
      "Epoch [18/25], Step [144/590], Loss: 0.1776\n",
      "Epoch [18/25], Step [148/590], Loss: 0.0941\n",
      "Epoch [18/25], Step [152/590], Loss: 5.5068\n",
      "Epoch [18/25], Step [156/590], Loss: 1.1805\n",
      "Epoch [18/25], Step [160/590], Loss: 0.0678\n",
      "Epoch [18/25], Step [164/590], Loss: 0.1031\n",
      "Epoch [18/25], Step [168/590], Loss: 0.0733\n",
      "Epoch [18/25], Step [172/590], Loss: 0.4998\n",
      "Epoch [18/25], Step [176/590], Loss: 5.0639\n",
      "Epoch [18/25], Step [180/590], Loss: 5.0580\n",
      "Epoch [18/25], Step [184/590], Loss: 0.0691\n",
      "Epoch [18/25], Step [188/590], Loss: 0.2991\n",
      "Epoch [18/25], Step [192/590], Loss: 2.7346\n",
      "Epoch [18/25], Step [196/590], Loss: 0.0554\n",
      "Epoch [18/25], Step [200/590], Loss: 3.8170\n",
      "Epoch [18/25], Step [204/590], Loss: 13.0402\n",
      "Epoch [18/25], Step [208/590], Loss: 0.5407\n",
      "Epoch [18/25], Step [212/590], Loss: 2.6354\n",
      "Epoch [18/25], Step [216/590], Loss: 0.1429\n",
      "Epoch [18/25], Step [220/590], Loss: 0.5214\n",
      "Epoch [18/25], Step [224/590], Loss: 0.0791\n",
      "Epoch [18/25], Step [228/590], Loss: 10.5928\n",
      "Epoch [18/25], Step [232/590], Loss: 0.0498\n",
      "Epoch [18/25], Step [236/590], Loss: 2.6263\n",
      "Epoch [18/25], Step [240/590], Loss: 0.7305\n",
      "Epoch [18/25], Step [244/590], Loss: 2.3035\n",
      "Epoch [18/25], Step [248/590], Loss: 0.1315\n",
      "Epoch [18/25], Step [252/590], Loss: 2.5480\n",
      "Epoch [18/25], Step [256/590], Loss: 0.6892\n",
      "Epoch [18/25], Step [260/590], Loss: 0.2210\n",
      "Epoch [18/25], Step [264/590], Loss: 0.0983\n",
      "Epoch [18/25], Step [268/590], Loss: 0.4745\n",
      "Epoch [18/25], Step [272/590], Loss: 0.2916\n",
      "Epoch [18/25], Step [276/590], Loss: 0.5892\n",
      "Epoch [18/25], Step [280/590], Loss: 0.8539\n",
      "Epoch [18/25], Step [284/590], Loss: 0.4675\n",
      "Epoch [18/25], Step [288/590], Loss: 0.2726\n",
      "Epoch [18/25], Step [292/590], Loss: 0.7821\n",
      "Epoch [18/25], Step [296/590], Loss: 0.6040\n",
      "Epoch [18/25], Step [300/590], Loss: 0.2212\n",
      "Epoch [18/25], Step [304/590], Loss: 0.6711\n",
      "Epoch [18/25], Step [308/590], Loss: 0.6036\n",
      "Epoch [18/25], Step [312/590], Loss: 1.8241\n",
      "Epoch [18/25], Step [316/590], Loss: 6.6164\n",
      "Epoch [18/25], Step [320/590], Loss: 0.5048\n",
      "Epoch [18/25], Step [324/590], Loss: 0.2050\n",
      "Epoch [18/25], Step [328/590], Loss: 0.0329\n",
      "Epoch [18/25], Step [332/590], Loss: 0.1251\n",
      "Epoch [18/25], Step [336/590], Loss: 0.3822\n",
      "Epoch [18/25], Step [340/590], Loss: 0.0167\n",
      "Epoch [18/25], Step [344/590], Loss: 0.1740\n",
      "Epoch [18/25], Step [348/590], Loss: 1.0203\n",
      "Epoch [18/25], Step [352/590], Loss: 0.2437\n",
      "Epoch [18/25], Step [356/590], Loss: 0.3904\n",
      "Epoch [18/25], Step [360/590], Loss: 0.1840\n",
      "Epoch [18/25], Step [364/590], Loss: 0.2911\n",
      "Epoch [18/25], Step [368/590], Loss: 0.0463\n",
      "Epoch [18/25], Step [372/590], Loss: 0.7804\n",
      "Epoch [18/25], Step [376/590], Loss: 3.6634\n",
      "Epoch [18/25], Step [380/590], Loss: 0.1374\n",
      "Epoch [18/25], Step [384/590], Loss: 1.5378\n",
      "Epoch [18/25], Step [388/590], Loss: 0.0303\n",
      "Epoch [18/25], Step [392/590], Loss: 0.1583\n",
      "Epoch [18/25], Step [396/590], Loss: 2.7398\n",
      "Epoch [18/25], Step [400/590], Loss: 0.4701\n",
      "Epoch [18/25], Step [404/590], Loss: 0.6606\n",
      "Epoch [18/25], Step [408/590], Loss: 0.1092\n",
      "Epoch [18/25], Step [412/590], Loss: 0.3328\n",
      "Epoch [18/25], Step [416/590], Loss: 3.7067\n",
      "Epoch [18/25], Step [420/590], Loss: 0.0606\n",
      "Epoch [18/25], Step [424/590], Loss: 0.0326\n",
      "Epoch [18/25], Step [428/590], Loss: 8.6433\n",
      "Epoch [18/25], Step [432/590], Loss: 0.3208\n",
      "Epoch [18/25], Step [436/590], Loss: 1.1356\n",
      "Epoch [18/25], Step [440/590], Loss: 2.7579\n",
      "Epoch [18/25], Step [444/590], Loss: 0.1135\n",
      "Epoch [18/25], Step [448/590], Loss: 0.8189\n",
      "Epoch [18/25], Step [452/590], Loss: 0.7597\n",
      "Epoch [18/25], Step [456/590], Loss: 0.7303\n",
      "Epoch [18/25], Step [460/590], Loss: 0.1773\n",
      "Epoch [18/25], Step [464/590], Loss: 3.8408\n",
      "Epoch [18/25], Step [468/590], Loss: 0.7873\n",
      "Epoch [18/25], Step [472/590], Loss: 0.7039\n",
      "Epoch [18/25], Step [476/590], Loss: 1.0684\n",
      "Epoch [18/25], Step [480/590], Loss: 0.8299\n",
      "Epoch [18/25], Step [484/590], Loss: 0.0409\n",
      "Epoch [18/25], Step [488/590], Loss: 4.3746\n",
      "Epoch [18/25], Step [492/590], Loss: 0.4419\n",
      "Epoch [18/25], Step [496/590], Loss: 0.2960\n",
      "Epoch [18/25], Step [500/590], Loss: 6.6447\n",
      "Epoch [18/25], Step [504/590], Loss: 1.7858\n",
      "Epoch [18/25], Step [508/590], Loss: 1.8332\n",
      "Epoch [18/25], Step [512/590], Loss: 0.1196\n",
      "Epoch [18/25], Step [516/590], Loss: 0.8718\n",
      "Epoch [18/25], Step [520/590], Loss: 6.3512\n",
      "Epoch [18/25], Step [524/590], Loss: 6.1585\n",
      "Epoch [18/25], Step [528/590], Loss: 3.6510\n",
      "Epoch [18/25], Step [532/590], Loss: 1.6029\n",
      "Epoch [18/25], Step [536/590], Loss: 1.0457\n",
      "Epoch [18/25], Step [540/590], Loss: 4.2979\n",
      "Epoch [18/25], Step [544/590], Loss: 1.6117\n",
      "Epoch [18/25], Step [548/590], Loss: 0.3753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Step [552/590], Loss: 4.2612\n",
      "Epoch [18/25], Step [556/590], Loss: 2.2364\n",
      "Epoch [18/25], Step [560/590], Loss: 0.2490\n",
      "Epoch [18/25], Step [564/590], Loss: 0.7383\n",
      "Epoch [18/25], Step [568/590], Loss: 1.8823\n",
      "Epoch [18/25], Step [572/590], Loss: 0.2622\n",
      "Epoch [18/25], Step [576/590], Loss: 1.5452\n",
      "Epoch [18/25], Step [580/590], Loss: 0.5333\n",
      "Epoch [18/25], Step [584/590], Loss: 2.0321\n",
      "Epoch [18/25], Step [588/590], Loss: 1.7739\n",
      "epoch 18\n",
      "Epoch [19/25], Step [4/590], Loss: 0.2696\n",
      "Epoch [19/25], Step [8/590], Loss: 1.5977\n",
      "Epoch [19/25], Step [12/590], Loss: 0.5346\n",
      "Epoch [19/25], Step [16/590], Loss: 0.8895\n",
      "Epoch [19/25], Step [20/590], Loss: 0.1129\n",
      "Epoch [19/25], Step [24/590], Loss: 0.3937\n",
      "Epoch [19/25], Step [28/590], Loss: 0.1275\n",
      "Epoch [19/25], Step [32/590], Loss: 0.0850\n",
      "Epoch [19/25], Step [36/590], Loss: 0.4654\n",
      "Epoch [19/25], Step [40/590], Loss: 1.0542\n",
      "Epoch [19/25], Step [44/590], Loss: 0.2735\n",
      "Epoch [19/25], Step [48/590], Loss: 0.3036\n",
      "Epoch [19/25], Step [52/590], Loss: 0.0100\n",
      "Epoch [19/25], Step [56/590], Loss: 0.0768\n",
      "Epoch [19/25], Step [60/590], Loss: 0.4089\n",
      "Epoch [19/25], Step [64/590], Loss: 0.0606\n",
      "Epoch [19/25], Step [68/590], Loss: 0.4140\n",
      "Epoch [19/25], Step [72/590], Loss: 0.0541\n",
      "Epoch [19/25], Step [76/590], Loss: 0.0364\n",
      "Epoch [19/25], Step [80/590], Loss: 0.6799\n",
      "Epoch [19/25], Step [84/590], Loss: 0.2086\n",
      "Epoch [19/25], Step [88/590], Loss: 0.6816\n",
      "Epoch [19/25], Step [92/590], Loss: 5.5677\n",
      "Epoch [19/25], Step [96/590], Loss: 0.0174\n",
      "Epoch [19/25], Step [100/590], Loss: 2.9990\n",
      "Epoch [19/25], Step [104/590], Loss: 4.3536\n",
      "Epoch [19/25], Step [108/590], Loss: 0.3312\n",
      "Epoch [19/25], Step [112/590], Loss: 4.4217\n",
      "Epoch [19/25], Step [116/590], Loss: 0.6957\n",
      "Epoch [19/25], Step [120/590], Loss: 0.8125\n",
      "Epoch [19/25], Step [124/590], Loss: 0.0330\n",
      "Epoch [19/25], Step [128/590], Loss: 0.7215\n",
      "Epoch [19/25], Step [132/590], Loss: 0.1461\n",
      "Epoch [19/25], Step [136/590], Loss: 0.0478\n",
      "Epoch [19/25], Step [140/590], Loss: 0.8750\n",
      "Epoch [19/25], Step [144/590], Loss: 0.1153\n",
      "Epoch [19/25], Step [148/590], Loss: 0.6014\n",
      "Epoch [19/25], Step [152/590], Loss: 1.0314\n",
      "Epoch [19/25], Step [156/590], Loss: 1.1755\n",
      "Epoch [19/25], Step [160/590], Loss: 1.7577\n",
      "Epoch [19/25], Step [164/590], Loss: 1.1911\n",
      "Epoch [19/25], Step [168/590], Loss: 0.3565\n",
      "Epoch [19/25], Step [172/590], Loss: 1.7333\n",
      "Epoch [19/25], Step [176/590], Loss: 0.0113\n",
      "Epoch [19/25], Step [180/590], Loss: 0.2170\n",
      "Epoch [19/25], Step [184/590], Loss: 0.3318\n",
      "Epoch [19/25], Step [188/590], Loss: 0.1630\n",
      "Epoch [19/25], Step [192/590], Loss: 2.0240\n",
      "Epoch [19/25], Step [196/590], Loss: 1.0538\n",
      "Epoch [19/25], Step [200/590], Loss: 0.5342\n",
      "Epoch [19/25], Step [204/590], Loss: 4.1749\n",
      "Epoch [19/25], Step [208/590], Loss: 0.0993\n",
      "Epoch [19/25], Step [212/590], Loss: 1.1358\n",
      "Epoch [19/25], Step [216/590], Loss: 0.0957\n",
      "Epoch [19/25], Step [220/590], Loss: 0.2144\n",
      "Epoch [19/25], Step [224/590], Loss: 1.0562\n",
      "Epoch [19/25], Step [228/590], Loss: 0.0537\n",
      "Epoch [19/25], Step [232/590], Loss: 0.9242\n",
      "Epoch [19/25], Step [236/590], Loss: 0.0274\n",
      "Epoch [19/25], Step [240/590], Loss: 3.1849\n",
      "Epoch [19/25], Step [244/590], Loss: 0.2199\n",
      "Epoch [19/25], Step [248/590], Loss: 0.9113\n",
      "Epoch [19/25], Step [252/590], Loss: 0.1170\n",
      "Epoch [19/25], Step [256/590], Loss: 0.2228\n",
      "Epoch [19/25], Step [260/590], Loss: 0.0206\n",
      "Epoch [19/25], Step [264/590], Loss: 0.2794\n",
      "Epoch [19/25], Step [268/590], Loss: 0.1229\n",
      "Epoch [19/25], Step [272/590], Loss: 8.3405\n",
      "Epoch [19/25], Step [276/590], Loss: 1.1128\n",
      "Epoch [19/25], Step [280/590], Loss: 0.1473\n",
      "Epoch [19/25], Step [284/590], Loss: 1.9252\n",
      "Epoch [19/25], Step [288/590], Loss: 0.0869\n",
      "Epoch [19/25], Step [292/590], Loss: 0.3537\n",
      "Epoch [19/25], Step [296/590], Loss: 0.0717\n",
      "Epoch [19/25], Step [300/590], Loss: 2.7804\n",
      "Epoch [19/25], Step [304/590], Loss: 0.2736\n",
      "Epoch [19/25], Step [308/590], Loss: 5.7819\n",
      "Epoch [19/25], Step [312/590], Loss: 1.7012\n",
      "Epoch [19/25], Step [316/590], Loss: 0.4253\n",
      "Epoch [19/25], Step [320/590], Loss: 0.5724\n",
      "Epoch [19/25], Step [324/590], Loss: 5.5173\n",
      "Epoch [19/25], Step [328/590], Loss: 0.7962\n",
      "Epoch [19/25], Step [332/590], Loss: 0.3597\n",
      "Epoch [19/25], Step [336/590], Loss: 0.6251\n",
      "Epoch [19/25], Step [340/590], Loss: 5.1042\n",
      "Epoch [19/25], Step [344/590], Loss: 0.7166\n",
      "Epoch [19/25], Step [348/590], Loss: 0.1705\n",
      "Epoch [19/25], Step [352/590], Loss: 0.3002\n",
      "Epoch [19/25], Step [356/590], Loss: 0.7305\n",
      "Epoch [19/25], Step [360/590], Loss: 0.1019\n",
      "Epoch [19/25], Step [364/590], Loss: 0.2299\n",
      "Epoch [19/25], Step [368/590], Loss: 3.5074\n",
      "Epoch [19/25], Step [372/590], Loss: 3.2436\n",
      "Epoch [19/25], Step [376/590], Loss: 0.2414\n",
      "Epoch [19/25], Step [380/590], Loss: 0.2063\n",
      "Epoch [19/25], Step [384/590], Loss: 0.3371\n",
      "Epoch [19/25], Step [388/590], Loss: 0.1564\n",
      "Epoch [19/25], Step [392/590], Loss: 0.1361\n",
      "Epoch [19/25], Step [396/590], Loss: 0.1798\n",
      "Epoch [19/25], Step [400/590], Loss: 0.9027\n",
      "Epoch [19/25], Step [404/590], Loss: 0.2891\n",
      "Epoch [19/25], Step [408/590], Loss: 0.1328\n",
      "Epoch [19/25], Step [412/590], Loss: 0.5468\n",
      "Epoch [19/25], Step [416/590], Loss: 3.4906\n",
      "Epoch [19/25], Step [420/590], Loss: 1.6517\n",
      "Epoch [19/25], Step [424/590], Loss: 0.1568\n",
      "Epoch [19/25], Step [428/590], Loss: 0.3069\n",
      "Epoch [19/25], Step [432/590], Loss: 6.3885\n",
      "Epoch [19/25], Step [436/590], Loss: 3.8322\n",
      "Epoch [19/25], Step [440/590], Loss: 1.8328\n",
      "Epoch [19/25], Step [444/590], Loss: 0.9561\n",
      "Epoch [19/25], Step [448/590], Loss: 4.7336\n",
      "Epoch [19/25], Step [452/590], Loss: 0.0606\n",
      "Epoch [19/25], Step [456/590], Loss: 5.1546\n",
      "Epoch [19/25], Step [460/590], Loss: 5.6181\n",
      "Epoch [19/25], Step [464/590], Loss: 0.6696\n",
      "Epoch [19/25], Step [468/590], Loss: 0.3632\n",
      "Epoch [19/25], Step [472/590], Loss: 0.9995\n",
      "Epoch [19/25], Step [476/590], Loss: 0.1074\n",
      "Epoch [19/25], Step [480/590], Loss: 4.5865\n",
      "Epoch [19/25], Step [484/590], Loss: 0.1082\n",
      "Epoch [19/25], Step [488/590], Loss: 2.2112\n",
      "Epoch [19/25], Step [492/590], Loss: 0.1754\n",
      "Epoch [19/25], Step [496/590], Loss: 0.2089\n",
      "Epoch [19/25], Step [500/590], Loss: 1.9096\n",
      "Epoch [19/25], Step [504/590], Loss: 0.1029\n",
      "Epoch [19/25], Step [508/590], Loss: 0.0340\n",
      "Epoch [19/25], Step [512/590], Loss: 0.1713\n",
      "Epoch [19/25], Step [516/590], Loss: 0.1027\n",
      "Epoch [19/25], Step [520/590], Loss: 1.2403\n",
      "Epoch [19/25], Step [524/590], Loss: 2.2392\n",
      "Epoch [19/25], Step [528/590], Loss: 0.5939\n",
      "Epoch [19/25], Step [532/590], Loss: 0.5121\n",
      "Epoch [19/25], Step [536/590], Loss: 0.7460\n",
      "Epoch [19/25], Step [540/590], Loss: 0.0191\n",
      "Epoch [19/25], Step [544/590], Loss: 1.2599\n",
      "Epoch [19/25], Step [548/590], Loss: 0.0301\n",
      "Epoch [19/25], Step [552/590], Loss: 0.0248\n",
      "Epoch [19/25], Step [556/590], Loss: 0.1179\n",
      "Epoch [19/25], Step [560/590], Loss: 0.4345\n",
      "Epoch [19/25], Step [564/590], Loss: 0.1396\n",
      "Epoch [19/25], Step [568/590], Loss: 0.1514\n",
      "Epoch [19/25], Step [572/590], Loss: 1.8685\n",
      "Epoch [19/25], Step [576/590], Loss: 0.0493\n",
      "Epoch [19/25], Step [580/590], Loss: 0.0767\n",
      "Epoch [19/25], Step [584/590], Loss: 0.1272\n",
      "Epoch [19/25], Step [588/590], Loss: 0.2148\n",
      "epoch 19\n",
      "Epoch [20/25], Step [4/590], Loss: 0.0923\n",
      "Epoch [20/25], Step [8/590], Loss: 0.2071\n",
      "Epoch [20/25], Step [12/590], Loss: 0.0877\n",
      "Epoch [20/25], Step [16/590], Loss: 0.2505\n",
      "Epoch [20/25], Step [20/590], Loss: 0.0717\n",
      "Epoch [20/25], Step [24/590], Loss: 0.1973\n",
      "Epoch [20/25], Step [28/590], Loss: 0.0085\n",
      "Epoch [20/25], Step [32/590], Loss: 0.4363\n",
      "Epoch [20/25], Step [36/590], Loss: 4.9266\n",
      "Epoch [20/25], Step [40/590], Loss: 0.0243\n",
      "Epoch [20/25], Step [44/590], Loss: 0.0430\n",
      "Epoch [20/25], Step [48/590], Loss: 0.2566\n",
      "Epoch [20/25], Step [52/590], Loss: 0.0574\n",
      "Epoch [20/25], Step [56/590], Loss: 1.2019\n",
      "Epoch [20/25], Step [60/590], Loss: 0.9264\n",
      "Epoch [20/25], Step [64/590], Loss: 0.0542\n",
      "Epoch [20/25], Step [68/590], Loss: 1.0602\n",
      "Epoch [20/25], Step [72/590], Loss: 1.1159\n",
      "Epoch [20/25], Step [76/590], Loss: 1.0672\n",
      "Epoch [20/25], Step [80/590], Loss: 0.0637\n",
      "Epoch [20/25], Step [84/590], Loss: 0.6306\n",
      "Epoch [20/25], Step [88/590], Loss: 0.3551\n",
      "Epoch [20/25], Step [92/590], Loss: 0.2904\n",
      "Epoch [20/25], Step [96/590], Loss: 0.1696\n",
      "Epoch [20/25], Step [100/590], Loss: 0.1230\n",
      "Epoch [20/25], Step [104/590], Loss: 3.5746\n",
      "Epoch [20/25], Step [108/590], Loss: 0.0332\n",
      "Epoch [20/25], Step [112/590], Loss: 0.0293\n",
      "Epoch [20/25], Step [116/590], Loss: 0.0164\n",
      "Epoch [20/25], Step [120/590], Loss: 0.0256\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], Step [124/590], Loss: 0.2671\n",
      "Epoch [20/25], Step [128/590], Loss: 0.0291\n",
      "Epoch [20/25], Step [132/590], Loss: 0.0084\n",
      "Epoch [20/25], Step [136/590], Loss: 0.0420\n",
      "Epoch [20/25], Step [140/590], Loss: 0.0796\n",
      "Epoch [20/25], Step [144/590], Loss: 0.3895\n",
      "Epoch [20/25], Step [148/590], Loss: 0.0067\n",
      "Epoch [20/25], Step [152/590], Loss: 0.1865\n",
      "Epoch [20/25], Step [156/590], Loss: 0.3470\n",
      "Epoch [20/25], Step [160/590], Loss: 0.1476\n",
      "Epoch [20/25], Step [164/590], Loss: 0.0862\n",
      "Epoch [20/25], Step [168/590], Loss: 0.0983\n",
      "Epoch [20/25], Step [172/590], Loss: 0.5496\n",
      "Epoch [20/25], Step [176/590], Loss: 0.0318\n",
      "Epoch [20/25], Step [180/590], Loss: 0.9056\n",
      "Epoch [20/25], Step [184/590], Loss: 0.3634\n",
      "Epoch [20/25], Step [188/590], Loss: 0.6637\n",
      "Epoch [20/25], Step [192/590], Loss: 0.7730\n",
      "Epoch [20/25], Step [196/590], Loss: 0.0162\n",
      "Epoch [20/25], Step [200/590], Loss: 9.6136\n",
      "Epoch [20/25], Step [204/590], Loss: 0.5040\n",
      "Epoch [20/25], Step [208/590], Loss: 4.7080\n",
      "Epoch [20/25], Step [212/590], Loss: 0.1251\n",
      "Epoch [20/25], Step [216/590], Loss: 3.8019\n",
      "Epoch [20/25], Step [220/590], Loss: 2.0733\n",
      "Epoch [20/25], Step [224/590], Loss: 1.0553\n",
      "Epoch [20/25], Step [228/590], Loss: 0.1974\n",
      "Epoch [20/25], Step [232/590], Loss: 0.0378\n",
      "Epoch [20/25], Step [236/590], Loss: 0.1131\n",
      "Epoch [20/25], Step [240/590], Loss: 0.4728\n",
      "Epoch [20/25], Step [244/590], Loss: 0.0218\n",
      "Epoch [20/25], Step [248/590], Loss: 0.0065\n",
      "Epoch [20/25], Step [252/590], Loss: 0.1182\n",
      "Epoch [20/25], Step [256/590], Loss: 0.0955\n",
      "Epoch [20/25], Step [260/590], Loss: 0.1174\n",
      "Epoch [20/25], Step [264/590], Loss: 0.1548\n",
      "Epoch [20/25], Step [268/590], Loss: 0.3240\n",
      "Epoch [20/25], Step [272/590], Loss: 0.0834\n",
      "Epoch [20/25], Step [276/590], Loss: 0.0537\n",
      "Epoch [20/25], Step [280/590], Loss: 0.0659\n",
      "Epoch [20/25], Step [284/590], Loss: 0.1729\n",
      "Epoch [20/25], Step [288/590], Loss: 4.9563\n",
      "Epoch [20/25], Step [292/590], Loss: 0.0834\n",
      "Epoch [20/25], Step [296/590], Loss: 0.2693\n",
      "Epoch [20/25], Step [300/590], Loss: 0.0096\n",
      "Epoch [20/25], Step [304/590], Loss: 0.1491\n",
      "Epoch [20/25], Step [308/590], Loss: 2.8366\n",
      "Epoch [20/25], Step [312/590], Loss: 0.0108\n",
      "Epoch [20/25], Step [316/590], Loss: 0.0965\n",
      "Epoch [20/25], Step [320/590], Loss: 5.0692\n",
      "Epoch [20/25], Step [324/590], Loss: 0.1320\n",
      "Epoch [20/25], Step [328/590], Loss: 0.2862\n",
      "Epoch [20/25], Step [332/590], Loss: 1.3140\n",
      "Epoch [20/25], Step [336/590], Loss: 0.3883\n",
      "Epoch [20/25], Step [340/590], Loss: 0.6630\n",
      "Epoch [20/25], Step [344/590], Loss: 1.1876\n",
      "Epoch [20/25], Step [348/590], Loss: 4.3182\n",
      "Epoch [20/25], Step [352/590], Loss: 1.7453\n",
      "Epoch [20/25], Step [356/590], Loss: 0.6198\n",
      "Epoch [20/25], Step [360/590], Loss: 16.3188\n",
      "Epoch [20/25], Step [364/590], Loss: 0.1806\n",
      "Epoch [20/25], Step [368/590], Loss: 5.2272\n",
      "Epoch [20/25], Step [372/590], Loss: 0.0198\n",
      "Epoch [20/25], Step [376/590], Loss: 0.0451\n",
      "Epoch [20/25], Step [380/590], Loss: 3.6670\n",
      "Epoch [20/25], Step [384/590], Loss: 0.0515\n",
      "Epoch [20/25], Step [388/590], Loss: 0.1633\n",
      "Epoch [20/25], Step [392/590], Loss: 0.0904\n",
      "Epoch [20/25], Step [396/590], Loss: 0.1045\n",
      "Epoch [20/25], Step [400/590], Loss: 0.0629\n",
      "Epoch [20/25], Step [404/590], Loss: 0.1402\n",
      "Epoch [20/25], Step [408/590], Loss: 0.2299\n",
      "Epoch [20/25], Step [412/590], Loss: 0.0122\n",
      "Epoch [20/25], Step [416/590], Loss: 2.7130\n",
      "Epoch [20/25], Step [420/590], Loss: 0.1473\n",
      "Epoch [20/25], Step [424/590], Loss: 0.0424\n",
      "Epoch [20/25], Step [428/590], Loss: 0.2578\n",
      "Epoch [20/25], Step [432/590], Loss: 0.0949\n",
      "Epoch [20/25], Step [436/590], Loss: 0.3733\n",
      "Epoch [20/25], Step [440/590], Loss: 0.1113\n",
      "Epoch [20/25], Step [444/590], Loss: 0.1700\n",
      "Epoch [20/25], Step [448/590], Loss: 0.5156\n",
      "Epoch [20/25], Step [452/590], Loss: 2.9163\n",
      "Epoch [20/25], Step [456/590], Loss: 0.0687\n",
      "Epoch [20/25], Step [460/590], Loss: 1.4085\n",
      "Epoch [20/25], Step [464/590], Loss: 0.0862\n",
      "Epoch [20/25], Step [468/590], Loss: 0.2503\n",
      "Epoch [20/25], Step [472/590], Loss: 0.0683\n",
      "Epoch [20/25], Step [476/590], Loss: 0.3150\n",
      "Epoch [20/25], Step [480/590], Loss: 1.4674\n",
      "Epoch [20/25], Step [484/590], Loss: 1.3602\n",
      "Epoch [20/25], Step [488/590], Loss: 0.2575\n",
      "Epoch [20/25], Step [492/590], Loss: 0.0617\n",
      "Epoch [20/25], Step [496/590], Loss: 3.6462\n",
      "Epoch [20/25], Step [500/590], Loss: 0.1793\n",
      "Epoch [20/25], Step [504/590], Loss: 0.0765\n",
      "Epoch [20/25], Step [508/590], Loss: 0.7865\n",
      "Epoch [20/25], Step [512/590], Loss: 0.0401\n",
      "Epoch [20/25], Step [516/590], Loss: 0.4971\n",
      "Epoch [20/25], Step [520/590], Loss: 0.0247\n",
      "Epoch [20/25], Step [524/590], Loss: 0.3341\n",
      "Epoch [20/25], Step [528/590], Loss: 0.0072\n",
      "Epoch [20/25], Step [532/590], Loss: 0.5462\n",
      "Epoch [20/25], Step [536/590], Loss: 0.6629\n",
      "Epoch [20/25], Step [540/590], Loss: 0.0980\n",
      "Epoch [20/25], Step [544/590], Loss: 0.0409\n",
      "Epoch [20/25], Step [548/590], Loss: 0.0453\n",
      "Epoch [20/25], Step [552/590], Loss: 0.0227\n",
      "Epoch [20/25], Step [556/590], Loss: 0.0786\n",
      "Epoch [20/25], Step [560/590], Loss: 0.1072\n",
      "Epoch [20/25], Step [564/590], Loss: 0.6769\n",
      "Epoch [20/25], Step [568/590], Loss: 0.0870\n",
      "Epoch [20/25], Step [572/590], Loss: 0.0501\n",
      "Epoch [20/25], Step [576/590], Loss: 0.0421\n",
      "Epoch [20/25], Step [580/590], Loss: 0.1013\n",
      "Epoch [20/25], Step [584/590], Loss: 0.0324\n",
      "Epoch [20/25], Step [588/590], Loss: 0.1587\n",
      "epoch 20\n",
      "Epoch [21/25], Step [4/590], Loss: 0.9538\n",
      "Epoch [21/25], Step [8/590], Loss: 0.2685\n",
      "Epoch [21/25], Step [12/590], Loss: 0.1047\n",
      "Epoch [21/25], Step [16/590], Loss: 0.6704\n",
      "Epoch [21/25], Step [20/590], Loss: 0.2490\n",
      "Epoch [21/25], Step [24/590], Loss: 0.1300\n",
      "Epoch [21/25], Step [28/590], Loss: 0.0171\n",
      "Epoch [21/25], Step [32/590], Loss: 0.1935\n",
      "Epoch [21/25], Step [36/590], Loss: 0.1366\n",
      "Epoch [21/25], Step [40/590], Loss: 0.3239\n",
      "Epoch [21/25], Step [44/590], Loss: 0.0764\n",
      "Epoch [21/25], Step [48/590], Loss: 0.2186\n",
      "Epoch [21/25], Step [52/590], Loss: 0.0423\n",
      "Epoch [21/25], Step [56/590], Loss: 0.0024\n",
      "Epoch [21/25], Step [60/590], Loss: 0.5235\n",
      "Epoch [21/25], Step [64/590], Loss: 0.0420\n",
      "Epoch [21/25], Step [68/590], Loss: 0.3241\n",
      "Epoch [21/25], Step [72/590], Loss: 0.0342\n",
      "Epoch [21/25], Step [76/590], Loss: 0.0274\n",
      "Epoch [21/25], Step [80/590], Loss: 1.1059\n",
      "Epoch [21/25], Step [84/590], Loss: 0.0178\n",
      "Epoch [21/25], Step [88/590], Loss: 0.4720\n",
      "Epoch [21/25], Step [92/590], Loss: 0.0152\n",
      "Epoch [21/25], Step [96/590], Loss: 0.1057\n",
      "Epoch [21/25], Step [100/590], Loss: 0.4235\n",
      "Epoch [21/25], Step [104/590], Loss: 0.0170\n",
      "Epoch [21/25], Step [108/590], Loss: 0.0194\n",
      "Epoch [21/25], Step [112/590], Loss: 0.0493\n",
      "Epoch [21/25], Step [116/590], Loss: 0.1823\n",
      "Epoch [21/25], Step [120/590], Loss: 0.2050\n",
      "Epoch [21/25], Step [124/590], Loss: 0.0288\n",
      "Epoch [21/25], Step [128/590], Loss: 0.0055\n",
      "Epoch [21/25], Step [132/590], Loss: 0.0420\n",
      "Epoch [21/25], Step [136/590], Loss: 2.7908\n",
      "Epoch [21/25], Step [140/590], Loss: 0.1156\n",
      "Epoch [21/25], Step [144/590], Loss: 0.1492\n",
      "Epoch [21/25], Step [148/590], Loss: 0.0236\n",
      "Epoch [21/25], Step [152/590], Loss: 0.0053\n",
      "Epoch [21/25], Step [156/590], Loss: 0.0714\n",
      "Epoch [21/25], Step [160/590], Loss: 0.0066\n",
      "Epoch [21/25], Step [164/590], Loss: 0.0205\n",
      "Epoch [21/25], Step [168/590], Loss: 0.0319\n",
      "Epoch [21/25], Step [172/590], Loss: 3.3202\n",
      "Epoch [21/25], Step [176/590], Loss: 0.6819\n",
      "Epoch [21/25], Step [180/590], Loss: 0.2357\n",
      "Epoch [21/25], Step [184/590], Loss: 0.0900\n",
      "Epoch [21/25], Step [188/590], Loss: 2.4921\n",
      "Epoch [21/25], Step [192/590], Loss: 0.0520\n",
      "Epoch [21/25], Step [196/590], Loss: 0.1095\n",
      "Epoch [21/25], Step [200/590], Loss: 0.1268\n",
      "Epoch [21/25], Step [204/590], Loss: 0.1015\n",
      "Epoch [21/25], Step [208/590], Loss: 0.0460\n",
      "Epoch [21/25], Step [212/590], Loss: 0.0277\n",
      "Epoch [21/25], Step [216/590], Loss: 0.1587\n",
      "Epoch [21/25], Step [220/590], Loss: 0.1346\n",
      "Epoch [21/25], Step [224/590], Loss: 0.0523\n",
      "Epoch [21/25], Step [228/590], Loss: 0.0309\n",
      "Epoch [21/25], Step [232/590], Loss: 0.4779\n",
      "Epoch [21/25], Step [236/590], Loss: 1.1262\n",
      "Epoch [21/25], Step [240/590], Loss: 0.0128\n",
      "Epoch [21/25], Step [244/590], Loss: 0.0207\n",
      "Epoch [21/25], Step [248/590], Loss: 0.0676\n",
      "Epoch [21/25], Step [252/590], Loss: 0.0691\n",
      "Epoch [21/25], Step [256/590], Loss: 0.4174\n",
      "Epoch [21/25], Step [260/590], Loss: 0.5201\n",
      "Epoch [21/25], Step [264/590], Loss: 2.6832\n",
      "Epoch [21/25], Step [268/590], Loss: 0.8584\n",
      "Epoch [21/25], Step [272/590], Loss: 0.1152\n",
      "Epoch [21/25], Step [276/590], Loss: 5.9338\n",
      "Epoch [21/25], Step [280/590], Loss: 0.1483\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], Step [284/590], Loss: 0.2102\n",
      "Epoch [21/25], Step [288/590], Loss: 0.0320\n",
      "Epoch [21/25], Step [292/590], Loss: 1.8069\n",
      "Epoch [21/25], Step [296/590], Loss: 2.7170\n",
      "Epoch [21/25], Step [300/590], Loss: 0.5528\n",
      "Epoch [21/25], Step [304/590], Loss: 0.9082\n",
      "Epoch [21/25], Step [308/590], Loss: 0.4817\n",
      "Epoch [21/25], Step [312/590], Loss: 0.4064\n",
      "Epoch [21/25], Step [316/590], Loss: 0.2697\n",
      "Epoch [21/25], Step [320/590], Loss: 3.0547\n",
      "Epoch [21/25], Step [324/590], Loss: 0.5742\n",
      "Epoch [21/25], Step [328/590], Loss: 0.0362\n",
      "Epoch [21/25], Step [332/590], Loss: 6.2837\n",
      "Epoch [21/25], Step [336/590], Loss: 0.2886\n",
      "Epoch [21/25], Step [340/590], Loss: 0.5744\n",
      "Epoch [21/25], Step [344/590], Loss: 0.1718\n",
      "Epoch [21/25], Step [348/590], Loss: 0.8326\n",
      "Epoch [21/25], Step [352/590], Loss: 0.0103\n",
      "Epoch [21/25], Step [356/590], Loss: 0.0915\n",
      "Epoch [21/25], Step [360/590], Loss: 0.5361\n",
      "Epoch [21/25], Step [364/590], Loss: 0.0118\n",
      "Epoch [21/25], Step [368/590], Loss: 0.2614\n",
      "Epoch [21/25], Step [372/590], Loss: 0.9686\n",
      "Epoch [21/25], Step [376/590], Loss: 0.4195\n",
      "Epoch [21/25], Step [380/590], Loss: 0.4718\n",
      "Epoch [21/25], Step [384/590], Loss: 0.0763\n",
      "Epoch [21/25], Step [388/590], Loss: 0.1071\n",
      "Epoch [21/25], Step [392/590], Loss: 0.1480\n",
      "Epoch [21/25], Step [396/590], Loss: 0.1554\n",
      "Epoch [21/25], Step [400/590], Loss: 1.3229\n",
      "Epoch [21/25], Step [404/590], Loss: 0.1146\n",
      "Epoch [21/25], Step [408/590], Loss: 0.1495\n",
      "Epoch [21/25], Step [412/590], Loss: 0.8329\n",
      "Epoch [21/25], Step [416/590], Loss: 0.1687\n",
      "Epoch [21/25], Step [420/590], Loss: 0.0322\n",
      "Epoch [21/25], Step [424/590], Loss: 1.3255\n",
      "Epoch [21/25], Step [428/590], Loss: 0.0771\n",
      "Epoch [21/25], Step [432/590], Loss: 0.6463\n",
      "Epoch [21/25], Step [436/590], Loss: 0.0438\n",
      "Epoch [21/25], Step [440/590], Loss: 0.2029\n",
      "Epoch [21/25], Step [444/590], Loss: 0.0473\n",
      "Epoch [21/25], Step [448/590], Loss: 0.0172\n",
      "Epoch [21/25], Step [452/590], Loss: 0.0960\n",
      "Epoch [21/25], Step [456/590], Loss: 0.0448\n",
      "Epoch [21/25], Step [460/590], Loss: 2.2838\n",
      "Epoch [21/25], Step [464/590], Loss: 0.4121\n",
      "Epoch [21/25], Step [468/590], Loss: 0.2562\n",
      "Epoch [21/25], Step [472/590], Loss: 1.3160\n",
      "Epoch [21/25], Step [476/590], Loss: 0.0664\n",
      "Epoch [21/25], Step [480/590], Loss: 4.8301\n",
      "Epoch [21/25], Step [484/590], Loss: 1.3389\n",
      "Epoch [21/25], Step [488/590], Loss: 0.2470\n",
      "Epoch [21/25], Step [492/590], Loss: 0.3891\n",
      "Epoch [21/25], Step [496/590], Loss: 1.7269\n",
      "Epoch [21/25], Step [500/590], Loss: 0.1347\n",
      "Epoch [21/25], Step [504/590], Loss: 0.0262\n",
      "Epoch [21/25], Step [508/590], Loss: 0.1152\n",
      "Epoch [21/25], Step [512/590], Loss: 0.1237\n",
      "Epoch [21/25], Step [516/590], Loss: 0.6700\n",
      "Epoch [21/25], Step [520/590], Loss: 0.2366\n",
      "Epoch [21/25], Step [524/590], Loss: 0.0234\n",
      "Epoch [21/25], Step [528/590], Loss: 0.0863\n",
      "Epoch [21/25], Step [532/590], Loss: 0.4158\n",
      "Epoch [21/25], Step [536/590], Loss: 6.7880\n",
      "Epoch [21/25], Step [540/590], Loss: 0.0279\n",
      "Epoch [21/25], Step [544/590], Loss: 0.0542\n",
      "Epoch [21/25], Step [548/590], Loss: 0.2163\n",
      "Epoch [21/25], Step [552/590], Loss: 0.2251\n",
      "Epoch [21/25], Step [556/590], Loss: 0.0877\n",
      "Epoch [21/25], Step [560/590], Loss: 0.2878\n",
      "Epoch [21/25], Step [564/590], Loss: 0.2572\n",
      "Epoch [21/25], Step [568/590], Loss: 0.0770\n",
      "Epoch [21/25], Step [572/590], Loss: 1.3706\n",
      "Epoch [21/25], Step [576/590], Loss: 0.1140\n",
      "Epoch [21/25], Step [580/590], Loss: 0.1487\n",
      "Epoch [21/25], Step [584/590], Loss: 0.1529\n",
      "Epoch [21/25], Step [588/590], Loss: 0.1189\n",
      "epoch 21\n",
      "Epoch [22/25], Step [4/590], Loss: 0.1342\n",
      "Epoch [22/25], Step [8/590], Loss: 1.5693\n",
      "Epoch [22/25], Step [12/590], Loss: 1.1216\n",
      "Epoch [22/25], Step [16/590], Loss: 0.3057\n",
      "Epoch [22/25], Step [20/590], Loss: 0.2686\n",
      "Epoch [22/25], Step [24/590], Loss: 0.2950\n",
      "Epoch [22/25], Step [28/590], Loss: 0.5390\n",
      "Epoch [22/25], Step [32/590], Loss: 2.0137\n",
      "Epoch [22/25], Step [36/590], Loss: 0.6664\n",
      "Epoch [22/25], Step [40/590], Loss: 0.1029\n",
      "Epoch [22/25], Step [44/590], Loss: 0.2153\n",
      "Epoch [22/25], Step [48/590], Loss: 0.2172\n",
      "Epoch [22/25], Step [52/590], Loss: 0.4152\n",
      "Epoch [22/25], Step [56/590], Loss: 0.1491\n",
      "Epoch [22/25], Step [60/590], Loss: 0.1534\n",
      "Epoch [22/25], Step [64/590], Loss: 1.4985\n",
      "Epoch [22/25], Step [68/590], Loss: 0.5291\n",
      "Epoch [22/25], Step [72/590], Loss: 0.2547\n",
      "Epoch [22/25], Step [76/590], Loss: 0.1404\n",
      "Epoch [22/25], Step [80/590], Loss: 5.9870\n",
      "Epoch [22/25], Step [84/590], Loss: 0.8020\n",
      "Epoch [22/25], Step [88/590], Loss: 0.3847\n",
      "Epoch [22/25], Step [92/590], Loss: 1.3401\n",
      "Epoch [22/25], Step [96/590], Loss: 0.7882\n",
      "Epoch [22/25], Step [100/590], Loss: 5.2705\n",
      "Epoch [22/25], Step [104/590], Loss: 0.7213\n",
      "Epoch [22/25], Step [108/590], Loss: 0.1080\n",
      "Epoch [22/25], Step [112/590], Loss: 0.0520\n",
      "Epoch [22/25], Step [116/590], Loss: 0.7877\n",
      "Epoch [22/25], Step [120/590], Loss: 2.8293\n",
      "Epoch [22/25], Step [124/590], Loss: 0.1626\n",
      "Epoch [22/25], Step [128/590], Loss: 0.8869\n",
      "Epoch [22/25], Step [132/590], Loss: 0.8011\n",
      "Epoch [22/25], Step [136/590], Loss: 1.1166\n",
      "Epoch [22/25], Step [140/590], Loss: 0.2897\n",
      "Epoch [22/25], Step [144/590], Loss: 3.9905\n",
      "Epoch [22/25], Step [148/590], Loss: 0.0360\n",
      "Epoch [22/25], Step [152/590], Loss: 0.1848\n",
      "Epoch [22/25], Step [156/590], Loss: 0.2122\n",
      "Epoch [22/25], Step [160/590], Loss: 0.0522\n",
      "Epoch [22/25], Step [164/590], Loss: 0.0740\n",
      "Epoch [22/25], Step [168/590], Loss: 0.0063\n",
      "Epoch [22/25], Step [172/590], Loss: 0.1465\n",
      "Epoch [22/25], Step [176/590], Loss: 0.0280\n",
      "Epoch [22/25], Step [180/590], Loss: 1.1895\n",
      "Epoch [22/25], Step [184/590], Loss: 0.0813\n",
      "Epoch [22/25], Step [188/590], Loss: 1.6283\n",
      "Epoch [22/25], Step [192/590], Loss: 1.5186\n",
      "Epoch [22/25], Step [196/590], Loss: 0.2516\n",
      "Epoch [22/25], Step [200/590], Loss: 0.0810\n",
      "Epoch [22/25], Step [204/590], Loss: 0.0194\n",
      "Epoch [22/25], Step [208/590], Loss: 0.0857\n",
      "Epoch [22/25], Step [212/590], Loss: 0.2785\n",
      "Epoch [22/25], Step [216/590], Loss: 0.0774\n",
      "Epoch [22/25], Step [220/590], Loss: 4.7432\n",
      "Epoch [22/25], Step [224/590], Loss: 0.0757\n",
      "Epoch [22/25], Step [228/590], Loss: 0.6930\n",
      "Epoch [22/25], Step [232/590], Loss: 0.0801\n",
      "Epoch [22/25], Step [236/590], Loss: 0.6587\n",
      "Epoch [22/25], Step [240/590], Loss: 0.0353\n",
      "Epoch [22/25], Step [244/590], Loss: 0.6535\n",
      "Epoch [22/25], Step [248/590], Loss: 2.7731\n",
      "Epoch [22/25], Step [252/590], Loss: 0.0117\n",
      "Epoch [22/25], Step [256/590], Loss: 0.0210\n",
      "Epoch [22/25], Step [260/590], Loss: 0.7614\n",
      "Epoch [22/25], Step [264/590], Loss: 1.3797\n",
      "Epoch [22/25], Step [268/590], Loss: 0.0528\n",
      "Epoch [22/25], Step [272/590], Loss: 2.3632\n",
      "Epoch [22/25], Step [276/590], Loss: 0.3578\n",
      "Epoch [22/25], Step [280/590], Loss: 0.0695\n",
      "Epoch [22/25], Step [284/590], Loss: 2.0861\n",
      "Epoch [22/25], Step [288/590], Loss: 1.6779\n",
      "Epoch [22/25], Step [292/590], Loss: 0.0424\n",
      "Epoch [22/25], Step [296/590], Loss: 0.1173\n",
      "Epoch [22/25], Step [300/590], Loss: 0.7182\n",
      "Epoch [22/25], Step [304/590], Loss: 0.5063\n",
      "Epoch [22/25], Step [308/590], Loss: 4.2767\n",
      "Epoch [22/25], Step [312/590], Loss: 3.2339\n",
      "Epoch [22/25], Step [316/590], Loss: 0.1618\n",
      "Epoch [22/25], Step [320/590], Loss: 0.8465\n",
      "Epoch [22/25], Step [324/590], Loss: 1.6243\n",
      "Epoch [22/25], Step [328/590], Loss: 0.1241\n",
      "Epoch [22/25], Step [332/590], Loss: 5.2823\n",
      "Epoch [22/25], Step [336/590], Loss: 0.6895\n",
      "Epoch [22/25], Step [340/590], Loss: 1.7068\n",
      "Epoch [22/25], Step [344/590], Loss: 2.3133\n",
      "Epoch [22/25], Step [348/590], Loss: 0.5057\n",
      "Epoch [22/25], Step [352/590], Loss: 0.2450\n",
      "Epoch [22/25], Step [356/590], Loss: 0.3305\n",
      "Epoch [22/25], Step [360/590], Loss: 0.4972\n",
      "Epoch [22/25], Step [364/590], Loss: 1.6902\n",
      "Epoch [22/25], Step [368/590], Loss: 0.1630\n",
      "Epoch [22/25], Step [372/590], Loss: 0.2884\n",
      "Epoch [22/25], Step [376/590], Loss: 0.2539\n",
      "Epoch [22/25], Step [380/590], Loss: 3.3173\n",
      "Epoch [22/25], Step [384/590], Loss: 0.5597\n",
      "Epoch [22/25], Step [388/590], Loss: 0.0418\n",
      "Epoch [22/25], Step [392/590], Loss: 0.3087\n",
      "Epoch [22/25], Step [396/590], Loss: 1.4421\n",
      "Epoch [22/25], Step [400/590], Loss: 0.0085\n",
      "Epoch [22/25], Step [404/590], Loss: 0.3479\n",
      "Epoch [22/25], Step [408/590], Loss: 0.0860\n",
      "Epoch [22/25], Step [412/590], Loss: 0.1487\n",
      "Epoch [22/25], Step [416/590], Loss: 0.0769\n",
      "Epoch [22/25], Step [420/590], Loss: 0.1900\n",
      "Epoch [22/25], Step [424/590], Loss: 2.2354\n",
      "Epoch [22/25], Step [428/590], Loss: 0.1448\n",
      "Epoch [22/25], Step [432/590], Loss: 0.1226\n",
      "Epoch [22/25], Step [436/590], Loss: 1.1746\n",
      "Epoch [22/25], Step [440/590], Loss: 0.1083\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], Step [444/590], Loss: 0.5363\n",
      "Epoch [22/25], Step [448/590], Loss: 0.0838\n",
      "Epoch [22/25], Step [452/590], Loss: 0.6823\n",
      "Epoch [22/25], Step [456/590], Loss: 1.1887\n",
      "Epoch [22/25], Step [460/590], Loss: 0.4684\n",
      "Epoch [22/25], Step [464/590], Loss: 0.5826\n",
      "Epoch [22/25], Step [468/590], Loss: 0.5306\n",
      "Epoch [22/25], Step [472/590], Loss: 0.0606\n",
      "Epoch [22/25], Step [476/590], Loss: 0.5496\n",
      "Epoch [22/25], Step [480/590], Loss: 1.6583\n",
      "Epoch [22/25], Step [484/590], Loss: 0.1116\n",
      "Epoch [22/25], Step [488/590], Loss: 0.6683\n",
      "Epoch [22/25], Step [492/590], Loss: 0.0174\n",
      "Epoch [22/25], Step [496/590], Loss: 0.2455\n",
      "Epoch [22/25], Step [500/590], Loss: 0.3104\n",
      "Epoch [22/25], Step [504/590], Loss: 0.2215\n",
      "Epoch [22/25], Step [508/590], Loss: 0.7880\n",
      "Epoch [22/25], Step [512/590], Loss: 0.2769\n",
      "Epoch [22/25], Step [516/590], Loss: 0.5248\n",
      "Epoch [22/25], Step [520/590], Loss: 0.0272\n",
      "Epoch [22/25], Step [524/590], Loss: 0.0134\n",
      "Epoch [22/25], Step [528/590], Loss: 0.9590\n",
      "Epoch [22/25], Step [532/590], Loss: 0.7400\n",
      "Epoch [22/25], Step [536/590], Loss: 0.0564\n",
      "Epoch [22/25], Step [540/590], Loss: 0.1964\n",
      "Epoch [22/25], Step [544/590], Loss: 0.0717\n",
      "Epoch [22/25], Step [548/590], Loss: 0.3153\n",
      "Epoch [22/25], Step [552/590], Loss: 0.3588\n",
      "Epoch [22/25], Step [556/590], Loss: 0.1169\n",
      "Epoch [22/25], Step [560/590], Loss: 0.3387\n",
      "Epoch [22/25], Step [564/590], Loss: 0.0936\n",
      "Epoch [22/25], Step [568/590], Loss: 0.2031\n",
      "Epoch [22/25], Step [572/590], Loss: 0.6166\n",
      "Epoch [22/25], Step [576/590], Loss: 2.7110\n",
      "Epoch [22/25], Step [580/590], Loss: 2.1575\n",
      "Epoch [22/25], Step [584/590], Loss: 2.2750\n",
      "Epoch [22/25], Step [588/590], Loss: 0.4320\n",
      "epoch 22\n",
      "Epoch [23/25], Step [4/590], Loss: 0.5711\n",
      "Epoch [23/25], Step [8/590], Loss: 0.0881\n",
      "Epoch [23/25], Step [12/590], Loss: 0.0615\n",
      "Epoch [23/25], Step [16/590], Loss: 1.2215\n",
      "Epoch [23/25], Step [20/590], Loss: 4.1736\n",
      "Epoch [23/25], Step [24/590], Loss: 2.7074\n",
      "Epoch [23/25], Step [28/590], Loss: 2.9120\n",
      "Epoch [23/25], Step [32/590], Loss: 0.4176\n",
      "Epoch [23/25], Step [36/590], Loss: 0.3706\n",
      "Epoch [23/25], Step [40/590], Loss: 0.2329\n",
      "Epoch [23/25], Step [44/590], Loss: 1.0315\n",
      "Epoch [23/25], Step [48/590], Loss: 0.6817\n",
      "Epoch [23/25], Step [52/590], Loss: 5.4193\n",
      "Epoch [23/25], Step [56/590], Loss: 3.7130\n",
      "Epoch [23/25], Step [60/590], Loss: 0.5390\n",
      "Epoch [23/25], Step [64/590], Loss: 2.0613\n",
      "Epoch [23/25], Step [68/590], Loss: 0.3867\n",
      "Epoch [23/25], Step [72/590], Loss: 0.3067\n",
      "Epoch [23/25], Step [76/590], Loss: 0.1045\n",
      "Epoch [23/25], Step [80/590], Loss: 2.7811\n",
      "Epoch [23/25], Step [84/590], Loss: 0.0208\n",
      "Epoch [23/25], Step [88/590], Loss: 2.3468\n",
      "Epoch [23/25], Step [92/590], Loss: 1.9427\n",
      "Epoch [23/25], Step [96/590], Loss: 0.1103\n",
      "Epoch [23/25], Step [100/590], Loss: 0.3685\n",
      "Epoch [23/25], Step [104/590], Loss: 0.4845\n",
      "Epoch [23/25], Step [108/590], Loss: 0.1259\n",
      "Epoch [23/25], Step [112/590], Loss: 1.0633\n",
      "Epoch [23/25], Step [116/590], Loss: 0.0115\n",
      "Epoch [23/25], Step [120/590], Loss: 0.7432\n",
      "Epoch [23/25], Step [124/590], Loss: 0.8167\n",
      "Epoch [23/25], Step [128/590], Loss: 0.0722\n",
      "Epoch [23/25], Step [132/590], Loss: 8.6073\n",
      "Epoch [23/25], Step [136/590], Loss: 0.9064\n",
      "Epoch [23/25], Step [140/590], Loss: 0.5812\n",
      "Epoch [23/25], Step [144/590], Loss: 1.2188\n",
      "Epoch [23/25], Step [148/590], Loss: 2.8011\n",
      "Epoch [23/25], Step [152/590], Loss: 3.2103\n",
      "Epoch [23/25], Step [156/590], Loss: 0.0580\n",
      "Epoch [23/25], Step [160/590], Loss: 0.0712\n",
      "Epoch [23/25], Step [164/590], Loss: 0.0764\n",
      "Epoch [23/25], Step [168/590], Loss: 0.2671\n",
      "Epoch [23/25], Step [172/590], Loss: 1.4344\n",
      "Epoch [23/25], Step [176/590], Loss: 0.0355\n",
      "Epoch [23/25], Step [180/590], Loss: 0.1445\n",
      "Epoch [23/25], Step [184/590], Loss: 0.2301\n",
      "Epoch [23/25], Step [188/590], Loss: 0.0983\n",
      "Epoch [23/25], Step [192/590], Loss: 0.2657\n",
      "Epoch [23/25], Step [196/590], Loss: 0.0372\n",
      "Epoch [23/25], Step [200/590], Loss: 1.3016\n",
      "Epoch [23/25], Step [204/590], Loss: 0.0627\n",
      "Epoch [23/25], Step [208/590], Loss: 3.1344\n",
      "Epoch [23/25], Step [212/590], Loss: 0.0624\n",
      "Epoch [23/25], Step [216/590], Loss: 3.0533\n",
      "Epoch [23/25], Step [220/590], Loss: 0.2939\n",
      "Epoch [23/25], Step [224/590], Loss: 0.0323\n",
      "Epoch [23/25], Step [228/590], Loss: 0.0377\n",
      "Epoch [23/25], Step [232/590], Loss: 0.0351\n",
      "Epoch [23/25], Step [236/590], Loss: 1.0837\n",
      "Epoch [23/25], Step [240/590], Loss: 0.0086\n",
      "Epoch [23/25], Step [244/590], Loss: 0.2328\n",
      "Epoch [23/25], Step [248/590], Loss: 0.2943\n",
      "Epoch [23/25], Step [252/590], Loss: 0.3940\n",
      "Epoch [23/25], Step [256/590], Loss: 0.0714\n",
      "Epoch [23/25], Step [260/590], Loss: 0.1997\n",
      "Epoch [23/25], Step [264/590], Loss: 0.0216\n",
      "Epoch [23/25], Step [268/590], Loss: 0.6740\n",
      "Epoch [23/25], Step [272/590], Loss: 0.8850\n",
      "Epoch [23/25], Step [276/590], Loss: 0.0619\n",
      "Epoch [23/25], Step [280/590], Loss: 0.4175\n",
      "Epoch [23/25], Step [284/590], Loss: 4.4085\n",
      "Epoch [23/25], Step [288/590], Loss: 2.0752\n",
      "Epoch [23/25], Step [292/590], Loss: 0.3256\n",
      "Epoch [23/25], Step [296/590], Loss: 0.1517\n",
      "Epoch [23/25], Step [300/590], Loss: 0.8735\n",
      "Epoch [23/25], Step [304/590], Loss: 3.9972\n",
      "Epoch [23/25], Step [308/590], Loss: 0.1705\n",
      "Epoch [23/25], Step [312/590], Loss: 0.3288\n",
      "Epoch [23/25], Step [316/590], Loss: 0.0525\n",
      "Epoch [23/25], Step [320/590], Loss: 0.0868\n",
      "Epoch [23/25], Step [324/590], Loss: 0.2895\n",
      "Epoch [23/25], Step [328/590], Loss: 4.0229\n",
      "Epoch [23/25], Step [332/590], Loss: 0.2821\n",
      "Epoch [23/25], Step [336/590], Loss: 1.8990\n",
      "Epoch [23/25], Step [340/590], Loss: 0.0916\n",
      "Epoch [23/25], Step [344/590], Loss: 0.1238\n",
      "Epoch [23/25], Step [348/590], Loss: 0.8886\n",
      "Epoch [23/25], Step [352/590], Loss: 1.6755\n",
      "Epoch [23/25], Step [356/590], Loss: 0.0842\n",
      "Epoch [23/25], Step [360/590], Loss: 3.0047\n",
      "Epoch [23/25], Step [364/590], Loss: 0.0740\n",
      "Epoch [23/25], Step [368/590], Loss: 0.5981\n",
      "Epoch [23/25], Step [372/590], Loss: 0.1676\n",
      "Epoch [23/25], Step [376/590], Loss: 0.6891\n",
      "Epoch [23/25], Step [380/590], Loss: 0.1365\n",
      "Epoch [23/25], Step [384/590], Loss: 0.0140\n",
      "Epoch [23/25], Step [388/590], Loss: 3.6902\n",
      "Epoch [23/25], Step [392/590], Loss: 0.0130\n",
      "Epoch [23/25], Step [396/590], Loss: 0.1398\n",
      "Epoch [23/25], Step [400/590], Loss: 0.4237\n",
      "Epoch [23/25], Step [404/590], Loss: 0.1896\n",
      "Epoch [23/25], Step [408/590], Loss: 4.0888\n",
      "Epoch [23/25], Step [412/590], Loss: 0.0823\n",
      "Epoch [23/25], Step [416/590], Loss: 0.4537\n",
      "Epoch [23/25], Step [420/590], Loss: 0.0904\n",
      "Epoch [23/25], Step [424/590], Loss: 0.5195\n",
      "Epoch [23/25], Step [428/590], Loss: 0.2141\n",
      "Epoch [23/25], Step [432/590], Loss: 0.3018\n",
      "Epoch [23/25], Step [436/590], Loss: 0.0855\n",
      "Epoch [23/25], Step [440/590], Loss: 0.0343\n",
      "Epoch [23/25], Step [444/590], Loss: 0.4505\n",
      "Epoch [23/25], Step [448/590], Loss: 0.0944\n",
      "Epoch [23/25], Step [452/590], Loss: 0.6005\n",
      "Epoch [23/25], Step [456/590], Loss: 0.4455\n",
      "Epoch [23/25], Step [460/590], Loss: 0.1720\n",
      "Epoch [23/25], Step [464/590], Loss: 0.5741\n",
      "Epoch [23/25], Step [468/590], Loss: 0.5823\n",
      "Epoch [23/25], Step [472/590], Loss: 0.0265\n",
      "Epoch [23/25], Step [476/590], Loss: 0.0417\n",
      "Epoch [23/25], Step [480/590], Loss: 0.0921\n",
      "Epoch [23/25], Step [484/590], Loss: 0.8987\n",
      "Epoch [23/25], Step [488/590], Loss: 0.9797\n",
      "Epoch [23/25], Step [492/590], Loss: 0.1507\n",
      "Epoch [23/25], Step [496/590], Loss: 0.3282\n",
      "Epoch [23/25], Step [500/590], Loss: 1.3829\n",
      "Epoch [23/25], Step [504/590], Loss: 0.0710\n",
      "Epoch [23/25], Step [508/590], Loss: 0.4648\n",
      "Epoch [23/25], Step [512/590], Loss: 0.0748\n",
      "Epoch [23/25], Step [516/590], Loss: 0.0656\n",
      "Epoch [23/25], Step [520/590], Loss: 0.0637\n",
      "Epoch [23/25], Step [524/590], Loss: 3.1073\n",
      "Epoch [23/25], Step [528/590], Loss: 0.0673\n",
      "Epoch [23/25], Step [532/590], Loss: 0.0142\n",
      "Epoch [23/25], Step [536/590], Loss: 0.0465\n",
      "Epoch [23/25], Step [540/590], Loss: 0.5043\n",
      "Epoch [23/25], Step [544/590], Loss: 0.0067\n",
      "Epoch [23/25], Step [548/590], Loss: 3.2827\n",
      "Epoch [23/25], Step [552/590], Loss: 1.1130\n",
      "Epoch [23/25], Step [556/590], Loss: 0.1822\n",
      "Epoch [23/25], Step [560/590], Loss: 0.0210\n",
      "Epoch [23/25], Step [564/590], Loss: 0.0999\n",
      "Epoch [23/25], Step [568/590], Loss: 0.0804\n",
      "Epoch [23/25], Step [572/590], Loss: 0.6176\n",
      "Epoch [23/25], Step [576/590], Loss: 0.8491\n",
      "Epoch [23/25], Step [580/590], Loss: 0.0236\n",
      "Epoch [23/25], Step [584/590], Loss: 0.0247\n",
      "Epoch [23/25], Step [588/590], Loss: 0.0041\n",
      "epoch 23\n",
      "Epoch [24/25], Step [4/590], Loss: 1.0155\n",
      "Epoch [24/25], Step [8/590], Loss: 0.2158\n",
      "Epoch [24/25], Step [12/590], Loss: 0.0082\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], Step [16/590], Loss: 3.5012\n",
      "Epoch [24/25], Step [20/590], Loss: 2.1276\n",
      "Epoch [24/25], Step [24/590], Loss: 0.0273\n",
      "Epoch [24/25], Step [28/590], Loss: 0.0182\n",
      "Epoch [24/25], Step [32/590], Loss: 0.0944\n",
      "Epoch [24/25], Step [36/590], Loss: 0.0420\n",
      "Epoch [24/25], Step [40/590], Loss: 0.0213\n",
      "Epoch [24/25], Step [44/590], Loss: 0.1191\n",
      "Epoch [24/25], Step [48/590], Loss: 0.5769\n",
      "Epoch [24/25], Step [52/590], Loss: 0.3950\n",
      "Epoch [24/25], Step [56/590], Loss: 1.3823\n",
      "Epoch [24/25], Step [60/590], Loss: 0.1769\n",
      "Epoch [24/25], Step [64/590], Loss: 0.0825\n",
      "Epoch [24/25], Step [68/590], Loss: 0.0148\n",
      "Epoch [24/25], Step [72/590], Loss: 0.1279\n",
      "Epoch [24/25], Step [76/590], Loss: 0.0129\n",
      "Epoch [24/25], Step [80/590], Loss: 9.7210\n",
      "Epoch [24/25], Step [84/590], Loss: 0.0772\n",
      "Epoch [24/25], Step [88/590], Loss: 0.0856\n",
      "Epoch [24/25], Step [92/590], Loss: 0.0133\n",
      "Epoch [24/25], Step [96/590], Loss: 0.0632\n",
      "Epoch [24/25], Step [100/590], Loss: 0.1796\n",
      "Epoch [24/25], Step [104/590], Loss: 0.0824\n",
      "Epoch [24/25], Step [108/590], Loss: 0.1250\n",
      "Epoch [24/25], Step [112/590], Loss: 0.0087\n",
      "Epoch [24/25], Step [116/590], Loss: 0.0209\n",
      "Epoch [24/25], Step [120/590], Loss: 0.0954\n",
      "Epoch [24/25], Step [124/590], Loss: 0.0464\n",
      "Epoch [24/25], Step [128/590], Loss: 0.0244\n",
      "Epoch [24/25], Step [132/590], Loss: 0.4358\n",
      "Epoch [24/25], Step [136/590], Loss: 0.0576\n",
      "Epoch [24/25], Step [140/590], Loss: 0.0024\n",
      "Epoch [24/25], Step [144/590], Loss: 0.0204\n",
      "Epoch [24/25], Step [148/590], Loss: 0.0075\n",
      "Epoch [24/25], Step [152/590], Loss: 0.1281\n",
      "Epoch [24/25], Step [156/590], Loss: 0.0205\n",
      "Epoch [24/25], Step [160/590], Loss: 0.0046\n",
      "Epoch [24/25], Step [164/590], Loss: 1.3461\n",
      "Epoch [24/25], Step [168/590], Loss: 0.0251\n",
      "Epoch [24/25], Step [172/590], Loss: 0.0341\n",
      "Epoch [24/25], Step [176/590], Loss: 0.0049\n",
      "Epoch [24/25], Step [180/590], Loss: 0.0506\n",
      "Epoch [24/25], Step [184/590], Loss: 0.0185\n",
      "Epoch [24/25], Step [188/590], Loss: 0.1279\n",
      "Epoch [24/25], Step [192/590], Loss: 0.1321\n",
      "Epoch [24/25], Step [196/590], Loss: 0.0361\n",
      "Epoch [24/25], Step [200/590], Loss: 0.0990\n",
      "Epoch [24/25], Step [204/590], Loss: 0.0433\n",
      "Epoch [24/25], Step [208/590], Loss: 0.0723\n",
      "Epoch [24/25], Step [212/590], Loss: 0.0139\n",
      "Epoch [24/25], Step [216/590], Loss: 0.0936\n",
      "Epoch [24/25], Step [220/590], Loss: 0.2925\n",
      "Epoch [24/25], Step [224/590], Loss: 0.3711\n",
      "Epoch [24/25], Step [228/590], Loss: 0.0186\n",
      "Epoch [24/25], Step [232/590], Loss: 0.2764\n",
      "Epoch [24/25], Step [236/590], Loss: 0.0073\n",
      "Epoch [24/25], Step [240/590], Loss: 0.0136\n",
      "Epoch [24/25], Step [244/590], Loss: 0.0281\n",
      "Epoch [24/25], Step [248/590], Loss: 0.0483\n",
      "Epoch [24/25], Step [252/590], Loss: 0.0092\n",
      "Epoch [24/25], Step [256/590], Loss: 0.0546\n",
      "Epoch [24/25], Step [260/590], Loss: 0.0271\n",
      "Epoch [24/25], Step [264/590], Loss: 0.0263\n",
      "Epoch [24/25], Step [268/590], Loss: 0.2214\n",
      "Epoch [24/25], Step [272/590], Loss: 0.0116\n",
      "Epoch [24/25], Step [276/590], Loss: 0.0016\n",
      "Epoch [24/25], Step [280/590], Loss: 0.0133\n",
      "Epoch [24/25], Step [284/590], Loss: 0.0547\n",
      "Epoch [24/25], Step [288/590], Loss: 0.0357\n",
      "Epoch [24/25], Step [292/590], Loss: 0.4925\n",
      "Epoch [24/25], Step [296/590], Loss: 0.0214\n",
      "Epoch [24/25], Step [300/590], Loss: 0.0018\n",
      "Epoch [24/25], Step [304/590], Loss: 0.0130\n",
      "Epoch [24/25], Step [308/590], Loss: 0.1088\n",
      "Epoch [24/25], Step [312/590], Loss: 0.0498\n",
      "Epoch [24/25], Step [316/590], Loss: 0.0114\n",
      "Epoch [24/25], Step [320/590], Loss: 0.0237\n",
      "Epoch [24/25], Step [324/590], Loss: 0.0079\n",
      "Epoch [24/25], Step [328/590], Loss: 0.0127\n",
      "Epoch [24/25], Step [332/590], Loss: 0.0284\n",
      "Epoch [24/25], Step [336/590], Loss: 0.0131\n",
      "Epoch [24/25], Step [340/590], Loss: 0.0364\n",
      "Epoch [24/25], Step [344/590], Loss: 1.7641\n",
      "Epoch [24/25], Step [348/590], Loss: 0.0491\n",
      "Epoch [24/25], Step [352/590], Loss: 0.0175\n",
      "Epoch [24/25], Step [356/590], Loss: 0.0061\n",
      "Epoch [24/25], Step [360/590], Loss: 0.0036\n",
      "Epoch [24/25], Step [364/590], Loss: 0.0109\n",
      "Epoch [24/25], Step [368/590], Loss: 0.0077\n",
      "Epoch [24/25], Step [372/590], Loss: 0.0584\n",
      "Epoch [24/25], Step [376/590], Loss: 0.0308\n",
      "Epoch [24/25], Step [380/590], Loss: 0.0084\n",
      "Epoch [24/25], Step [384/590], Loss: 0.5631\n",
      "Epoch [24/25], Step [388/590], Loss: 0.1173\n",
      "Epoch [24/25], Step [392/590], Loss: 0.0063\n",
      "Epoch [24/25], Step [396/590], Loss: 0.1641\n",
      "Epoch [24/25], Step [400/590], Loss: 0.0191\n",
      "Epoch [24/25], Step [404/590], Loss: 0.5745\n",
      "Epoch [24/25], Step [408/590], Loss: 0.7330\n",
      "Epoch [24/25], Step [412/590], Loss: 0.0311\n",
      "Epoch [24/25], Step [416/590], Loss: 0.0015\n",
      "Epoch [24/25], Step [420/590], Loss: 0.0139\n",
      "Epoch [24/25], Step [424/590], Loss: 0.0143\n",
      "Epoch [24/25], Step [428/590], Loss: 0.0208\n",
      "Epoch [24/25], Step [432/590], Loss: 0.0684\n",
      "Epoch [24/25], Step [436/590], Loss: 0.8031\n",
      "Epoch [24/25], Step [440/590], Loss: 0.0252\n",
      "Epoch [24/25], Step [444/590], Loss: 0.0375\n",
      "Epoch [24/25], Step [448/590], Loss: 0.2967\n",
      "Epoch [24/25], Step [452/590], Loss: 5.0479\n",
      "Epoch [24/25], Step [456/590], Loss: 0.2157\n",
      "Epoch [24/25], Step [460/590], Loss: 0.0782\n",
      "Epoch [24/25], Step [464/590], Loss: 0.0839\n",
      "Epoch [24/25], Step [468/590], Loss: 0.4281\n",
      "Epoch [24/25], Step [472/590], Loss: 0.0826\n",
      "Epoch [24/25], Step [476/590], Loss: 0.2801\n",
      "Epoch [24/25], Step [480/590], Loss: 0.0277\n",
      "Epoch [24/25], Step [484/590], Loss: 0.2204\n",
      "Epoch [24/25], Step [488/590], Loss: 0.0239\n",
      "Epoch [24/25], Step [492/590], Loss: 0.1849\n",
      "Epoch [24/25], Step [496/590], Loss: 0.1133\n",
      "Epoch [24/25], Step [500/590], Loss: 0.0123\n",
      "Epoch [24/25], Step [504/590], Loss: 0.0634\n",
      "Epoch [24/25], Step [508/590], Loss: 0.0442\n",
      "Epoch [24/25], Step [512/590], Loss: 0.0015\n",
      "Epoch [24/25], Step [516/590], Loss: 0.0126\n",
      "Epoch [24/25], Step [520/590], Loss: 0.0662\n",
      "Epoch [24/25], Step [524/590], Loss: 0.0151\n",
      "Epoch [24/25], Step [528/590], Loss: 0.1299\n",
      "Epoch [24/25], Step [532/590], Loss: 0.0060\n",
      "Epoch [24/25], Step [536/590], Loss: 0.0070\n",
      "Epoch [24/25], Step [540/590], Loss: 0.0188\n",
      "Epoch [24/25], Step [544/590], Loss: 0.6773\n",
      "Epoch [24/25], Step [548/590], Loss: 0.0095\n",
      "Epoch [24/25], Step [552/590], Loss: 6.5890\n",
      "Epoch [24/25], Step [556/590], Loss: 0.0930\n",
      "Epoch [24/25], Step [560/590], Loss: 1.2407\n",
      "Epoch [24/25], Step [564/590], Loss: 0.0839\n",
      "Epoch [24/25], Step [568/590], Loss: 4.4176\n",
      "Epoch [24/25], Step [572/590], Loss: 0.5672\n",
      "Epoch [24/25], Step [576/590], Loss: 0.0361\n",
      "Epoch [24/25], Step [580/590], Loss: 0.0772\n",
      "Epoch [24/25], Step [584/590], Loss: 0.1159\n",
      "Epoch [24/25], Step [588/590], Loss: 0.0702\n",
      "epoch 24\n",
      "Epoch [25/25], Step [4/590], Loss: 0.0198\n",
      "Epoch [25/25], Step [8/590], Loss: 0.0080\n",
      "Epoch [25/25], Step [12/590], Loss: 0.0363\n",
      "Epoch [25/25], Step [16/590], Loss: 0.0691\n",
      "Epoch [25/25], Step [20/590], Loss: 0.0309\n",
      "Epoch [25/25], Step [24/590], Loss: 0.1072\n",
      "Epoch [25/25], Step [28/590], Loss: 9.0744\n",
      "Epoch [25/25], Step [32/590], Loss: 0.0386\n",
      "Epoch [25/25], Step [36/590], Loss: 0.6204\n",
      "Epoch [25/25], Step [40/590], Loss: 0.0229\n",
      "Epoch [25/25], Step [44/590], Loss: 0.5537\n",
      "Epoch [25/25], Step [48/590], Loss: 0.1617\n",
      "Epoch [25/25], Step [52/590], Loss: 0.8942\n",
      "Epoch [25/25], Step [56/590], Loss: 7.1721\n",
      "Epoch [25/25], Step [60/590], Loss: 0.0469\n",
      "Epoch [25/25], Step [64/590], Loss: 6.8614\n",
      "Epoch [25/25], Step [68/590], Loss: 0.0967\n",
      "Epoch [25/25], Step [72/590], Loss: 3.8560\n",
      "Epoch [25/25], Step [76/590], Loss: 0.1091\n",
      "Epoch [25/25], Step [80/590], Loss: 1.3976\n",
      "Epoch [25/25], Step [84/590], Loss: 0.0834\n",
      "Epoch [25/25], Step [88/590], Loss: 0.3263\n",
      "Epoch [25/25], Step [92/590], Loss: 0.1036\n",
      "Epoch [25/25], Step [96/590], Loss: 0.6709\n",
      "Epoch [25/25], Step [100/590], Loss: 4.9138\n",
      "Epoch [25/25], Step [104/590], Loss: 0.0893\n",
      "Epoch [25/25], Step [108/590], Loss: 2.7028\n",
      "Epoch [25/25], Step [112/590], Loss: 0.0387\n",
      "Epoch [25/25], Step [116/590], Loss: 0.0679\n",
      "Epoch [25/25], Step [120/590], Loss: 0.0970\n",
      "Epoch [25/25], Step [124/590], Loss: 3.5912\n",
      "Epoch [25/25], Step [128/590], Loss: 0.5195\n",
      "Epoch [25/25], Step [132/590], Loss: 1.2107\n",
      "Epoch [25/25], Step [136/590], Loss: 0.0454\n",
      "Epoch [25/25], Step [140/590], Loss: 0.3467\n",
      "Epoch [25/25], Step [144/590], Loss: 0.0857\n",
      "Epoch [25/25], Step [148/590], Loss: 0.1905\n",
      "Epoch [25/25], Step [152/590], Loss: 0.9367\n",
      "Epoch [25/25], Step [156/590], Loss: 1.7160\n",
      "Epoch [25/25], Step [160/590], Loss: 0.0974\n",
      "Epoch [25/25], Step [164/590], Loss: 0.0685\n",
      "Epoch [25/25], Step [168/590], Loss: 0.5321\n",
      "Epoch [25/25], Step [172/590], Loss: 0.8682\n",
      "Epoch [25/25], Step [176/590], Loss: 0.1130\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Step [180/590], Loss: 0.4279\n",
      "Epoch [25/25], Step [184/590], Loss: 1.9606\n",
      "Epoch [25/25], Step [188/590], Loss: 0.0340\n",
      "Epoch [25/25], Step [192/590], Loss: 0.7612\n",
      "Epoch [25/25], Step [196/590], Loss: 5.7060\n",
      "Epoch [25/25], Step [200/590], Loss: 1.1746\n",
      "Epoch [25/25], Step [204/590], Loss: 1.0475\n",
      "Epoch [25/25], Step [208/590], Loss: 0.1702\n",
      "Epoch [25/25], Step [212/590], Loss: 1.4999\n",
      "Epoch [25/25], Step [216/590], Loss: 0.0201\n",
      "Epoch [25/25], Step [220/590], Loss: 0.5560\n",
      "Epoch [25/25], Step [224/590], Loss: 0.2219\n",
      "Epoch [25/25], Step [228/590], Loss: 0.1151\n",
      "Epoch [25/25], Step [232/590], Loss: 0.4803\n",
      "Epoch [25/25], Step [236/590], Loss: 2.1617\n",
      "Epoch [25/25], Step [240/590], Loss: 1.4183\n",
      "Epoch [25/25], Step [244/590], Loss: 0.2286\n",
      "Epoch [25/25], Step [248/590], Loss: 1.2321\n",
      "Epoch [25/25], Step [252/590], Loss: 0.0057\n",
      "Epoch [25/25], Step [256/590], Loss: 0.0125\n",
      "Epoch [25/25], Step [260/590], Loss: 0.5586\n",
      "Epoch [25/25], Step [264/590], Loss: 0.0179\n",
      "Epoch [25/25], Step [268/590], Loss: 0.0594\n",
      "Epoch [25/25], Step [272/590], Loss: 2.1678\n",
      "Epoch [25/25], Step [276/590], Loss: 0.2500\n",
      "Epoch [25/25], Step [280/590], Loss: 0.1349\n",
      "Epoch [25/25], Step [284/590], Loss: 0.2375\n",
      "Epoch [25/25], Step [288/590], Loss: 0.3257\n",
      "Epoch [25/25], Step [292/590], Loss: 5.3997\n",
      "Epoch [25/25], Step [296/590], Loss: 0.0223\n",
      "Epoch [25/25], Step [300/590], Loss: 0.3289\n",
      "Epoch [25/25], Step [304/590], Loss: 0.0102\n",
      "Epoch [25/25], Step [308/590], Loss: 0.0457\n",
      "Epoch [25/25], Step [312/590], Loss: 0.3856\n",
      "Epoch [25/25], Step [316/590], Loss: 0.0038\n",
      "Epoch [25/25], Step [320/590], Loss: 0.7377\n",
      "Epoch [25/25], Step [324/590], Loss: 0.0127\n",
      "Epoch [25/25], Step [328/590], Loss: 2.1625\n",
      "Epoch [25/25], Step [332/590], Loss: 0.0328\n",
      "Epoch [25/25], Step [336/590], Loss: 0.0276\n",
      "Epoch [25/25], Step [340/590], Loss: 0.0122\n",
      "Epoch [25/25], Step [344/590], Loss: 0.3445\n",
      "Epoch [25/25], Step [348/590], Loss: 0.2643\n",
      "Epoch [25/25], Step [352/590], Loss: 0.9782\n",
      "Epoch [25/25], Step [356/590], Loss: 0.0585\n",
      "Epoch [25/25], Step [360/590], Loss: 0.0793\n",
      "Epoch [25/25], Step [364/590], Loss: 0.2470\n",
      "Epoch [25/25], Step [368/590], Loss: 0.2088\n",
      "Epoch [25/25], Step [372/590], Loss: 0.1996\n",
      "Epoch [25/25], Step [376/590], Loss: 0.0112\n",
      "Epoch [25/25], Step [380/590], Loss: 0.0418\n",
      "Epoch [25/25], Step [384/590], Loss: 0.2491\n",
      "Epoch [25/25], Step [388/590], Loss: 0.2886\n",
      "Epoch [25/25], Step [392/590], Loss: 0.4706\n",
      "Epoch [25/25], Step [396/590], Loss: 0.0110\n",
      "Epoch [25/25], Step [400/590], Loss: 0.1747\n",
      "Epoch [25/25], Step [404/590], Loss: 0.0287\n",
      "Epoch [25/25], Step [408/590], Loss: 0.1066\n",
      "Epoch [25/25], Step [412/590], Loss: 0.0023\n",
      "Epoch [25/25], Step [416/590], Loss: 0.0166\n",
      "Epoch [25/25], Step [420/590], Loss: 7.9362\n",
      "Epoch [25/25], Step [424/590], Loss: 0.1533\n",
      "Epoch [25/25], Step [428/590], Loss: 0.3465\n",
      "Epoch [25/25], Step [432/590], Loss: 0.1016\n",
      "Epoch [25/25], Step [436/590], Loss: 0.1177\n",
      "Epoch [25/25], Step [440/590], Loss: 0.0584\n",
      "Epoch [25/25], Step [444/590], Loss: 0.0396\n",
      "Epoch [25/25], Step [448/590], Loss: 0.0103\n",
      "Epoch [25/25], Step [452/590], Loss: 0.0691\n",
      "Epoch [25/25], Step [456/590], Loss: 1.2936\n",
      "Epoch [25/25], Step [460/590], Loss: 0.0495\n",
      "Epoch [25/25], Step [464/590], Loss: 1.4562\n",
      "Epoch [25/25], Step [468/590], Loss: 0.0684\n",
      "Epoch [25/25], Step [472/590], Loss: 0.4605\n",
      "Epoch [25/25], Step [476/590], Loss: 0.3871\n",
      "Epoch [25/25], Step [480/590], Loss: 0.4431\n",
      "Epoch [25/25], Step [484/590], Loss: 0.0362\n",
      "Epoch [25/25], Step [488/590], Loss: 3.0672\n",
      "Epoch [25/25], Step [492/590], Loss: 2.3647\n",
      "Epoch [25/25], Step [496/590], Loss: 0.0413\n",
      "Epoch [25/25], Step [500/590], Loss: 0.0720\n",
      "Epoch [25/25], Step [504/590], Loss: 0.0717\n",
      "Epoch [25/25], Step [508/590], Loss: 0.0082\n",
      "Epoch [25/25], Step [512/590], Loss: 0.0265\n",
      "Epoch [25/25], Step [516/590], Loss: 2.2646\n",
      "Epoch [25/25], Step [520/590], Loss: 0.0608\n",
      "Epoch [25/25], Step [524/590], Loss: 3.0143\n",
      "Epoch [25/25], Step [528/590], Loss: 0.0130\n",
      "Epoch [25/25], Step [532/590], Loss: 9.9796\n",
      "Epoch [25/25], Step [536/590], Loss: 0.1966\n",
      "Epoch [25/25], Step [540/590], Loss: 1.0563\n",
      "Epoch [25/25], Step [544/590], Loss: 0.2473\n",
      "Epoch [25/25], Step [548/590], Loss: 1.2107\n",
      "Epoch [25/25], Step [552/590], Loss: 0.1462\n",
      "Epoch [25/25], Step [556/590], Loss: 1.3115\n",
      "Epoch [25/25], Step [560/590], Loss: 0.0096\n",
      "Epoch [25/25], Step [564/590], Loss: 0.5263\n",
      "Epoch [25/25], Step [568/590], Loss: 0.0201\n",
      "Epoch [25/25], Step [572/590], Loss: 0.0848\n",
      "Epoch [25/25], Step [576/590], Loss: 1.9309\n",
      "Epoch [25/25], Step [580/590], Loss: 0.4752\n",
      "Epoch [25/25], Step [584/590], Loss: 0.0970\n",
      "Epoch [25/25], Step [588/590], Loss: 0.9986\n",
      "Time: 33315.15098667145\n",
      "Beginning Testing..\n",
      "Accuracy of the network on the 11406 test images: 71.31334385411188 %\n",
      "reach: 0.43555093555093555\n",
      "squat: 0.507168458781362\n",
      "inline: 0.6902654867256637\n",
      "lunge: 0.776414451261077\n",
      "hamstrings: 0.7124227865477007\n",
      "stretch: 0.8567041965199591\n",
      "deadbug: 0.8306233062330624\n",
      "pushup: 0.7170868347338936\n"
     ]
    }
   ],
   "source": [
    "model = NeuralNet().to(device)\n",
    "loss_list2 = []\n",
    "# if we're only testing, we don't want to train for any epochs, and we want to load a model\n",
    "if not is_train:\n",
    "    num_epochs = 0\n",
    "    model.load_state_dict(torch.load('model.ckpt'))\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss(reduce=False, size_average=False) #TODO: define your loss here. hint: should just require calling a built-in pytorch layer.\n",
    "# NOTE: you can use a different optimizer besides Adam, like RMSProp or SGD, if you'd like\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the model\n",
    "# Loop over epochs\n",
    "print('Beginning training..')\n",
    "total_step = len(train_loader)\n",
    "loss_list2 = []\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    print('epoch {}'.format(epoch))\n",
    "    for i, (local_batch,local_labels) in enumerate(train_loader):\n",
    "        # Transfer to GPU\n",
    "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model.forward(local_ims)\n",
    "        loss = criterion(outputs, local_labels).sum()\n",
    "        # TODO: maintain a list of your losses as a function of number of steps\n",
    "        #       because we ask you to plot this information\n",
    "        # NOTE: if you use Google Colab's tensorboard-like feature to visualize\n",
    "        #       the loss, you do not need to plot it here. just take a screenshot\n",
    "        #       of the loss curve and include it in your write-up.\n",
    "        #print(loss.item())\n",
    "        loss_list2.append(loss.item())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 4 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "\n",
    "end = time.time()\n",
    "print('Time: {}'.format(end - start))\n",
    "\n",
    "# Test the model\n",
    "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
    "print('Beginning Testing..')\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    predicted_list = []\n",
    "    groundtruth_list = []\n",
    "    for (local_batch,local_labels) in val_loader:\n",
    "        # Transfer to GPU\n",
    "        local_ims, local_labels = local_batch.to(device), local_labels.to(device)\n",
    "\n",
    "        outputs = model.forward(local_ims)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += local_labels.size(0)\n",
    "        predicted_list.extend(predicted)\n",
    "        groundtruth_list.extend(local_labels)\n",
    "        correct += (predicted == local_labels).sum().item()\n",
    "\n",
    "    print('Accuracy of the network on the {} test images: {} %'\n",
    "          .format(total, 100 * correct / total))\n",
    "\n",
    "# Look at some things about the model results..\n",
    "# convert the predicted_list and groundtruth_list Tensors to lists\n",
    "pl = [p.cpu().numpy().tolist() for p in predicted_list]\n",
    "gt = [p.cpu().numpy().tolist() for p in groundtruth_list]\n",
    "\n",
    "# TODO: use pl and gt to produce your confusion matrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_m = confusion_matrix(gt, pl)\n",
    "\n",
    "# view the per-movement accuracy\n",
    "label_map = ['reach','squat','inline','lunge','hamstrings','stretch','deadbug','pushup']\n",
    "for id in range(len(label_map)):\n",
    "    print('{}: {}'.format(label_map[id],sum([p and g for (p,g) in zip(np.array(pl)==np.array(gt),np.array(gt)==id)])/(sum(np.array(gt)==id)+0.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 419,  250,  212,   47,    1,   13,    2,   18],\n",
       "       [ 353,  566,   72,   87,   13,   12,    4,    9],\n",
       "       [ 293,  180, 1560,  171,    5,   21,    8,   22],\n",
       "       [  31,   71,  128, 1139,   15,   37,    3,   43],\n",
       "       [  19,    5,   21,   16, 1038,  161,  108,   89],\n",
       "       [   7,   24,    0,    8,  130, 1674,   35,   76],\n",
       "       [  17,    3,    5,   53,   69,   63, 1226,   40],\n",
       "       [   7,   17,   15,   14,   30,   64,   55,  512]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_m "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = Mds189(label_file_test,loader=default_loader,transform=transforms.Compose([\n",
    "                                                   #transforms.Pad(1),\n",
    "                                                   #transforms.Resize(256),\n",
    "                                                   transforms.ToTensor(),\n",
    "                                                   transforms.Normalize(mean, std)\n",
    "                                               ]))\n",
    "test_loader = data.DataLoader(test_dataset, **params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f0e11e7a828>"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    predicted_list = []\n",
    "    for (local_batch, _) in test_loader:\n",
    "        # Transfer to GPU\n",
    "        local_ims = local_batch.to(device)\n",
    "\n",
    "        outputs = model.forward(local_ims)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        predicted_list.extend(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6197"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(predicted_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('resources/problem4/kaggle_submission_format.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>Category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001.jpg</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0002.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0003.jpg</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0004.jpg</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Id Category\n",
       "0  0000.jpg        5\n",
       "1  0001.jpg        1\n",
       "2  0002.jpg        5\n",
       "3  0003.jpg        2\n",
       "4  0004.jpg        5"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for i in range(len(df)):\n",
    "    df['Category'][i] = predicted_list[i].item()\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"kaggle_submission_format.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
